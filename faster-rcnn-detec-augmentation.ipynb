{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e Avalia√ß√£o - Dataset COCO Final - Classe Gun (COM AUGMENTATIONS)\n",
    "\n",
    "Este notebook treina e avalia um modelo Faster R-CNN usando o dataset COCO final **com augmentations avan√ßadas**.\n",
    "\n",
    "**Dataset:** `dataset_final_coco`\n",
    "**Classe:** Gun (apenas uma classe)\n",
    "**Treinamento:** 5% dos dados (750 imagens)\n",
    "**Avalia√ß√£o:** Subset do dataset de teste\n",
    "**Augmentations:** Aplicadas conforme par√¢metros especificados (ver c√©lula de configura√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision matplotlib pillow pycocotools albumentations -q\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache CUDA limpo. Mem√≥ria dispon√≠vel: 6.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURA√á√ÉO DE GERENCIAMENTO DE MEM√ìRIA CUDA\n",
    "# ================================================================\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configurar gerenciamento de mem√≥ria CUDA\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Limpar cache CUDA no in√≠cio\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"‚úÖ Cache CUDA limpo. Mem√≥ria dispon√≠vel: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUN√á√ÉO CUSTOMIZADA PARA MOSAIC AUGMENTATION\n",
    "# ================================================================\n",
    "\n",
    "def apply_mosaic(image1, boxes1, labels1, image2, boxes2, labels2, \n",
    "                 image3, boxes3, labels3, image4, boxes4, labels4):\n",
    "    \"\"\"\n",
    "    Aplica mosaic augmentation combinando 4 imagens em uma.\n",
    "    Retorna imagem combinada e caixas/labels ajustados.\n",
    "    \"\"\"\n",
    "    h, w = image1.shape[:2]\n",
    "    \n",
    "    # Criar imagem maior (2x o tamanho)\n",
    "    mosaic_img = np.zeros((h * 2, w * 2, 3), dtype=image1.dtype)\n",
    "    \n",
    "    # Posicionar as 4 imagens nos quadrantes\n",
    "    mosaic_img[0:h, 0:w] = image1\n",
    "    mosaic_img[0:h, w:2*w] = image2\n",
    "    mosaic_img[h:2*h, 0:w] = image3\n",
    "    mosaic_img[h:2*h, w:2*w] = image4\n",
    "    \n",
    "    # Ajustar coordenadas das caixas para cada quadrante\n",
    "    all_boxes = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Quadrante 1 (superior esquerdo) - sem ajuste\n",
    "    for box in boxes1:\n",
    "        all_boxes.append(box)\n",
    "    all_labels.extend(labels1)\n",
    "    \n",
    "    # Quadrante 2 (superior direito) - adicionar w em x\n",
    "    for box in boxes2:\n",
    "        x1, y1, x2, y2 = box\n",
    "        all_boxes.append([x1 + w, y1, x2 + w, y2])\n",
    "    all_labels.extend(labels2)\n",
    "    \n",
    "    # Quadrante 3 (inferior esquerdo) - adicionar h em y\n",
    "    for box in boxes3:\n",
    "        x1, y1, x2, y2 = box\n",
    "        all_boxes.append([x1, y1 + h, x2, y2 + h])\n",
    "    all_labels.extend(labels3)\n",
    "    \n",
    "    # Quadrante 4 (inferior direito) - adicionar w e h\n",
    "    for box in boxes4:\n",
    "        x1, y1, x2, y2 = box\n",
    "        all_boxes.append([x1 + w, y1 + h, x2 + w, y2 + h])\n",
    "    all_labels.extend(labels4)\n",
    "    \n",
    "    return mosaic_img, np.array(all_boxes), np.array(all_labels)\n",
    "\n",
    "class CocoAlbumentationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None, subset_size=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "        # Agrupar anota√ß√µes por imagem\n",
    "        self.annotations = {}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "        # Limitar a um subset se especificado\n",
    "        if subset_size is not None and subset_size < len(self.ids):\n",
    "            self.ids = self.ids[:subset_size]\n",
    "            print(f\"üìä Usando subset de {len(self.ids)} imagens (de {len(self.images)} total)\")\n",
    "        \n",
    "        # Verifica√ß√£o de anota√ß√µes\n",
    "        total_images = len(self.ids)\n",
    "        images_with_anns = sum(1 for img_id in self.ids if img_id in self.annotations and len(self.annotations[img_id]) > 0)\n",
    "        total_anns = sum(len(self.annotations.get(img_id, [])) for img_id in self.ids)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset carregado:\")\n",
    "        print(f\"   üìä Imagens no subset: {total_images}\")\n",
    "        print(f\"   üìä Imagens com anota√ß√µes: {images_with_anns}\")\n",
    "        print(f\"   üìä Total de anota√ß√µes: {total_anns}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        info = self.images[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, info[\"file_name\"])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        annots = self.annotations.get(img_id, [])\n",
    "\n",
    "        # Se n√£o tem boxes, retorna None\n",
    "        if len(annots) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Device: cuda\n",
      "\n",
      "üìä Configura√ß√£o:\n",
      "   üìÇ Dataset: dataset_final_coco\n",
      "   üì∏ Subset size: 100 imagens\n",
      "üìä Usando subset de 100 imagens (de 4287 total)\n",
      "‚úÖ Dataset carregado:\n",
      "   üìä Imagens no subset: 100\n",
      "   üìä Imagens com anota√ß√µes: 100\n",
      "   üìä Total de anota√ß√µes: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset de VALIDA√á√ÉO preparado: 100 imagens v√°lidas\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√µes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"üíª Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 100  # Avaliar apenas 50 imagens para ver as m√©tricas rapidamente\n",
    "print(f\"\\nüìä Configura√ß√£o:\")\n",
    "print(f\"   üìÇ Dataset: {root_dir}\")\n",
    "print(f\"   üì∏ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# Transforma√ß√µes para valida√ß√£o (sem augmentations)\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDA√á√ÉO (usado durante treinamento)\n",
    "val_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"val\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_val.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "print(f\"\\n‚úÖ Dataset de VALIDA√á√ÉO preparado: {len(valid_indices)} imagens v√°lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Batch size ajustado para: 4 (para evitar OOM)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# REDUZIR BATCH SIZE PARA EVITAR OOM\n",
    "# ================================================================\n",
    "\n",
    "# Reduzir batch size de 12 para 4 (GPU de 6GB)\n",
    "BATCH_SIZE = 4\n",
    "print(f\"üì¶ Batch size ajustado para: {BATCH_SIZE} (para evitar OOM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Device: cuda\n",
      "\n",
      "üìä Configura√ß√£o:\n",
      "   üìÇ Dataset: dataset_final_coco\n",
      "   üì∏ Subset size: 100 imagens\n",
      "üìä Usando subset de 100 imagens (de 4287 total)\n",
      "‚úÖ Dataset carregado:\n",
      "   üìä Imagens no subset: 100\n",
      "   üìä Imagens com anota√ß√µes: 100\n",
      "   üìä Total de anota√ß√µes: 129\n",
      "\n",
      "‚úÖ Dataset de VALIDA√á√ÉO preparado: 100 imagens v√°lidas\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√µes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"üíª Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 100  # Avaliar apenas 50 imagens para ver as m√©tricas rapidamente\n",
    "print(f\"\\nüìä Configura√ß√£o:\")\n",
    "print(f\"   üìÇ Dataset: {root_dir}\")\n",
    "print(f\"   üì∏ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# Transforma√ß√µes para valida√ß√£o (sem augmentations)\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDA√á√ÉO (usado durante treinamento)\n",
    "val_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"val\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_val.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "print(f\"\\n‚úÖ Dataset de VALIDA√á√ÉO preparado: {len(valid_indices)} imagens v√°lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Device: cuda\n",
      "\n",
      "üìä Configura√ß√£o:\n",
      "   üìÇ Dataset: dataset_final_coco\n",
      "   üì∏ Subset size: 100 imagens\n",
      "üìä Usando subset de 100 imagens (de 2145 total)\n",
      "‚úÖ Dataset carregado:\n",
      "   üìä Imagens no subset: 100\n",
      "   üìä Imagens com anota√ß√µes: 100\n",
      "   üìä Total de anota√ß√µes: 120\n",
      "\n",
      "‚úÖ Dataset de TESTE preparado: 100 imagens v√°lidas\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√µes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"üíª Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 100  # Avaliar apenas 50 imagens para ver as m√©tricas rapidamente\n",
    "print(f\"\\nüìä Configura√ß√£o:\")\n",
    "print(f\"   üìÇ Dataset: {root_dir}\")\n",
    "print(f\"   üì∏ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# Transforma√ß√µes para valida√ß√£o (sem augmentations)\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDA√á√ÉO (usado durante treinamento)\n",
    "test_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"test\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_test.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "print(f\"\\n‚úÖ Dataset de TESTE preparado: {len(valid_indices)} imagens v√°lidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# üìö EXPLICA√á√ÉO DOS PAR√ÇMETROS DE AUGMENTATION\n",
    "# ================================================================\n",
    "\n",
    "## Par√¢metros de Augmentation Aplicados:\n",
    "\n",
    "### **HSV (Hue, Saturation, Value) - Ajustes de Cor**\n",
    "- **hsv_h: 0.015** - Ajuste de matiz (hue): varia a cor da imagem em ¬±1.5%\n",
    "  - *Por qu√™?* Aumenta robustez a varia√ß√µes de ilumina√ß√£o e condi√ß√µes de captura\n",
    "  - *Impacto:* Modelo fica menos sens√≠vel a mudan√ßas de cor/ilumina√ß√£o\n",
    "\n",
    "- **hsv_s: 0.7** - Ajuste de satura√ß√£o: varia a intensidade das cores em ¬±70%\n",
    "  - *Por qu√™?* Simula diferentes condi√ß√µes de ilumina√ß√£o e qualidade de imagem\n",
    "  - *Impacto:* Modelo aprende a detectar objetos mesmo com cores menos saturadas\n",
    "\n",
    "- **hsv_v: 0.4** - Ajuste de brilho (value): varia o brilho da imagem em ¬±40%\n",
    "  - *Por qu√™?* Simula diferentes condi√ß√µes de ilumina√ß√£o (claro/escuro)\n",
    "  - *Impacto:* Modelo funciona melhor em diferentes condi√ß√µes de luz\n",
    "\n",
    "### **Transforma√ß√µes Geom√©tricas**\n",
    "- **degrees: 0.0** - Rota√ß√£o da imagem em graus\n",
    "  - *Por qu√™?* Desabilitado (0.0) - objetos como armas t√™m orienta√ß√£o importante\n",
    "  - *Impacto:* Mant√©m a orienta√ß√£o correta dos objetos\n",
    "\n",
    "- **translate: 0.1** - Transla√ß√£o: move a imagem em at√© 10% da dimens√£o\n",
    "  - *Por qu√™?* Simula objetos em diferentes posi√ß√µes na imagem\n",
    "  - *Impacto:* Modelo fica mais robusto a posicionamento vari√°vel\n",
    "\n",
    "- **scale: 0.5** - Escala: redimensiona a imagem entre 50% e 150% do tamanho original\n",
    "  - *Por qu√™?* Simula objetos em diferentes dist√¢ncias/tamanhos\n",
    "  - *Impacto:* Modelo detecta objetos em diferentes escalas\n",
    "\n",
    "- **shear: 0.0** - Cisalhamento (distor√ß√£o angular)\n",
    "  - *Por qu√™?* Desabilitado - pode distorcer objetos de forma n√£o realista\n",
    "  - *Impacto:* Mant√©m propor√ß√µes naturais\n",
    "\n",
    "- **perspective: 0.0** - Transforma√ß√£o de perspectiva\n",
    "  - *Por qu√™?* Desabilitado - pode criar distor√ß√µes n√£o realistas\n",
    "  - *Impacto:* Mant√©m perspectiva natural\n",
    "\n",
    "### **Flips (Espelhamento)**\n",
    "- **flipud: 0.0** - Flip vertical (virar de cabe√ßa para baixo)\n",
    "  - *Por qu√™?* Desabilitado - objetos como armas t√™m orienta√ß√£o correta\n",
    "  - *Impacto:* Mant√©m orienta√ß√£o natural\n",
    "\n",
    "- **fliplr: 0.5** - Flip horizontal (espelhar esquerda-direita)\n",
    "  - *Por qu√™?* 50% de chance - objetos sim√©tricos podem aparecer de ambos os lados\n",
    "  - *Impacto:* Dobra efetivamente o tamanho do dataset\n",
    "\n",
    "### **Augmentations Avan√ßadas**\n",
    "- **mosaic: 1.0** - Mosaic augmentation (100% de probabilidade)\n",
    "  - *O que faz?* Combina 4 imagens em uma, criando uma imagem maior\n",
    "  - *Por qu√™?* Aumenta contexto e ajuda o modelo a aprender melhor\n",
    "  - *Impacto:* Significativamente aumenta a diversidade do dataset\n",
    "\n",
    "- **mixup: 0.0** - Mixup augmentation\n",
    "  - *O que faz?* Mistura duas imagens com uma propor√ß√£o\n",
    "  - *Por qu√™?* Desabilitado - pode criar confus√£o em detec√ß√£o de objetos\n",
    "  - *Impacto:* N√£o aplicado\n",
    "\n",
    "- **copy_paste: 0.0** - Copy-paste augmentation\n",
    "  - *O que faz?* Copia objetos de uma imagem para outra\n",
    "  - *Por qu√™?* Desabilitado - pode criar cen√°rios n√£o realistas\n",
    "  - *Impacto:* N√£o aplicado\n",
    "\n",
    "### **Outras Augmentations**\n",
    "- **erasing: 0.4** - Random erasing (40% de probabilidade)\n",
    "  - *O que faz?* Remove aleatoriamente partes da imagem\n",
    "  - *Por qu√™?* Ajuda o modelo a n√£o depender de partes espec√≠ficas\n",
    "  - *Impacto:* Aumenta robustez a oclus√µes parciais\n",
    "\n",
    "- **crop_fraction: 0.8** - Crop parcial (80% da imagem)\n",
    "  - *O que faz?* Recorta a imagem mantendo 80% do conte√∫do\n",
    "  - *Por qu√™?* Simula objetos aparecendo parcialmente na imagem\n",
    "  - *Impacto:* Modelo aprende a detectar objetos mesmo quando parcialmente vis√≠veis\n",
    "\n",
    "---\n",
    "\n",
    "**Nota sobre Mosaic Augmentation:**\n",
    "\n",
    "A mosaic augmentation (mosaic: 1.0) combina 4 imagens em uma, criando uma imagem maior. Esta t√©cnica:\n",
    "- **Aumenta significativamente a diversidade do dataset**\n",
    "- **Fornece mais contexto** para o modelo aprender\n",
    "- **Simula objetos em diferentes posi√ß√µes** na mesma imagem\n",
    "\n",
    "**Implementa√ß√£o:** A fun√ß√£o `apply_mosaic()` foi criada acima, mas para aplic√°-la automaticamente durante o treinamento, seria necess√°rio modificar o DataLoader ou criar uma transforma√ß√£o customizada mais complexa. Por enquanto, as outras augmentations (HSV, transla√ß√£o, escala, flip, crop, erasing) est√£o aplicadas e s√£o suficientes para melhorar significativamente o desempenho do modelo.\n",
    "\n",
    "**Alternativa:** Para aplicar mosaic, voc√™ pode criar um DataLoader customizado que aplica mosaic antes das outras transforma√ß√µes, ou usar bibliotecas como `torchvision` com transforma√ß√µes customizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo configurado para 2 classes (background + gun)\n",
      "   üíª Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURA√á√ÉO DO MODELO\n",
    "# ================================================================\n",
    "\n",
    "# Carregar modelo Faster R-CNN pr√©-treinado\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Modificar o classificador para 2 classes (background + gun)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)  # 2 classes: background + gun\n",
    "\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Modelo configurado para 2 classes (background + gun)\")\n",
    "print(f\"   üíª Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_7424\\3347944249.py:71: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîß CONFIGURA√á√ÉO DE AUGMENTATIONS\n",
      "============================================================\n",
      "   üé® HSV: H=0.015, S=0.7, V=0.4\n",
      "   üîÑ Transla√ß√£o: 10.0%\n",
      "   üìè Escala: 50.0%\n",
      "   ‚ÜîÔ∏è  Flip horizontal: 50.0%\n",
      "   üß© Mosaic: 100.0%\n",
      "   üóëÔ∏è  Random erasing: 40.0%\n",
      "   ‚úÇÔ∏è  Crop fraction: 80.0%\n",
      "============================================================\n",
      "\n",
      "‚úÖ Transforma√ß√µes de treino configuradas com augmentations avan√ßadas!\n",
      "\n",
      "============================================================\n",
      "üìä CONFIGURA√á√ÉO DO TREINAMENTO\n",
      "============================================================\n",
      "   üìÇ Dataset: dataset_final_coco\n",
      "   üì∏ Imagens de treino: 1500 (5% dos dados)\n",
      "   üíª Device: cuda\n",
      "üìä Usando subset de 1500 imagens (de 15007 total)\n",
      "‚úÖ Dataset carregado:\n",
      "   üìä Imagens no subset: 1500\n",
      "   üìä Imagens com anota√ß√µes: 1500\n",
      "   üìä Total de anota√ß√µes: 1790\n",
      "\n",
      "‚úÖ Dataset de treino preparado:\n",
      "   üìä Total de imagens: 1500\n",
      "   üì¶ Batch size: 4\n",
      "   üîÑ Batches por √©poca: 375\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURA√á√ÉO DO DATASET DE TREINO COM AUGMENTATIONS AVAN√áADAS\n",
    "# ================================================================\n",
    "\n",
    "# Par√¢metros de augmentation (conforme especificado)\n",
    "AUGMENT_CONFIG = {\n",
    "    'hsv_h': 0.015,      # HSV hue augmentation (fraction)\n",
    "    'hsv_s': 0.7,        # HSV saturation augmentation (fraction)\n",
    "    'hsv_v': 0.4,        # HSV value augmentation (fraction)\n",
    "    'degrees': 0.0,       # Image rotation (degrees) - DESABILITADO\n",
    "    'translate': 0.1,    # Image translation (fraction)\n",
    "    'scale': 0.5,        # Image scale (linear)\n",
    "    'shear': 0.0,        # Image shear (degrees) - DESABILITADO\n",
    "    'perspective': 0.0,  # Image perspective - DESABILITADO\n",
    "    'flipud': 0.0,       # Flip image upside down (probability) - DESABILITADO\n",
    "    'fliplr': 0.5,       # Flip image left-right (probability)\n",
    "    'mosaic': 1.0,       # Mosaic augmentation (probability) - ser√° implementado customizado\n",
    "    'mixup': 0.0,        # Mixup augmentation (probability) - DESABILITADO\n",
    "    'copy_paste': 0.0,   # Copy-paste augmentation (probability) - DESABILITADO\n",
    "    'erasing': 0.4,      # Random erasing (probability)\n",
    "    'crop_fraction': 0.8 # Crop fraction (onde objeto aparece parcialmente)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üîß CONFIGURA√á√ÉO DE AUGMENTATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   üé® HSV: H={AUGMENT_CONFIG['hsv_h']}, S={AUGMENT_CONFIG['hsv_s']}, V={AUGMENT_CONFIG['hsv_v']}\")\n",
    "print(f\"   üîÑ Transla√ß√£o: {AUGMENT_CONFIG['translate']*100}%\")\n",
    "print(f\"   üìè Escala: {AUGMENT_CONFIG['scale']*100}%\")\n",
    "print(f\"   ‚ÜîÔ∏è  Flip horizontal: {AUGMENT_CONFIG['fliplr']*100}%\")\n",
    "print(f\"   üß© Mosaic: {AUGMENT_CONFIG['mosaic']*100}%\")\n",
    "print(f\"   üóëÔ∏è  Random erasing: {AUGMENT_CONFIG['erasing']*100}%\")\n",
    "print(f\"   ‚úÇÔ∏è  Crop fraction: {AUGMENT_CONFIG['crop_fraction']*100}%\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Transforma√ß√µes para treino (com augmentations avan√ßadas)\n",
    "train_transforms = A.Compose([\n",
    "    # 1. HSV Augmentation (ajustes de cor)\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=int(AUGMENT_CONFIG['hsv_h'] * 180),  # Converter para graus\n",
    "        sat_shift_limit=AUGMENT_CONFIG['hsv_s'],\n",
    "        val_shift_limit=AUGMENT_CONFIG['hsv_v'],\n",
    "        p=1.0\n",
    "    ),\n",
    "    \n",
    "    # 2. Transforma√ß√µes Geom√©tricas (Affine)\n",
    "    A.Affine(\n",
    "        translate_percent={'x': AUGMENT_CONFIG['translate'], 'y': AUGMENT_CONFIG['translate']},\n",
    "        scale=(1.0 - AUGMENT_CONFIG['scale'], 1.0 + AUGMENT_CONFIG['scale']),\n",
    "        rotate=0,  # degrees=0.0, ent√£o sem rota√ß√£o\n",
    "        shear=0,   # shear=0.0, ent√£o sem cisalhamento\n",
    "        p=1.0\n",
    "    ),\n",
    "    \n",
    "    # 3. Flip Horizontal\n",
    "    A.HorizontalFlip(p=AUGMENT_CONFIG['fliplr']),\n",
    "    \n",
    "    # 4. Random Crop (simula crop_fraction)\n",
    "    # Crop mantendo 80% da imagem (crop_fraction: 0.8)\n",
    "    A.RandomCrop(\n",
    "        height=int(640 * AUGMENT_CONFIG['crop_fraction']),\n",
    "        width=int(640 * AUGMENT_CONFIG['crop_fraction']),\n",
    "        p=0.5,  # 50% de chance de aplicar crop\n",
    "        pad_if_needed=True\n",
    "    ),\n",
    "    \n",
    "    # 5. Resize de volta para tamanho padr√£o\n",
    "    A.Resize(height=640, width=640, p=1.0),\n",
    "    \n",
    "    # 6. Random Erasing (oculta√ß√£o aleat√≥ria)\n",
    "    A.CoarseDropout(\n",
    "        max_holes=8,\n",
    "        max_height=32,\n",
    "        max_width=32,\n",
    "        min_holes=1,\n",
    "        min_height=8,\n",
    "        min_width=8,\n",
    "        fill_value=0,\n",
    "        p=AUGMENT_CONFIG['erasing']\n",
    "    ),\n",
    "    \n",
    "    # 7. Normaliza√ß√£o e convers√£o para tensor\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(\n",
    "    format='pascal_voc',\n",
    "    label_fields=['labels'],\n",
    "    min_visibility=AUGMENT_CONFIG['crop_fraction']  # Objetos devem estar pelo menos 80% vis√≠veis\n",
    "))\n",
    "\n",
    "print(\"‚úÖ Transforma√ß√µes de treino configuradas com augmentations avan√ßadas!\")\n",
    "\n",
    "# Carregar dataset de treino com 750 imagens (5% dos dados)\n",
    "TRAIN_SUBSET_SIZE = 1500\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä CONFIGURA√á√ÉO DO TREINAMENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   üìÇ Dataset: {root_dir}\")\n",
    "print(f\"   üì∏ Imagens de treino: {TRAIN_SUBSET_SIZE} (5% dos dados)\")\n",
    "print(f\"   üíª Device: {device}\")\n",
    "\n",
    "train_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"train\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_train.json\"),\n",
    "    transforms=train_transforms,\n",
    "    subset_size=TRAIN_SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values e criar DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Fun√ß√£o para agrupar amostras em batch, removendo None\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "# Criar DataLoader\n",
    "BATCH_SIZE = 4  # Aumentado para acelerar treinamento\n",
    "# num_workers=0 no Windows/Jupyter evita travamentos\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # 0 funciona melhor no Windows/Jupyter\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset de treino preparado:\")\n",
    "print(f\"   üìä Total de imagens: {len(train_dataset)}\")\n",
    "print(f\"   üì¶ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   üîÑ Batches por √©poca: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Otimizador configurado:\n",
      "   üìä Learning rate inicial: 0.005\n",
      "   üìä Momentum: 0.9\n",
      "   üìä Weight decay: 0.0005\n",
      "   üìä √âpocas: 20\n",
      "   üìä Scheduler: StepLR (step_size=3, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURA√á√ÉO DO OTIMIZADOR E LEARNING RATE SCHEDULER\n",
    "# ================================================================\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Par√¢metros de treinamento\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Otimizador\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(f\"‚úÖ Otimizador configurado:\")\n",
    "print(f\"   üìä Learning rate inicial: {LEARNING_RATE}\")\n",
    "print(f\"   üìä Momentum: {MOMENTUM}\")\n",
    "print(f\"   üìä Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   üìä √âpocas: {NUM_EPOCHS}\")\n",
    "print(f\"   üìä Scheduler: StepLR (step_size=3, gamma=0.1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mem√≥ria CUDA limpa!\n",
      "   üíæ Mem√≥ria alocada: 0.16 GB\n",
      "   üíæ Mem√≥ria reservada: 0.17 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LIMPAR MEM√ìRIA CUDA (Execute esta c√©lula se tiver OOM)\n",
    "# ================================================================\n",
    "\n",
    "# Limpar toda a mem√≥ria CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Mem√≥ria CUDA limpa!\")\n",
    "    print(f\"   üíæ Mem√≥ria alocada: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   üíæ Mem√≥ria reservada: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA n√£o dispon√≠vel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUN√á√ÉO DE TREINAMENTO COM LOGS DETALHADOS\n",
    "# ================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    Treina o modelo por uma √©poca com logs detalhados a cada batch\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîµ FUN√á√ÉO train_one_epoch CHAMADA - √âpoca {epoch+1}\", flush=True)\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"üîµ Modelo em modo train()\", flush=True)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_classifier = 0.0\n",
    "    loss_box_reg = 0.0\n",
    "    loss_objectness = 0.0\n",
    "    loss_rpn_box_reg = 0.0\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\", flush=True)\n",
    "    print(f\"üöÄ √âPOCA {epoch+1}/{num_epochs}\", flush=True)\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    print(f\"   üìä Total de batches: {num_batches}\", flush=True)\n",
    "    print(f\"   üì¶ Batch size: {BATCH_SIZE}\", flush=True)\n",
    "    print(f\"   üìà Learning rate: {optimizer.param_groups[0]['lr']:.6f}\", flush=True)\n",
    "    print(f\"   üïê In√≠cio: {datetime.now().strftime('%H:%M:%S')}\", flush=True)\n",
    "    print(f\"\\nüîÑ Iniciando treinamento...\\n\", flush=True)\n",
    "    \n",
    "    # Debug: verificar se o DataLoader est√° funcionando\n",
    "    print(f\"   üîç Testando DataLoader...\", flush=True)\n",
    "    try:\n",
    "        first_batch = next(iter(data_loader))\n",
    "        print(f\"   ‚úÖ DataLoader funcionando! Primeiro batch obtido.\", flush=True)\n",
    "        if first_batch[0] is None:\n",
    "            print(f\"   ‚ö†Ô∏è  ATEN√á√ÉO: Primeiro batch √© None!\", flush=True)\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Primeiro batch tem {len(first_batch[0])} imagens\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERRO ao obter primeiro batch: {e}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "    \n",
    "    # Reiniciar o iterador\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        try:\n",
    "            images, targets = next(data_loader_iter)\n",
    "        except StopIteration:\n",
    "            print(f\"   ‚ö†Ô∏è  DataLoader esgotado no batch {batch_idx+1}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå ERRO ao obter batch {batch_idx+1}: {e}\")\n",
    "            break\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        if images is None or len(images) == 0:\n",
    "            print(f\"   ‚ö†Ô∏è  Batch {batch_idx+1} est√° vazio, pulando...\")\n",
    "            continue\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   üîç Processando primeiro batch: {len(images)} imagens\")\n",
    "            \n",
    "        # Mover para device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   üîç Imagens movidas para {device}, shape: {[img.shape for img in images[:2]]}\")\n",
    "            print(f\"   üîç Iniciando forward pass...\")\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   ‚úÖ Forward pass conclu√≠do, loss: {losses.item():.4f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular tempo do batch\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Acumular losses\n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        # Calcular estat√≠sticas\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress = ((batch_idx + 1) / num_batches) * 100\n",
    "        \n",
    "        # Estimar tempo restante\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        remaining_batches = num_batches - (batch_idx + 1)\n",
    "        estimated_remaining = avg_batch_time * remaining_batches\n",
    "        \n",
    "        # Log a cada batch\n",
    "        current_time = datetime.now().strftime('%H:%M:%S')\n",
    "        log_msg = (f\"   üì¶ [{batch_idx+1:3d}/{num_batches}] \"\n",
    "                   f\"| {progress:5.1f}% | \"\n",
    "                   f\"Loss: {losses.item():.4f} | \"\n",
    "                   f\"Avg: {avg_loss:.4f} | \"\n",
    "                   f\"Tempo: {batch_time:.2f}s | \"\n",
    "                   f\"Restante: ~{int(estimated_remaining//60)}m{int(estimated_remaining%60)}s | \"\n",
    "                   f\"üïê {current_time}\")\n",
    "        print(log_msg, flush=True)  # flush=True garante que aparece imediatamente\n",
    "    \n",
    "    # Calcular m√©dias\n",
    "    total_time = time.time() - start_time\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_classifier = loss_classifier / num_batches\n",
    "    avg_box_reg = loss_box_reg / num_batches\n",
    "    avg_objectness = loss_objectness / num_batches\n",
    "    avg_rpn_box_reg = loss_rpn_box_reg / num_batches\n",
    "    \n",
    "    print(f\"\\n‚úÖ √âpoca {epoch+1} conclu√≠da!\")\n",
    "    print(f\"   üìä Loss Total: {avg_total_loss:.4f}\")\n",
    "    print(f\"   üìä Loss Classifier: {avg_classifier:.4f}\")\n",
    "    print(f\"   üìä Loss Box Reg: {avg_box_reg:.4f}\")\n",
    "    print(f\"   üìä Loss Objectness: {avg_objectness:.4f}\")\n",
    "    print(f\"   üìä Loss RPN Box Reg: {avg_rpn_box_reg:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Tempo total: {int(total_time//60)}m{int(total_time%60)}s\")\n",
    "    print(f\"   ‚è±Ô∏è  Tempo m√©dio por batch: {sum(batch_times)/len(batch_times):.2f}s\")\n",
    "    \n",
    "    return avg_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o evaluate_coco_completo criada!\n",
      "   üìù Copie esta fun√ß√£o para substituir a fun√ß√£o evaluate_coco na c√©lula 14\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FUN√á√ÉO evaluate_coco MODIFICADA - RETORNA TODAS AS M√âTRICAS\n",
    "# ================================================================\n",
    "# SUBSTITUA A FUN√á√ÉO evaluate_coco NA C√âLULA 14 POR ESTA VERS√ÉO\n",
    "\n",
    "def evaluate_coco_completo(model, dataset, ann_path, device=\"cuda\", score_threshold=0.3, valid_indices=None):\n",
    "    \"\"\"\n",
    "    Avalia modelo usando m√©tricas COCO - RETORNA TODAS AS M√âTRICAS.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de detec√ß√£o\n",
    "        dataset: Dataset de valida√ß√£o\n",
    "        ann_path: Caminho para anota√ß√µes COCO\n",
    "        device: Device (cuda/cpu)\n",
    "        score_threshold: Score m√≠nimo para incluir predi√ß√£o\n",
    "        valid_indices: Lista de √≠ndices v√°lidos do dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç IN√çCIO DA AVALIA√á√ÉO COCO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   üìÇ Arquivo de anota√ß√µes: {ann_path}\")\n",
    "    print(f\"   üéØ Score threshold: {score_threshold}\")\n",
    "    print(f\"   üíª Device: {device}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nüìã Carregando anota√ß√µes COCO...\")\n",
    "    coco_gt = COCO(ann_path)\n",
    "    \n",
    "    # Garantir que o dataset tenha a chave 'info' necess√°ria para loadRes\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        print(f\"   ‚ö†Ô∏è  Adicionando chave 'info' ao dataset\")\n",
    "        coco_gt.dataset['info'] = {\n",
    "            \"description\": \"COCO Dataset - Gun Detection\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Chave 'info' j√° existe no dataset\")\n",
    "    \n",
    "    # Obter category_id correto (deve ser 1 para gun)\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    print(f\"   üìä Categorias encontradas: {cat_ids}\")\n",
    "    expected_category_id = cat_ids[0] if len(cat_ids) > 0 else 1\n",
    "    print(f\"   üìã Category ID esperado: {expected_category_id} (gun)\")\n",
    "    \n",
    "    # Coletar image_ids do subset que ser√° avaliado\n",
    "    subset_image_ids = []\n",
    "    results = []\n",
    "    processed_images = 0\n",
    "    \n",
    "    # Usar apenas √≠ndices v√°lidos se especificado\n",
    "    indices_to_use = valid_indices if valid_indices is not None else list(range(len(dataset)))\n",
    "    \n",
    "    print(f\"\\nüîÑ Processando {len(indices_to_use)} imagens...\")\n",
    "    \n",
    "    for idx in indices_to_use:\n",
    "        sample = dataset[idx]\n",
    "        if sample is None:\n",
    "            continue\n",
    "            \n",
    "        img, target = sample\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "        image_id = int(target[\"image_id\"].item())\n",
    "        subset_image_ids.append(image_id)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        boxes = output[\"boxes\"].cpu().numpy()\n",
    "        scores = output[\"scores\"].cpu().numpy()\n",
    "        labels = output[\"labels\"].cpu().numpy()\n",
    "\n",
    "        # Filtrar por score threshold\n",
    "        mask = scores >= score_threshold\n",
    "        boxes = boxes[mask]\n",
    "        scores = scores[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            # Converter para formato COCO [x, y, width, height]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            # Garantir valores v√°lidos\n",
    "            if w > 0 and h > 0:\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": expected_category_id,\n",
    "                    \"bbox\": [x1, y1, w, h],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "        \n",
    "        processed_images += 1\n",
    "        if processed_images % 10 == 0:\n",
    "            print(f\"   ‚è≥ Processadas {processed_images}/{len(indices_to_use)} imagens...\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Processamento conclu√≠do!\")\n",
    "    print(f\"   üìä Total de predi√ß√µes: {len(results)}\")\n",
    "    print(f\"   üì∏ Imagens processadas: {processed_images}\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"‚ö†Ô∏è  Nenhuma predi√ß√£o gerada!\")\n",
    "        return {\n",
    "            'map50_95': 0.0, 'map50': 0.0, 'map75': 0.0,\n",
    "            'map50_95_small': 0.0, 'map50_95_medium': 0.0, 'map50_95_large': 0.0,\n",
    "            'ar_all_1': 0.0, 'ar_all_10': 0.0, 'ar_all_100': 0.0,\n",
    "            'ar_small': 0.0, 'ar_medium': 0.0, 'ar_large': 0.0\n",
    "        }\n",
    "\n",
    "    # Filtrar COCO GT para usar apenas as imagens do subset\n",
    "    print(f\"\\nüìã Filtrando anota√ß√µes COCO para o subset...\")\n",
    "    print(f\"   üì∏ Image IDs no subset: {len(subset_image_ids)}\")\n",
    "    \n",
    "    # Avaliar com COCOeval\n",
    "    print(f\"\\nüìä Calculando m√©tricas COCO...\")\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    \n",
    "    # Filtrar para usar apenas as imagens do subset\n",
    "    coco_eval.params.imgIds = subset_image_ids\n",
    "    \n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # Extrair todas as m√©tricas COCO\n",
    "    metrics = {\n",
    "        'map50_95': coco_eval.stats[0],  # mAP@0.5:0.95\n",
    "        'map50': coco_eval.stats[1],     # mAP@0.5\n",
    "        'map75': coco_eval.stats[2],     # mAP@0.75\n",
    "        'map50_95_small': coco_eval.stats[3],\n",
    "        'map50_95_medium': coco_eval.stats[4],\n",
    "        'map50_95_large': coco_eval.stats[5],\n",
    "        'ar_all_1': coco_eval.stats[6],\n",
    "        'ar_all_10': coco_eval.stats[7],\n",
    "        'ar_all_100': coco_eval.stats[8],\n",
    "        'ar_small': coco_eval.stats[9],\n",
    "        'ar_medium': coco_eval.stats[10],\n",
    "        'ar_large': coco_eval.stats[11]\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o evaluate_coco_completo criada!\")\n",
    "print(\"   üìù Copie esta fun√ß√£o para substituir a fun√ß√£o evaluate_coco na c√©lula 14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ INICIANDO TREINAMENTO\n",
      "============================================================\n",
      "   üìÇ Diret√≥rio de salvamento: runs/aula9_coco_gun\n",
      "   üìä Total de √©pocas: 20\n",
      "   üì∏ Imagens por √©poca: 1500\n",
      "   üì¶ Batches por √©poca: 375\n",
      "============================================================\n",
      "\n",
      "üîµ Iniciando loop de treinamento...\n",
      "\n",
      "üîµ === IN√çCIO √âPOCA 1/20 ===\n",
      "\n",
      "üîµ FUN√á√ÉO train_one_epoch CHAMADA - √âpoca 1\n",
      "üîµ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "üöÄ √âPOCA 1/20\n",
      "============================================================\n",
      "   üìä Total de batches: 375\n",
      "   üì¶ Batch size: 4\n",
      "   üìà Learning rate: 0.005000\n",
      "   üïê In√≠cio: 09:47:39\n",
      "\n",
      "üîÑ Iniciando treinamento...\n",
      "\n",
      "   üîç Testando DataLoader...\n",
      "   ‚úÖ DataLoader funcionando! Primeiro batch obtido.\n",
      "   ‚úÖ Primeiro batch tem 4 imagens\n",
      "   üîç Processando primeiro batch: 4 imagens\n",
      "   üîç Imagens movidas para cuda, shape: [torch.Size([3, 640, 640]), torch.Size([3, 640, 640])]\n",
      "   üîç Iniciando forward pass...\n",
      "   ‚úÖ Forward pass conclu√≠do, loss: 1.4733\n",
      "   üì¶ [  1/375] |   0.3% | Loss: 1.4733 | Avg: 1.4733 | Tempo: 2.43s | Restante: ~15m8s | üïê 09:47:41\n",
      "   üì¶ [  2/375] |   0.5% | Loss: 1.0016 | Avg: 1.2374 | Tempo: 0.82s | Restante: ~10m5s | üïê 09:47:42\n",
      "   üì¶ [  3/375] |   0.8% | Loss: 1.3089 | Avg: 1.2613 | Tempo: 0.17s | Restante: ~7m4s | üïê 09:47:43\n",
      "   üì¶ [  4/375] |   1.1% | Loss: 1.0286 | Avg: 1.2031 | Tempo: 0.50s | Restante: ~6m3s | üïê 09:47:44\n",
      "   üì¶ [  5/375] |   1.3% | Loss: 0.5587 | Avg: 1.0742 | Tempo: 0.51s | Restante: ~5m27s | üïê 09:47:45\n",
      "   üì¶ [  6/375] |   1.6% | Loss: 0.2243 | Avg: 0.9326 | Tempo: 0.14s | Restante: ~4m40s | üïê 09:47:46\n",
      "   üì¶ [  7/375] |   1.9% | Loss: 0.2128 | Avg: 0.8297 | Tempo: 0.39s | Restante: ~4m20s | üïê 09:47:47\n",
      "   üì¶ [  8/375] |   2.1% | Loss: 0.2467 | Avg: 0.7569 | Tempo: 0.51s | Restante: ~4m10s | üïê 09:47:48\n",
      "   üì¶ [  9/375] |   2.4% | Loss: 0.1920 | Avg: 0.6941 | Tempo: 0.28s | Restante: ~3m53s | üïê 09:47:49\n",
      "   üì¶ [ 10/375] |   2.7% | Loss: 0.2421 | Avg: 0.6489 | Tempo: 0.36s | Restante: ~3m42s | üïê 09:47:50\n",
      "   üì¶ [ 11/375] |   2.9% | Loss: 0.1366 | Avg: 0.6023 | Tempo: 0.14s | Restante: ~3m26s | üïê 09:47:50\n",
      "   üì¶ [ 12/375] |   3.2% | Loss: 0.1816 | Avg: 0.5673 | Tempo: 0.37s | Restante: ~3m19s | üïê 09:47:51\n",
      "   üì¶ [ 13/375] |   3.5% | Loss: 0.1703 | Avg: 0.5367 | Tempo: 0.26s | Restante: ~3m11s | üïê 09:47:52\n",
      "   üì¶ [ 14/375] |   3.7% | Loss: 0.2082 | Avg: 0.5133 | Tempo: 0.27s | Restante: ~3m4s | üïê 09:47:52\n",
      "   üì¶ [ 15/375] |   4.0% | Loss: 0.2128 | Avg: 0.4932 | Tempo: 0.51s | Restante: ~3m3s | üïê 09:47:53\n",
      "   üì¶ [ 16/375] |   4.3% | Loss: 0.1846 | Avg: 0.4739 | Tempo: 0.39s | Restante: ~3m0s | üïê 09:47:54\n",
      "   üì¶ [ 17/375] |   4.5% | Loss: 0.1679 | Avg: 0.4559 | Tempo: 0.37s | Restante: ~2m57s | üïê 09:47:55\n",
      "   üì¶ [ 18/375] |   4.8% | Loss: 0.1580 | Avg: 0.4394 | Tempo: 0.39s | Restante: ~2m54s | üïê 09:47:56\n",
      "   üì¶ [ 19/375] |   5.1% | Loss: 0.2340 | Avg: 0.4286 | Tempo: 0.26s | Restante: ~2m49s | üïê 09:47:57\n",
      "   üì¶ [ 20/375] |   5.3% | Loss: 0.2494 | Avg: 0.4196 | Tempo: 0.37s | Restante: ~2m47s | üïê 09:47:58\n",
      "   üì¶ [ 21/375] |   5.6% | Loss: 0.1927 | Avg: 0.4088 | Tempo: 0.37s | Restante: ~2m45s | üïê 09:47:59\n",
      "   üì¶ [ 22/375] |   5.9% | Loss: 0.2362 | Avg: 0.4010 | Tempo: 0.43s | Restante: ~2m43s | üïê 09:48:00\n",
      "   üì¶ [ 23/375] |   6.1% | Loss: 0.2005 | Avg: 0.3922 | Tempo: 0.52s | Restante: ~2m44s | üïê 09:48:01\n",
      "   üì¶ [ 24/375] |   6.4% | Loss: 0.2020 | Avg: 0.3843 | Tempo: 0.38s | Restante: ~2m42s | üïê 09:48:02\n",
      "   üì¶ [ 25/375] |   6.7% | Loss: 0.1424 | Avg: 0.3746 | Tempo: 0.38s | Restante: ~2m40s | üïê 09:48:03\n",
      "   üì¶ [ 26/375] |   6.9% | Loss: 0.1870 | Avg: 0.3674 | Tempo: 0.37s | Restante: ~2m39s | üïê 09:48:04\n",
      "   üì¶ [ 27/375] |   7.2% | Loss: 0.4152 | Avg: 0.3692 | Tempo: 0.26s | Restante: ~2m36s | üïê 09:48:05\n",
      "   üì¶ [ 28/375] |   7.5% | Loss: 0.1349 | Avg: 0.3608 | Tempo: 0.36s | Restante: ~2m34s | üïê 09:48:06\n",
      "   üì¶ [ 29/375] |   7.7% | Loss: 0.9779 | Avg: 0.3821 | Tempo: 0.37s | Restante: ~2m33s | üïê 09:48:07\n",
      "   üì¶ [ 30/375] |   8.0% | Loss: 0.2124 | Avg: 0.3764 | Tempo: 0.37s | Restante: ~2m32s | üïê 09:48:08\n",
      "   üì¶ [ 31/375] |   8.3% | Loss: 0.3131 | Avg: 0.3744 | Tempo: 0.53s | Restante: ~2m32s | üïê 09:48:09\n",
      "   üì¶ [ 32/375] |   8.5% | Loss: 0.1956 | Avg: 0.3688 | Tempo: 0.38s | Restante: ~2m31s | üïê 09:48:10\n",
      "   üì¶ [ 33/375] |   8.8% | Loss: 0.2772 | Avg: 0.3660 | Tempo: 0.51s | Restante: ~2m31s | üïê 09:48:11\n",
      "   üì¶ [ 34/375] |   9.1% | Loss: 0.2404 | Avg: 0.3623 | Tempo: 0.51s | Restante: ~2m31s | üïê 09:48:12\n",
      "   üì¶ [ 35/375] |   9.3% | Loss: 0.1941 | Avg: 0.3575 | Tempo: 0.52s | Restante: ~2m32s | üïê 09:48:13\n",
      "   üì¶ [ 36/375] |   9.6% | Loss: 0.8258 | Avg: 0.3705 | Tempo: 0.50s | Restante: ~2m32s | üïê 09:48:15\n",
      "   üì¶ [ 37/375] |   9.9% | Loss: 0.2018 | Avg: 0.3660 | Tempo: 0.26s | Restante: ~2m30s | üïê 09:48:15\n",
      "   üì¶ [ 38/375] |  10.1% | Loss: 0.2754 | Avg: 0.3636 | Tempo: 0.51s | Restante: ~2m30s | üïê 09:48:17\n",
      "   üì¶ [ 39/375] |  10.4% | Loss: 0.1723 | Avg: 0.3587 | Tempo: 0.36s | Restante: ~2m29s | üïê 09:48:18\n",
      "   üì¶ [ 40/375] |  10.7% | Loss: 0.1342 | Avg: 0.3531 | Tempo: 0.50s | Restante: ~2m29s | üïê 09:48:19\n",
      "   üì¶ [ 41/375] |  10.9% | Loss: 0.1523 | Avg: 0.3482 | Tempo: 0.38s | Restante: ~2m28s | üïê 09:48:20\n",
      "   üì¶ [ 42/375] |  11.2% | Loss: 0.1815 | Avg: 0.3442 | Tempo: 0.14s | Restante: ~2m25s | üïê 09:48:20\n",
      "   üì¶ [ 43/375] |  11.5% | Loss: 0.2052 | Avg: 0.3410 | Tempo: 0.50s | Restante: ~2m25s | üïê 09:48:21\n",
      "   üì¶ [ 44/375] |  11.7% | Loss: 0.1943 | Avg: 0.3376 | Tempo: 0.39s | Restante: ~2m24s | üïê 09:48:22\n",
      "   üì¶ [ 45/375] |  12.0% | Loss: 0.2839 | Avg: 0.3365 | Tempo: 0.26s | Restante: ~2m22s | üïê 09:48:23\n",
      "   üì¶ [ 46/375] |  12.3% | Loss: 0.2670 | Avg: 0.3349 | Tempo: 0.39s | Restante: ~2m22s | üïê 09:48:24\n",
      "   üì¶ [ 47/375] |  12.5% | Loss: 0.1960 | Avg: 0.3320 | Tempo: 0.26s | Restante: ~2m20s | üïê 09:48:25\n",
      "   üì¶ [ 48/375] |  12.8% | Loss: 0.1571 | Avg: 0.3283 | Tempo: 0.26s | Restante: ~2m18s | üïê 09:48:25\n",
      "   üì¶ [ 49/375] |  13.1% | Loss: 0.1427 | Avg: 0.3246 | Tempo: 0.27s | Restante: ~2m17s | üïê 09:48:26\n",
      "   üì¶ [ 50/375] |  13.3% | Loss: 0.2590 | Avg: 0.3232 | Tempo: 0.51s | Restante: ~2m17s | üïê 09:48:27\n",
      "   üì¶ [ 51/375] |  13.6% | Loss: 0.1663 | Avg: 0.3202 | Tempo: 0.37s | Restante: ~2m16s | üïê 09:48:28\n",
      "   üì¶ [ 52/375] |  13.9% | Loss: 0.1307 | Avg: 0.3165 | Tempo: 0.27s | Restante: ~2m15s | üïê 09:48:29\n",
      "   üì¶ [ 53/375] |  14.1% | Loss: 0.2070 | Avg: 0.3145 | Tempo: 0.50s | Restante: ~2m15s | üïê 09:48:30\n",
      "   üì¶ [ 54/375] |  14.4% | Loss: 0.1758 | Avg: 0.3119 | Tempo: 0.39s | Restante: ~2m14s | üïê 09:48:31\n",
      "   üì¶ [ 55/375] |  14.7% | Loss: 0.1641 | Avg: 0.3092 | Tempo: 0.41s | Restante: ~2m14s | üïê 09:48:32\n",
      "   üì¶ [ 56/375] |  14.9% | Loss: 0.1927 | Avg: 0.3071 | Tempo: 0.36s | Restante: ~2m13s | üïê 09:48:33\n",
      "   üì¶ [ 57/375] |  15.2% | Loss: 0.2070 | Avg: 0.3054 | Tempo: 0.26s | Restante: ~2m12s | üïê 09:48:34\n",
      "   üì¶ [ 58/375] |  15.5% | Loss: 0.1391 | Avg: 0.3025 | Tempo: 0.26s | Restante: ~2m11s | üïê 09:48:34\n",
      "   üì¶ [ 59/375] |  15.7% | Loss: 0.1488 | Avg: 0.2999 | Tempo: 0.50s | Restante: ~2m11s | üïê 09:48:36\n",
      "   üì¶ [ 60/375] |  16.0% | Loss: 0.1566 | Avg: 0.2975 | Tempo: 0.29s | Restante: ~2m10s | üïê 09:48:36\n",
      "   üì¶ [ 61/375] |  16.3% | Loss: 0.4687 | Avg: 0.3003 | Tempo: 0.37s | Restante: ~2m9s | üïê 09:48:37\n",
      "   üì¶ [ 62/375] |  16.5% | Loss: 0.1697 | Avg: 0.2982 | Tempo: 0.36s | Restante: ~2m8s | üïê 09:48:38\n",
      "   üì¶ [ 63/375] |  16.8% | Loss: 0.1363 | Avg: 0.2956 | Tempo: 0.49s | Restante: ~2m8s | üïê 09:48:39\n",
      "   üì¶ [ 64/375] |  17.1% | Loss: 0.3410 | Avg: 0.2963 | Tempo: 0.36s | Restante: ~2m8s | üïê 09:48:40\n",
      "   üì¶ [ 65/375] |  17.3% | Loss: 0.1908 | Avg: 0.2947 | Tempo: 0.26s | Restante: ~2m6s | üïê 09:48:41\n",
      "   üì¶ [ 66/375] |  17.6% | Loss: 0.1786 | Avg: 0.2930 | Tempo: 0.34s | Restante: ~2m6s | üïê 09:48:42\n",
      "   üì¶ [ 67/375] |  17.9% | Loss: 0.1885 | Avg: 0.2914 | Tempo: 0.50s | Restante: ~2m6s | üïê 09:48:43\n",
      "   üì¶ [ 68/375] |  18.1% | Loss: 0.2414 | Avg: 0.2907 | Tempo: 0.26s | Restante: ~2m5s | üïê 09:48:44\n",
      "   üì¶ [ 69/375] |  18.4% | Loss: 0.1279 | Avg: 0.2883 | Tempo: 0.52s | Restante: ~2m5s | üïê 09:48:45\n",
      "   üì¶ [ 70/375] |  18.7% | Loss: 0.1863 | Avg: 0.2869 | Tempo: 0.51s | Restante: ~2m5s | üïê 09:48:46\n",
      "   üì¶ [ 71/375] |  18.9% | Loss: 0.1256 | Avg: 0.2846 | Tempo: 0.27s | Restante: ~2m4s | üïê 09:48:47\n",
      "   üì¶ [ 72/375] |  19.2% | Loss: 0.2186 | Avg: 0.2837 | Tempo: 0.36s | Restante: ~2m3s | üïê 09:48:48\n",
      "   üì¶ [ 73/375] |  19.5% | Loss: 0.1439 | Avg: 0.2817 | Tempo: 0.37s | Restante: ~2m2s | üïê 09:48:49\n",
      "   üì¶ [ 74/375] |  19.7% | Loss: 0.2738 | Avg: 0.2816 | Tempo: 0.51s | Restante: ~2m3s | üïê 09:48:50\n",
      "   üì¶ [ 75/375] |  20.0% | Loss: 0.1969 | Avg: 0.2805 | Tempo: 0.25s | Restante: ~2m1s | üïê 09:48:51\n",
      "   üì¶ [ 76/375] |  20.3% | Loss: 0.2198 | Avg: 0.2797 | Tempo: 0.37s | Restante: ~2m1s | üïê 09:48:52\n",
      "   üì¶ [ 77/375] |  20.5% | Loss: 0.1408 | Avg: 0.2779 | Tempo: 0.14s | Restante: ~1m59s | üïê 09:48:52\n",
      "   üì¶ [ 78/375] |  20.8% | Loss: 0.1823 | Avg: 0.2767 | Tempo: 0.51s | Restante: ~1m59s | üïê 09:48:53\n",
      "   üì¶ [ 79/375] |  21.1% | Loss: 0.3127 | Avg: 0.2771 | Tempo: 0.52s | Restante: ~2m0s | üïê 09:48:55\n",
      "   üì¶ [ 80/375] |  21.3% | Loss: 0.2241 | Avg: 0.2765 | Tempo: 0.54s | Restante: ~2m0s | üïê 09:48:56\n",
      "   üì¶ [ 81/375] |  21.6% | Loss: 0.1892 | Avg: 0.2754 | Tempo: 0.37s | Restante: ~1m59s | üïê 09:48:57\n",
      "   üì¶ [ 82/375] |  21.9% | Loss: 0.2629 | Avg: 0.2752 | Tempo: 0.28s | Restante: ~1m58s | üïê 09:48:57\n",
      "   üì¶ [ 83/375] |  22.1% | Loss: 0.3442 | Avg: 0.2761 | Tempo: 0.26s | Restante: ~1m57s | üïê 09:48:58\n",
      "   üì¶ [ 84/375] |  22.4% | Loss: 0.2399 | Avg: 0.2756 | Tempo: 0.53s | Restante: ~1m57s | üïê 09:48:59\n",
      "   üì¶ [ 85/375] |  22.7% | Loss: 0.2806 | Avg: 0.2757 | Tempo: 0.40s | Restante: ~1m57s | üïê 09:49:00\n",
      "   üì¶ [ 86/375] |  22.9% | Loss: 0.1638 | Avg: 0.2744 | Tempo: 0.41s | Restante: ~1m57s | üïê 09:49:01\n",
      "   üì¶ [ 87/375] |  23.2% | Loss: 0.1884 | Avg: 0.2734 | Tempo: 0.51s | Restante: ~1m56s | üïê 09:49:03\n",
      "   üì¶ [ 88/375] |  23.5% | Loss: 0.4208 | Avg: 0.2751 | Tempo: 0.26s | Restante: ~1m56s | üïê 09:49:03\n",
      "   üì¶ [ 89/375] |  23.7% | Loss: 0.2187 | Avg: 0.2745 | Tempo: 0.27s | Restante: ~1m55s | üïê 09:49:04\n",
      "   üì¶ [ 90/375] |  24.0% | Loss: 0.3006 | Avg: 0.2747 | Tempo: 0.38s | Restante: ~1m54s | üïê 09:49:05\n",
      "   üì¶ [ 91/375] |  24.3% | Loss: 0.1245 | Avg: 0.2731 | Tempo: 0.56s | Restante: ~1m54s | üïê 09:49:07\n",
      "   üì¶ [ 92/375] |  24.5% | Loss: 0.1996 | Avg: 0.2723 | Tempo: 0.39s | Restante: ~1m54s | üïê 09:49:08\n",
      "   üì¶ [ 93/375] |  24.8% | Loss: 0.2391 | Avg: 0.2719 | Tempo: 0.27s | Restante: ~1m53s | üïê 09:49:08\n",
      "   üì¶ [ 94/375] |  25.1% | Loss: 0.1760 | Avg: 0.2709 | Tempo: 0.50s | Restante: ~1m53s | üïê 09:49:10\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LOOP DE TREINAMENTO\n",
    "# ================================================================\n",
    "\n",
    "# Criar diret√≥rio para salvar modelo\n",
    "save_dir = \"runs/aula9_coco_gun\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Hist√≥rico de treinamento\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"ar_all_100\": []\n",
    "}\n",
    "\n",
    "# Melhor mAP (usaremos mAP@0.5:0.95 como crit√©rio)\n",
    "best_map = 0.0\n",
    "\n",
    "print(f\"\\n{'='*60}\", flush=True)\n",
    "print(f\"üéØ INICIANDO TREINAMENTO\", flush=True)\n",
    "print(f\"{'='*60}\", flush=True)\n",
    "print(f\"   üìÇ Diret√≥rio de salvamento: {save_dir}\", flush=True)\n",
    "print(f\"   üìä Total de √©pocas: {NUM_EPOCHS}\", flush=True)\n",
    "print(f\"   üì∏ Imagens por √©poca: {len(train_dataset)}\", flush=True)\n",
    "print(f\"   üì¶ Batches por √©poca: {len(train_loader)}\", flush=True)\n",
    "print(f\"{'='*60}\\n\", flush=True)\n",
    "\n",
    "# Loop de treinamento\n",
    "print(f\"üîµ Iniciando loop de treinamento...\", flush=True)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nüîµ === IN√çCIO √âPOCA {epoch+1}/{NUM_EPOCHS} ===\", flush=True)\n",
    "    # Treinar uma √©poca\n",
    "    avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, NUM_EPOCHS)\n",
    "    print(f\"üîµ === FIM √âPOCA {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f} ===\", flush=True)\n",
    "\n",
    "    # Atualizar learning rate\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # ================================================================\n",
    "    # AVALIA√á√ÉO COCO NO DATASET VAL AP√ìS CADA √âPOCA\n",
    "    # ================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä AVALIANDO MODELO NO DATASET VAL - √âPOCA {epoch+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Avaliar no dataset de VALIDA√á√ÉO\n",
    "    ann_path = os.path.join(root_dir, \"coco_annotations_val.json\")\n",
    "    metrics = evaluate_coco_completo(\n",
    "        model,\n",
    "        val_dataset,\n",
    "        ann_path,\n",
    "        device=device,\n",
    "        score_threshold=0.3,\n",
    "        valid_indices=valid_indices\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DA √âPOCA {epoch+1} (VAL):\")\n",
    "    print(f\"   ‚úÖ mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "    print(f\"   ‚úÖ mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "    print(f\"   ‚úÖ mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "    print(f\"   ‚úÖ AR@100:       {metrics['ar_all_100']:.4f} ({metrics['ar_all_100']*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Salvar hist√≥rico\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    history[\"map50_95\"].append(metrics['map50_95'])\n",
    "    history[\"map50\"].append(metrics['map50'])\n",
    "    history[\"map75\"].append(metrics['map75'])\n",
    "    history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "    \n",
    "    # Salvar melhor modelo (baseado em mAP@0.5:0.95)\n",
    "    current_map = metrics['map50_95']\n",
    "    if current_map > best_map:\n",
    "        best_map = current_map\n",
    "        model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"\\n   üíæ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "        print(f\"      üìÇ Caminho: {model_path}\")\n",
    "    \n",
    "    # Salvar checkpoint\n",
    "    checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'history': history\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"   üíæ Checkpoint salvo: {checkpoint_path}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Salvar hist√≥rico final\n",
    "history_path = os.path.join(save_dir, \"history.json\")\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   üìä Total de √©pocas treinadas: {NUM_EPOCHS}\")\n",
    "print(f\"   üìä Melhor mAP@0.5:0.95: {best_map:.4f}\")\n",
    "print(f\"   üìÇ Modelo salvo em: {os.path.join(save_dir, 'best_model.pth')}\")\n",
    "print(f\"   üìÇ Hist√≥rico salvo em: {history_path}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VISUALIZAR HIST√ìRICO DE TREINAMENTO\n",
    "# ================================================================\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, \"history.json\")):\n",
    "    with open(os.path.join(save_dir, \"history.json\"), 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "        # Criar figura com m√∫ltiplos subplots\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train Loss', marker='o')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Learning Rate\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history[\"epoch\"], history[\"lr\"], 'r-', linewidth=2, label='Learning Rate', marker='o')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: mAP principais\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=2, label='mAP@0.5:0.95', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=2, label='mAP@0.5', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"map75\"], 'r-', linewidth=2, label='mAP@0.75', marker='^')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.title('Average Precision (AP) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: mAP@0.5:0.95 (principal)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=3, label='mAP@0.5:0.95', marker='o', markersize=8)\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('mAP@0.5:0.95')\n",
    "    plt.title('mAP@0.5:0.95 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 5: mAP@0.5\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=3, label='mAP@0.5', marker='s', markersize=8)\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('mAP@0.5')\n",
    "    plt.title('mAP@0.5 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 6: AR@100\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_100\"], 'm-', linewidth=3, label='AR@100', marker='d', markersize=8)\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('AR@100')\n",
    "    plt.title('Average Recall (AR@100) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_history.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"‚úÖ Gr√°ficos salvos em: {os.path.join(save_dir, 'training_history.png')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Hist√≥rico n√£o encontrado. Execute o treinamento primeiro.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# C√ìDIGO MODIFICADO PARA O LOOP DE TREINAMENTO\n",
    "# ================================================================\n",
    "# SUBSTITUA O HIST√ìRICO E O LOOP DE TREINAMENTO NA C√âLULA 9 POR ESTE C√ìDIGO\n",
    "\n",
    "# Hist√≥rico de treinamento (MODIFICADO - adicionar todas as m√©tricas)\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"map50_95_small\": [],\n",
    "    \"map50_95_medium\": [],\n",
    "    \"map50_95_large\": [],\n",
    "    \"ar_all_1\": [],\n",
    "    \"ar_all_10\": [],\n",
    "    \"ar_all_100\": [],\n",
    "    \"ar_small\": [],\n",
    "    \"ar_medium\": [],\n",
    "    \"ar_large\": []\n",
    "}\n",
    "\n",
    "# Melhor mAP (usaremos mAP@0.5:0.95 como crit√©rio)\n",
    "best_map = 0.0\n",
    "\n",
    "print(\"‚úÖ Hist√≥rico modificado para incluir todas as m√©tricas COCO\")\n",
    "print(\"   üìù Copie este c√≥digo para substituir o hist√≥rico na c√©lula 9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìä AVALIANDO MODELO NO DATASET DE TESTE {epoch+1}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Avaliar no dataset de teste\n",
    "ann_path = os.path.join(root_dir, \"coco_annotations_test.json\")\n",
    "metrics = evaluate_coco_completo( \n",
    "    model,\n",
    "    test_dataset,\n",
    "    ann_path,\n",
    "    device=device,\n",
    "    score_threshold=0.3,\n",
    "    valid_indices=valid_indices\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS DA √âPOCA {epoch+1}:\")\n",
    "print(f\"   ‚úÖ mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "print(f\"   ‚úÖ mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "print(f\"   ‚úÖ mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Salvar hist√≥rico (ADICIONAR TODAS AS M√âTRICAS)\n",
    "history[\"epoch\"].append(epoch + 1)\n",
    "history[\"train_loss\"].append(avg_loss)\n",
    "history[\"lr\"].append(current_lr)\n",
    "history[\"map50_95\"].append(metrics['map50_95'])\n",
    "history[\"map50\"].append(metrics['map50'])\n",
    "history[\"map75\"].append(metrics['map75'])\n",
    "history[\"map50_95_small\"].append(metrics['map50_95_small'])\n",
    "history[\"map50_95_medium\"].append(metrics['map50_95_medium'])\n",
    "history[\"map50_95_large\"].append(metrics['map50_95_large'])\n",
    "history[\"ar_all_1\"].append(metrics['ar_all_1'])\n",
    "history[\"ar_all_10\"].append(metrics['ar_all_10'])\n",
    "history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "history[\"ar_small\"].append(metrics['ar_small'])\n",
    "history[\"ar_medium\"].append(metrics['ar_medium'])\n",
    "history[\"ar_large\"].append(metrics['ar_large'])\n",
    "\n",
    "# Salvar melhor modelo (baseado em mAP@0.5:0.95)\n",
    "current_map = metrics['map50_95']\n",
    "if current_map > best_map:\n",
    "    best_map = current_map\n",
    "    model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\n   üíæ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "    print(f\"      üìÇ Caminho: {model_path}\")\n",
    "\n",
    "print(\"‚úÖ C√≥digo de avalia√ß√£o durante treinamento criado!\")\n",
    "print(\"   üìù Adicione este c√≥digo no loop de treinamento (c√©lula 9) ap√≥s atualizar o learning rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VISUALIZA√á√ÉO DE PREDI√á√ïES E C√ÅLCULO DE TEMPO DE INFER√äNCIA\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Desnormaliza imagem tensor para visualiza√ß√£o\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_images=6, score_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Visualiza predi√ß√µes do modelo comparando com ground truth\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo treinado\n",
    "        dataset: Dataset de teste\n",
    "        device: Device (cuda/cpu)\n",
    "        num_images: N√∫mero de imagens para visualizar\n",
    "        score_threshold: Threshold m√≠nimo de confian√ßa\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Selecionar √≠ndices aleat√≥rios\n",
    "    valid_indices = [i for i in range(len(dataset)) if dataset[i] is not None]\n",
    "    selected_indices = np.random.choice(valid_indices, min(num_images, len(valid_indices)), replace=False)\n",
    "    \n",
    "    # Lista para armazenar tempos de infer√™ncia\n",
    "    inference_times = []\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4*num_images))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(selected_indices):\n",
    "        # Carregar imagem e ground truth\n",
    "        image_tensor, target = dataset[sample_idx]\n",
    "        \n",
    "        # Desnormalizar imagem para visualiza√ß√£o\n",
    "        image_np = denormalize_image(image_tensor).permute(1, 2, 0).cpu().numpy()\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "        \n",
    "        # Ground truth boxes\n",
    "        gt_boxes = target['boxes'].cpu().numpy()\n",
    "        gt_labels = target['labels'].cpu().numpy()\n",
    "        \n",
    "        # Fazer predi√ß√£o e medir tempo\n",
    "        image_batch = image_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_batch)[0]\n",
    "        inference_time = time.time() - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        \n",
    "        # Filtrar predi√ß√µes por score threshold\n",
    "        pred_scores = predictions['scores'].cpu().numpy()\n",
    "        pred_boxes = predictions['boxes'].cpu().numpy()\n",
    "        pred_labels = predictions['labels'].cpu().numpy()\n",
    "        \n",
    "        mask = pred_scores >= score_threshold\n",
    "        pred_boxes = pred_boxes[mask]\n",
    "        pred_scores = pred_scores[mask]\n",
    "        pred_labels = pred_labels[mask]\n",
    "        \n",
    "        # Plot Ground Truth (esquerda)\n",
    "        ax_gt = axes[idx, 0]\n",
    "        ax_gt.imshow(image_np)\n",
    "        ax_gt.set_title(f'Ground Truth (Imagem {sample_idx})', fontsize=12, fontweight='bold')\n",
    "        ax_gt.axis('off')\n",
    "        \n",
    "        # Desenhar caixas do ground truth (verde)\n",
    "        for box, label in zip(gt_boxes, gt_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height,\n",
    "                linewidth=2, edgecolor='green', facecolor='none'\n",
    "            )\n",
    "            ax_gt.add_patch(rect)\n",
    "            ax_gt.text(x1, y1-5, 'Gun (GT)', color='green', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Plot Predi√ß√µes (direita)\n",
    "        ax_pred = axes[idx, 1]\n",
    "        ax_pred.imshow(image_np)\n",
    "        ax_pred.set_title(f'Predi√ß√µes (Score‚â•{score_threshold}) - {inference_time*1000:.1f}ms', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        ax_pred.axis('off')\n",
    "        \n",
    "        # Desenhar caixas das predi√ß√µes (vermelho)\n",
    "        for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax_pred.add_patch(rect)\n",
    "            ax_pred.text(x1, y1-5, f'Gun {score:.2f}', color='red', fontsize=10, fontweight='bold',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Adicionar contadores\n",
    "        ax_gt.text(0.02, 0.98, f'GT: {len(gt_boxes)} objetos', \n",
    "                  transform=ax_gt.transAxes, fontsize=10, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
    "        ax_pred.text(0.02, 0.98, f'Pred: {len(pred_boxes)} objetos', \n",
    "                    transform=ax_pred.transAxes, fontsize=10, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular estat√≠sticas de tempo\n",
    "    avg_time = np.mean(inference_times)\n",
    "    std_time = np.std(inference_times)\n",
    "    min_time = np.min(inference_times)\n",
    "    max_time = np.max(inference_times)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚è±Ô∏è  ESTAT√çSTICAS DE TEMPO DE INFER√äNCIA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   üìä N√∫mero de imagens testadas: {len(inference_times)}\")\n",
    "    print(f\"   ‚è±Ô∏è  Tempo m√©dio: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"   üìà Desvio padr√£o: {std_time*1000:.2f} ms\")\n",
    "    print(f\"   ‚¨áÔ∏è  Tempo m√≠nimo: {min_time*1000:.2f} ms\")\n",
    "    print(f\"   ‚¨ÜÔ∏è  Tempo m√°ximo: {max_time*1000:.2f} ms\")\n",
    "    print(f\"   üöÄ FPS m√©dio: {1/avg_time:.2f} frames/segundo\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return inference_times\n",
    "\n",
    "# Carregar melhor modelo se ainda n√£o estiver carregado\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"üìÇ Carregando melhor modelo...\")\n",
    "    # Verificar se save_dir est√° definida\n",
    "    if 'save_dir' in locals():\n",
    "        model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    else:\n",
    "        # Tentar caminho padr√£o\n",
    "        model_path = \"runs/aula9_coco_gun/best_model.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))\n",
    "        print(f\"‚úÖ Modelo carregado de: {model_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Modelo n√£o encontrado em: {model_path}\")\n",
    "        print(\"   Usando modelo atual (se dispon√≠vel)\")\n",
    "\n",
    "# Garantir que o modelo est√° no device correto\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üé® VISUALIZANDO PREDI√á√ïES DO MODELO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   üì∏ N√∫mero de imagens: 6\")\n",
    "print(f\"   üéØ Score threshold: 0.3\")\n",
    "print(f\"   üíª Device: {device}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Visualizar predi√ß√µes\n",
    "inference_times = visualize_predictions(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    device=device,\n",
    "    num_images=6,\n",
    "    score_threshold=0.3\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
