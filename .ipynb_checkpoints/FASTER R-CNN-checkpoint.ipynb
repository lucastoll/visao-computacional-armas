{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 6244,
     "status": "ok",
     "timestamp": 1763253439528,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "qEsQBCPL4j6f"
   },
   "source": [
    "<div style='background-color: #f0f8ff; border: 3px solid Blue;'>\n",
    "    <font size=\"+1\" color=\"Blue\">\n",
    "        <b>üöÄ Notebook de Treinamento Faster R-CNN para Detec√ß√£o de Armas</b>\n",
    "    </font>\n",
    "</div>\n",
    "<div style='background-color: #fff7f7; border: 2px solid Green;'>\n",
    "    <font size=\"+1\" color=\"Green\">\n",
    "        <b>‚úÖ Dataset configurado: dataset_final_coco</b>\n",
    "    </font>\n",
    "</div>\n",
    "<div style='background-color: #fffacd; border: 2px solid Orange;'>\n",
    "    <font size=\"+1\" color=\"Orange\">\n",
    "        <b>üì¶ Modelo: Faster R-CNN com ResNet-50 FPN</b>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102310,
     "status": "ok",
     "timestamp": 1763253541847,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "rqNnp8_Ov8zA",
    "outputId": "4ba8a921-34e7-47bc-de44-affe02a0834d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Instalando depend√™ncias...\n",
      "‚úÖ pycocotools instalado com sucesso!\n",
      "‚úÖ albumentations instalado com sucesso!\n",
      "‚úÖ seaborn instalado com sucesso!\n",
      "‚úÖ albumentations importado com sucesso! Vers√£o: 2.0.8\n",
      "‚úÖ seaborn importado com sucesso! Vers√£o: 0.13.2\n",
      "üîç Verificando GPU...\n",
      "‚úÖ GPU detectada: NVIDIA GeForce GTX 1060 6GB\n",
      "‚úÖ CUDA Version: 12.1\n",
      "\n",
      "üìÅ Diret√≥rio base: D:\\Desktop\\Projetos\\visao\n",
      "üìÅ Dataset: D:\\Desktop\\Projetos\\visao\\dataset_final_coco\n",
      "‚úÖ Dataset encontrado!\n",
      "üìä Imagens de treino: 15015\n",
      "üìä Imagens de valida√ß√£o: 4290\n",
      "üìä Imagens de teste: 2146\n",
      "‚úÖ Anota√ß√µes de treino encontradas: D:\\Desktop\\Projetos\\visao\\dataset_final_coco\\coco_annotations_train.json\n",
      "‚úÖ Anota√ß√µes de valida√ß√£o encontradas: D:\\Desktop\\Projetos\\visao\\dataset_final_coco\\coco_annotations_val.json\n",
      "‚úÖ Anota√ß√µes de teste encontradas: D:\\Desktop\\Projetos\\visao\\dataset_final_coco\\coco_annotations_test.json\n"
     ]
    }
   ],
   "source": [
    "# Instala depend√™ncias necess√°rias\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Instala um pacote usando pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--upgrade\"])\n",
    "        print(f\"‚úÖ {package} instalado com sucesso!\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Erro ao instalar {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Instala pacotes necess√°rios\n",
    "print(\"üì¶ Instalando depend√™ncias...\")\n",
    "install_package(\"pycocotools\")\n",
    "install_package(\"albumentations\")\n",
    "install_package(\"seaborn\")\n",
    "\n",
    "# Verifica se os pacotes foram instalados corretamente\n",
    "try:\n",
    "    import albumentations as A\n",
    "    print(f\"‚úÖ albumentations importado com sucesso! Vers√£o: {A.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERRO: N√£o foi poss√≠vel importar albumentations: {e}\")\n",
    "    print(\"üí° Tente reiniciar o kernel ap√≥s a instala√ß√£o!\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(f\"‚úÖ seaborn importado com sucesso! Vers√£o: {sns.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERRO: N√£o foi poss√≠vel importar seaborn: {e}\")\n",
    "    print(\"üí° Tente reiniciar o kernel ap√≥s a instala√ß√£o!\")\n",
    "    raise\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "# Verifica GPU\n",
    "print(\"üîç Verificando GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU n√£o dispon√≠vel - usando CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Define o diret√≥rio base e caminhos do dataset\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"dataset_final_coco\")\n",
    "root_dir = DATASET_DIR\n",
    "\n",
    "print(f\"\\nüìÅ Diret√≥rio base: {BASE_DIR}\")\n",
    "print(f\"üìÅ Dataset: {DATASET_DIR}\")\n",
    "\n",
    "# Verifica se o dataset existe\n",
    "if os.path.exists(DATASET_DIR):\n",
    "    print(\"‚úÖ Dataset encontrado!\")\n",
    "    \n",
    "    # Verifica estrutura do dataset\n",
    "    train_images_path = os.path.join(DATASET_DIR, \"train\", \"images\")\n",
    "    val_images_path = os.path.join(DATASET_DIR, \"val\", \"images\")\n",
    "    test_images_path = os.path.join(DATASET_DIR, \"test\", \"images\")\n",
    "    \n",
    "    train_images = len([f for f in os.listdir(train_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(train_images_path) else 0\n",
    "    val_images = len([f for f in os.listdir(val_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(val_images_path) else 0\n",
    "    test_images = len([f for f in os.listdir(test_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(test_images_path) else 0\n",
    "    \n",
    "    print(f\"üìä Imagens de treino: {train_images}\")\n",
    "    print(f\"üìä Imagens de valida√ß√£o: {val_images}\")\n",
    "    print(f\"üìä Imagens de teste: {test_images}\")\n",
    "    \n",
    "    # Verifica arquivos de anota√ß√£o COCO\n",
    "    train_ann = os.path.join(DATASET_DIR, \"coco_annotations_train.json\")\n",
    "    val_ann = os.path.join(DATASET_DIR, \"coco_annotations_val.json\")\n",
    "    test_ann = os.path.join(DATASET_DIR, \"coco_annotations_test.json\")\n",
    "    \n",
    "    if os.path.exists(train_ann):\n",
    "        print(f\"‚úÖ Anota√ß√µes de treino encontradas: {train_ann}\")\n",
    "    if os.path.exists(val_ann):\n",
    "        print(f\"‚úÖ Anota√ß√µes de valida√ß√£o encontradas: {val_ann}\")\n",
    "    if os.path.exists(test_ann):\n",
    "        print(f\"‚úÖ Anota√ß√µes de teste encontradas: {test_ann}\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset n√£o encontrado! Verifique o caminho.\")\n",
    "    print(f\"   Caminho esperado: {DATASET_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1526,
     "status": "ok",
     "timestamp": 1763253543383,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "fkLOOU-34F8z"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ------------------------------\n",
    "# TRAIN TRANSFORMS (fortes)\n",
    "# ------------------------------\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=0.015,\n",
    "                sat_shift_limit=0.7,\n",
    "                val_shift_limit=0.4,\n",
    "                p=1.0\n",
    "            ),\n",
    "\n",
    "            A.Rotate(limit=30, border_mode=0, p=1.0),\n",
    "\n",
    "            A.Affine(\n",
    "                translate_percent=0.1,\n",
    "                scale=(0.5, 1.5),\n",
    "                shear=(-5, 5),\n",
    "                p=1.0\n",
    "            ),\n",
    "\n",
    "            A.Perspective(scale=(0.0005, 0.0005), p=1.0),\n",
    "\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.RandomCrop(height=416, width=416, p=1.0, pad_if_needed=True),\n",
    "            A.Resize(height=512, width=512, p=1.0),  # Reduzido de 640 para 512\n",
    "\n",
    "            A.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "\n",
    "            ToTensorV2()\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\",\n",
    "            label_fields=[\"labels\"],\n",
    "            min_visibility=0.2\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# VAL/TEST TRANSFORMS (leves)\n",
    "# ------------------------------\n",
    "def get_eval_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(512, 512),  # Reduzido de 640 para 512\n",
    "            A.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\",\n",
    "            label_fields=[\"labels\"],\n",
    "            min_visibility=0.0\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763253543398,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "pyREej1016ZH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class CocoAlbumentationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None):\n",
    "        import json\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "        # agrupar anota√ß√µes por imagem\n",
    "        self.annotations = {}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        info = self.images[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, info[\"file_name\"])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        annots = self.annotations.get(img_id, [])\n",
    "\n",
    "        # se n√£o tem boxes ‚Üí pula imagem\n",
    "        if len(annots) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "\n",
    "        # üö® Prote√ß√£o contra augmentations removerem TODAS as caixas\n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1763253543410,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "ZjJplR4N2E8V"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1763253543690,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "IedNChtP4Fvz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset completo de treino: 15007 imagens\n",
      "üìä Dataset completo de valida√ß√£o: 4287 imagens\n",
      "\n",
      "‚úÖ Dataset reduzido:\n",
      "   üìä Treino: 750 imagens (5% do original)\n",
      "   üìä Valida√ß√£o: 428 imagens (10% do original)\n",
      "   üìä Batch size: 32\n",
      "\n",
      "üìä Batches por √©poca:\n",
      "   üéØ Treino: ~24 batches\n",
      "   üéØ Valida√ß√£o: ~14 batches\n",
      "\n",
      "üí° Redu√ß√£o de ~914 batches por √©poca!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# REDU√á√ÉO DRASTICA DO DATASET PARA ECONOMIZAR MEM√ìRIA\n",
    "# ================================================================\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# Criar datasets completos\n",
    "train_dataset_full = CocoAlbumentationsDataset(\n",
    "    f\"{root_dir}/train/images\",\n",
    "    f\"{root_dir}/coco_annotations_train.json\",\n",
    "    transforms=get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset_full = CocoAlbumentationsDataset(\n",
    "    f\"{root_dir}/val/images\",\n",
    "    f\"{root_dir}/coco_annotations_val.json\",\n",
    "    transforms=get_eval_transforms()\n",
    ")\n",
    "\n",
    "test_dataset = CocoAlbumentationsDataset(\n",
    "    f\"{root_dir}/test/images\",\n",
    "    f\"{root_dir}/coco_annotations_test.json\",\n",
    "    transforms=get_eval_transforms()\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURA√á√ÉO: Reduzir dataset drasticamente\n",
    "# ================================================================\n",
    "TRAIN_SUBSET_PERCENT = 0.05  # Usar apenas 10% dos dados de treino\n",
    "VAL_SUBSET_PERCENT = 0.10    # Usar apenas 20% dos dados de valida√ß√£o\n",
    "BATCH_SIZE = 24              # Reduzido de 16 para 8\n",
    "\n",
    "# Criar subsets reduzidos\n",
    "print(f\"üìä Dataset completo de treino: {len(train_dataset_full)} imagens\")\n",
    "print(f\"üìä Dataset completo de valida√ß√£o: {len(val_dataset_full)} imagens\")\n",
    "\n",
    "# Selecionar √≠ndices aleat√≥rios para o subset\n",
    "train_size = int(len(train_dataset_full) * TRAIN_SUBSET_PERCENT)\n",
    "val_size = int(len(val_dataset_full) * VAL_SUBSET_PERCENT)\n",
    "\n",
    "# Fixar seed para reprodutibilidade\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(train_dataset_full)), train_size)\n",
    "val_indices = random.sample(range(len(val_dataset_full)), val_size)\n",
    "\n",
    "# Criar subsets\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset reduzido:\")\n",
    "print(f\"   üìä Treino: {len(train_dataset)} imagens ({TRAIN_SUBSET_PERCENT*100:.0f}% do original)\")\n",
    "print(f\"   üìä Valida√ß√£o: {len(val_dataset)} imagens ({VAL_SUBSET_PERCENT*100:.0f}% do original)\")\n",
    "print(f\"   üìä Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Criar DataLoaders com batch_size reduzido\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Calcular n√∫mero de batches\n",
    "num_train_batches = len(train_loader)\n",
    "num_val_batches = len(val_loader)\n",
    "\n",
    "print(f\"\\nüìä Batches por √©poca:\")\n",
    "print(f\"   üéØ Treino: ~{num_train_batches} batches\")\n",
    "print(f\"   üéØ Valida√ß√£o: ~{num_val_batches} batches\")\n",
    "print(f\"\\nüí° Redu√ß√£o de ~{938 - num_train_batches} batches por √©poca!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7269,
     "status": "ok",
     "timestamp": 1763253550961,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "_vlvjq1XweXt",
    "outputId": "38a3a092-3c89-49b3-824c-430e54620840"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = 2  # background + gun\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"DEFAULT\",\n",
    "    box_score_thresh=0.5,\n",
    ")\n",
    "\n",
    "# substituir head com dropout\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "    in_features, num_classes\n",
    ")\n",
    "\n",
    "# aplicar dropout manualmente\n",
    "model.roi_heads.box_predictor.cls_score = nn.Sequential(\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# Otimizador\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1.0,\n",
    "    end_factor=0.01,\n",
    "    total_iters=100  # 100 √©pocas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763253550966,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "vTcwm5wOw5X2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "save_dir = \"runs/fasterrcnn/train1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"epoch\": [],\n",
    "    \"map50\": [],\n",
    "    \"map5095\": []\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1763253552091,
     "user": {
      "displayName": "Jo√£o Tagliarini",
      "userId": "11097646579698174582"
     },
     "user_tz": 180
    },
    "id": "enXUlgmTw8-F"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# M√âTRICAS COCO COMPLETAS\n",
    "# ================================================================\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_coco(model, dataset, ann_path, device=\"cuda\"):\n",
    "    model.eval()\n",
    "\n",
    "    coco_gt = COCO(ann_path)\n",
    "    results = []\n",
    "\n",
    "    for img, target in dataset:\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        boxes = output[\"boxes\"].cpu().numpy()\n",
    "        scores = output[\"scores\"].cpu().numpy()\n",
    "        labels = output[\"labels\"].cpu().numpy()\n",
    "\n",
    "        image_id = int(target[\"image_id\"].item())\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": 1,\n",
    "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # mAP50 = COCO metric 1\n",
    "    # mAP50-95 = COCO metric 0\n",
    "    map50 = coco_eval.stats[1]\n",
    "    map5095 = coco_eval.stats[0]\n",
    "\n",
    "    return map50, map5095\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# FUNCTION ‚Äî CONFUSION MATRIX\n",
    "# ================================================================\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
    "    labels = [\"background\", \"Gun\"]\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t][p] += 1\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# FUNCTION ‚Äî PRECISION-RECALL CURVE\n",
    "# ================================================================\n",
    "def plot_pr_curve(precisions, recalls, save_path):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(recalls, precisions)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# FUNCTION ‚Äî F1 CURVE\n",
    "# ================================================================\n",
    "def plot_f1_curve(precisions, recalls, save_path):\n",
    "    f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(recalls, f1)\n",
    "    plt.title(\"F1 Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    plt.grid()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "oL4alx3NwhcS",
    "outputId": "4132dbfc-9e8d-45ec-e8b6-46538f0e7e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Batch 0: 31 imagens\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.18 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.56 GiB is allocated by PyTorch, and 687.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m targets = [{k: v.cuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t.items()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m loss_dict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m losses = \u001b[38;5;28msum\u001b[39m(loss_dict.values())\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# outra linha de LOG opcional:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m     94\u001b[39m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] = boxes[bb_idx].tolist()\n\u001b[32m     95\u001b[39m             torch._assert(\n\u001b[32m     96\u001b[39m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     97\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAll bounding boxes should have positive height and width.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:58\u001b[39m, in \u001b[36mBackboneWithFPN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m     57\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.body(x)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:194\u001b[39m, in \u001b[36mFeaturePyramidNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    192\u001b[39m inner_lateral = \u001b[38;5;28mself\u001b[39m.get_result_from_inner_blocks(x[idx], idx)\n\u001b[32m    193\u001b[39m feat_shape = inner_lateral.shape[-\u001b[32m2\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m inner_top_down = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeat_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnearest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m last_inner = inner_lateral + inner_top_down\n\u001b[32m    196\u001b[39m results.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.get_result_from_layer_blocks(last_inner, idx))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:4536\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[32m   4535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.18 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.56 GiB is allocated by PyTorch, and 687.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience = 20\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "\n",
    "        # üîç LOG DO BATCH\n",
    "        print(f\"[Epoch {epoch}] Batch {batch_idx}: {len(imgs)} imagens\")\n",
    "\n",
    "        # detectar erros comuns\n",
    "        for i, t in enumerate(targets):\n",
    "            if t[\"boxes\"].shape[0] == 0:\n",
    "                print(f\"  ‚ö†Ô∏è  Aten√ß√£o: imagem {i} no batch {batch_idx} est√° com boxes vazios!\")\n",
    "\n",
    "        # GPU\n",
    "        imgs = [img.cuda() for img in imgs]\n",
    "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # forward\n",
    "        loss_dict = model(imgs, targets)\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        # outra linha de LOG opcional:\n",
    "        print(f\"  Loss do batch: {losses.item():.4f}\")\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # =======================================================\n",
    "    # AVALIA√á√ÉO COCO EM VAL\n",
    "    # =======================================================\n",
    "    map50, map5095 = evaluate_coco(\n",
    "        model,\n",
    "        val_dataset,\n",
    "        f\"{root_dir}/annotations/instances_val.json\"\n",
    "    )\n",
    "\n",
    "    print(f\"   mAP50={map50:.4f} | mAP50-95={map5095:.4f}\")\n",
    "\n",
    "    # =======================================================\n",
    "    # Salvar hist√≥rico\n",
    "    # =======================================================\n",
    "    history[\"train_loss\"].append(total_loss)\n",
    "    history[\"map50\"].append(map50)\n",
    "    history[\"map5095\"].append(map5095)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    history[\"epoch\"].append(epoch)\n",
    "\n",
    "    with open(f\"{save_dir}/history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    # =======================================================\n",
    "    # EARLY STOPPING\n",
    "    # =======================================================\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), f\"{save_dir}/best_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"EARLY STOPPING\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# GERAR GR√ÅFICOS AP√ìS O TREINAMENTO\n",
    "# =======================================================\n",
    "\n",
    "# Loss plot\n",
    "plt.figure()\n",
    "plt.plot(history[\"epoch\"], history[\"train_loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.savefig(f\"{save_dir}/losses.png\")\n",
    "plt.close()\n",
    "\n",
    "# mAP plot\n",
    "plt.figure()\n",
    "plt.plot(history[\"epoch\"], history[\"map50\"], label=\"mAP50\")\n",
    "plt.plot(history[\"epoch\"], history[\"map5095\"], label=\"mAP50-95\")\n",
    "plt.legend()\n",
    "plt.title(\"mAP Curve\")\n",
    "plt.savefig(f\"{save_dir}/map_curve.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TboIwUKrxDna"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FINAL EVALUATION FOR PR + F1 + CONFUSION MATRIX\n",
    "# ================================================================\n",
    "\n",
    "all_scores = []\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "model.eval()\n",
    "for img, target in val_dataset:\n",
    "    img_tensor = img.cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(img_tensor)[0]\n",
    "\n",
    "    scores = out[\"scores\"].cpu().numpy()\n",
    "    labels_pred = out[\"labels\"].cpu().numpy()\n",
    "    labels_true = target[\"labels\"].numpy()\n",
    "\n",
    "    # armazenar\n",
    "    all_scores.extend(scores)\n",
    "    all_preds.extend(labels_pred)\n",
    "    all_labels.extend(labels_true)\n",
    "\n",
    "\n",
    "# ORDENAR POR SCORE PARA PR CURVE\n",
    "order = np.argsort(-np.array(all_scores))\n",
    "preds = np.array(all_preds)[order]\n",
    "labels = np.array(all_labels)[order]\n",
    "\n",
    "tp = (preds == labels)\n",
    "fp = (preds != labels)\n",
    "fn = (labels != preds)\n",
    "\n",
    "precision = np.cumsum(tp) / (np.cumsum(tp + fp) + 1e-6)\n",
    "recall = np.cumsum(tp) / (len(labels) + 1e-6)\n",
    "\n",
    "plot_pr_curve(precision, recall, f\"{save_dir}/pr_curve.png\")\n",
    "plot_f1_curve(precision, recall, f\"{save_dir}/f1_curve.png\")\n",
    "plot_confusion_matrix(all_labels, all_preds, f\"{save_dir}/confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xweWGwbbxG8N"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img1 = Image.open(f\"{save_dir}/losses.png\")\n",
    "img2 = Image.open(f\"{save_dir}/map_curve.png\")\n",
    "img3 = Image.open(f\"{save_dir}/pr_curve.png\")\n",
    "img4 = Image.open(f\"{save_dir}/f1_curve.png\")\n",
    "\n",
    "width = max(img1.width, img2.width)\n",
    "height = img1.height + img2.height + img3.height + img4.height\n",
    "\n",
    "results = Image.new(\"RGB\", (width, height), \"white\")\n",
    "\n",
    "y = 0\n",
    "for img in [img1, img2, img3, img4]:\n",
    "    results.paste(img, (0, y))\n",
    "    y += img.height\n",
    "\n",
    "results.save(f\"{save_dir}/results.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WPlBbJh3wkSr"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_fasterrcnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "\n",
    "for img, target in test_dataset:\n",
    "    img_tensor = img.cuda().unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"image_id\": target[\"image_id\"].item(),\n",
    "        \"boxes\": output[\"boxes\"].cpu().tolist(),\n",
    "        \"scores\": output[\"scores\"].cpu().tolist(),\n",
    "        \"labels\": output[\"labels\"].cpu().tolist()\n",
    "    })\n",
    "\n",
    "print(\"Infer√™ncia conclu√≠da!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKDT1Mx+KuLjaktMfEc1GO",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
