{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e AvaliaÃ§Ã£o - Dataset COCO Final - Classe Gun\n",
    "\n",
    "Este notebook treina e avalia um modelo EfficientDet usando o dataset COCO final.\n",
    "\n",
    "**Dataset:** `dataset_final_coco`\n",
    "**Classe:** Gun (apenas uma classe)\n",
    "**Modelo:** EfficientDet-D1\n",
    "**Treinamento:** 5% dos dados (750 imagens)\n",
    "**AvaliaÃ§Ã£o:** Subset do dataset de teste\n",
    "\n",
    "**Nota:** O modelo EfficientDet baixa automaticamente os pesos prÃ©-treinados quando vocÃª executa a cÃ©lula de configuraÃ§Ã£o do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bibliotecas importadas com sucesso!\n",
      "ğŸ“¦ EfficientDet serÃ¡ baixado automaticamente quando vocÃª carregar o modelo prÃ©-treinado.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ImportaÃ§Ãµes do EfficientDet\n",
    "from effdet import get_efficientdet_config, create_model, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "from effdet.bench import DetBenchPredict\n",
    "\n",
    "print(\"âœ… Bibliotecas importadas com sucesso!\")\n",
    "print(\"ğŸ“¦ EfficientDet serÃ¡ baixado automaticamente quando vocÃª carregar o modelo prÃ©-treinado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cache CUDA limpo. MemÃ³ria total da GPU: 6.00 GB\n",
      "Nome da GPU: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DE GERENCIAMENTO DE MEMÃ“RIA CUDA\n",
    "# ================================================================\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Limpar cache CUDA no inÃ­cio\n",
    "if torch.cuda.is_available():\n",
    "    # Apenas a limpeza do cache Ã© universalmente suportada.\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # REMOVIDO: torch.cuda.reset_peak_memory_stats() - NÃ£o compatÃ­vel com PyTorch 1.x\n",
    "    \n",
    "    # VerificaÃ§Ã£o de memÃ³ria compatÃ­vel com PyTorch 1.x\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    # O PyTorch 1.x nÃ£o tem uma maneira direta de obter a memÃ³ria 'disponÃ­vel' de forma simples\n",
    "    # no inÃ­cio, entÃ£o mostramos a memÃ³ria total.\n",
    "    print(f\"âœ… Cache CUDA limpo. MemÃ³ria total da GPU: {total_memory:.2f} GB\")\n",
    "    print(f\"Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Se a verificaÃ§Ã£o for True e o nome da GPU for exibido, o ambiente estÃ¡ pronto para uso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoAlbumentationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None, subset_size=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "        # Agrupar anotaÃ§Ãµes por imagem\n",
    "        self.annotations = {}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "        # Limitar a um subset se especificado\n",
    "        if subset_size is not None and subset_size < len(self.ids):\n",
    "            self.ids = self.ids[:subset_size]\n",
    "            print(f\"ğŸ“Š Usando subset de {len(self.ids)} imagens (de {len(self.images)} total)\")\n",
    "        \n",
    "        # VerificaÃ§Ã£o de anotaÃ§Ãµes\n",
    "        total_images = len(self.ids)\n",
    "        images_with_anns = sum(1 for img_id in self.ids if img_id in self.annotations and len(self.annotations[img_id]) > 0)\n",
    "        total_anns = sum(len(self.annotations.get(img_id, [])) for img_id in self.ids)\n",
    "        \n",
    "        print(f\"âœ… Dataset carregado:\")\n",
    "        print(f\"   ğŸ“Š Imagens no subset: {total_images}\")\n",
    "        print(f\"   ğŸ“Š Imagens com anotaÃ§Ãµes: {images_with_anns}\")\n",
    "        print(f\"   ğŸ“Š Total de anotaÃ§Ãµes: {total_anns}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        info = self.images[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, info[\"file_name\"])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        annots = self.annotations.get(img_id, [])\n",
    "\n",
    "        # Se nÃ£o tem boxes, retorna None\n",
    "        if len(annots) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            # EfficientDet aceita category_id diretamente (1 para gun)\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Batch size ajustado para: 4 (para evitar OOM)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# REDUZIR BATCH SIZE PARA EVITAR OOM\n",
    "# ================================================================\n",
    "\n",
    "# Reduzir batch size de 12 para 8 (GPU de 6GB)\n",
    "BATCH_SIZE = 4\n",
    "print(f\"ğŸ“¦ Batch size ajustado para: {BATCH_SIZE} (para evitar OOM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Device: cuda\n",
      "\n",
      "ğŸ“Š ConfiguraÃ§Ã£o:\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Tamanho de Entrada: 640x640\n",
      "   ğŸ“¸ Subset size: 3000 imagens\n",
      "ğŸ“Š Usando subset de 3000 imagens (de 4287 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 3000\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 3000\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 4543\n",
      "\n",
      "âœ… Dataset de VALIDAÃ‡ÃƒO final preparado: 3000 imagens vÃ¡lidas\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ DEFINIÃ‡ÃƒO DO MODEL_SIZE ğŸš¨\n",
    "MODEL_SIZE = 640 # Tamanho de entrada do EfficientDet-D1\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 3000 \n",
    "print(f\"\\nğŸ“Š ConfiguraÃ§Ã£o:\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Tamanho de Entrada: {MODEL_SIZE}x{MODEL_SIZE}\")\n",
    "print(f\"   ğŸ“¸ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# ğŸš¨ CORRIGIDO: TransformaÃ§Ãµes para validaÃ§Ã£o (AdiÃ§Ã£o de A.Resize) ğŸš¨\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=MODEL_SIZE, width=MODEL_SIZE, p=1.0), # CORREÃ‡ÃƒO DE TAMANHO\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDAÃ‡ÃƒO (usado durante treinamento)\n",
    "val_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"val\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_val.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "\n",
    "# ğŸš¨ CRIAÃ‡ÃƒO DO SUBSET FINAL (Robustez) ğŸš¨\n",
    "# Este Ã© o dataset que deve ser usado no DataLoader ou na avaliaÃ§Ã£o.\n",
    "final_val_dataset = Subset(val_dataset, valid_indices)\n",
    "\n",
    "print(f\"\\nâœ… Dataset de VALIDAÃ‡ÃƒO final preparado: {len(final_val_dataset)} imagens vÃ¡lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Device: cuda\n",
      "\n",
      "ğŸ“Š ConfiguraÃ§Ã£o:\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Tamanho de Entrada: 640x640\n",
      "   ğŸ“¸ Subset size: 100 imagens\n",
      "ğŸ“Š Usando subset de 100 imagens (de 4287 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 100\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 100\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 129\n",
      "\n",
      "âœ… Dataset de VALIDAÃ‡ÃƒO final preparado: 100 imagens vÃ¡lidas\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ DEFINIÃ‡ÃƒO DO MODEL_SIZE ğŸš¨\n",
    "MODEL_SIZE = 640 # Tamanho de entrada do EfficientDet-D1\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 100 \n",
    "print(f\"\\nğŸ“Š ConfiguraÃ§Ã£o:\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Tamanho de Entrada: {MODEL_SIZE}x{MODEL_SIZE}\")\n",
    "print(f\"   ğŸ“¸ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# ğŸš¨ CORREÃ‡ÃƒO 1: Adicionar A.Resize Ã s transformaÃ§Ãµes ğŸš¨\n",
    "val_transforms = A.Compose([\n",
    "    # ADIÃ‡ÃƒO CRUCIAL: Fixa o tamanho para 640x640 (resolve o erro de stack)\n",
    "    A.Resize(height=MODEL_SIZE, width=MODEL_SIZE, p=1.0), \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDAÃ‡ÃƒO (usado durante treinamento)\n",
    "val_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"val\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_val.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# ğŸš¨ CORREÃ‡ÃƒO 2: Filtrar None values e criar um Subset ğŸš¨\n",
    "# Cria a lista de Ã­ndices que nÃ£o retornam None\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "\n",
    "# Cria um Subset PyTorch contendo apenas os dados vÃ¡lidos\n",
    "final_val_dataset = Subset(val_dataset, valid_indices)\n",
    "\n",
    "print(f\"\\nâœ… Dataset de VALIDAÃ‡ÃƒO final preparado: {len(final_val_dataset)} imagens vÃ¡lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Device: cuda\n",
      "\n",
      "ğŸ“Š ConfiguraÃ§Ã£o:\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Tamanho de Entrada: 640x640\n",
      "   ğŸ“¸ Subset size: 100 imagens\n",
      "ğŸ“Š Usando subset de 100 imagens (de 2145 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 100\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 100\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 120\n",
      "\n",
      "âœ… Dataset de TESTE final preparado: 100 imagens vÃ¡lidas\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ DEFINIÃ‡ÃƒO DO MODEL_SIZE ğŸš¨\n",
    "MODEL_SIZE = 640 # Tamanho de entrada do EfficientDet-D1\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 100 \n",
    "print(f\"\\nğŸ“Š ConfiguraÃ§Ã£o:\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Tamanho de Entrada: {MODEL_SIZE}x{MODEL_SIZE}\")\n",
    "print(f\"   ğŸ“¸ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# ğŸš¨ CORREÃ‡ÃƒO 1: Adicionar A.Resize Ã s transformaÃ§Ãµes ğŸš¨\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=MODEL_SIZE, width=MODEL_SIZE, p=1.0), \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de TESTE\n",
    "test_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"test\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_test.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# ğŸš¨ CORREÃ‡ÃƒO 2: Filtrar None values sobre o test_dataset ğŸš¨\n",
    "# Cria a lista de Ã­ndices que nÃ£o retornam None\n",
    "valid_indices = [i for i in range(len(test_dataset)) if test_dataset[i] is not None]\n",
    "\n",
    "# ğŸš¨ CORREÃ‡ÃƒO 3: Criar um Subset robusto para o DataLoader ğŸš¨\n",
    "final_test_dataset = Subset(test_dataset, valid_indices)\n",
    "\n",
    "print(f\"\\nâœ… Dataset de TESTE final preparado: {len(final_test_dataset)} imagens vÃ¡lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Carregando modelo EfficientDet-D1...\n",
      "   â³ Os pesos prÃ©-treinados serÃ£o baixados automaticamente na primeira execuÃ§Ã£o.\n",
      "   ğŸ“¥ Isso pode levar alguns minutos na primeira vez...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Modelo EfficientDet-D1 configurado com sucesso!\n",
      "   ğŸ“Š Classes: 1 (Gun)\n",
      "   ğŸ“ Tamanho da imagem: [640, 640]\n",
      "   ğŸ’» Device: cuda\n",
      "   ğŸ“¦ Pesos prÃ©-treinados carregados automaticamente!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO MODELO EFFICIENTDET\n",
    "# ================================================================\n",
    "\n",
    "print(\"ğŸ“¦ Carregando modelo EfficientDet-D1...\")\n",
    "print(\"   â³ Os pesos prÃ©-treinados serÃ£o baixados automaticamente na primeira execuÃ§Ã£o.\")\n",
    "print(\"   ğŸ“¥ Isso pode levar alguns minutos na primeira vez...\\n\")\n",
    "\n",
    "# ConfiguraÃ§Ã£o do EfficientDet\n",
    "# ğŸ”¥ IMPORTANTE: EfficientDet NÃƒO inclui \"background\" como classe separada\n",
    "# Faster R-CNN usa num_classes=2 (background + gun), mas EfficientDet usa num_classes=1 (apenas gun)\n",
    "# EfficientDet aceita category_id diretamente do COCO (1 para gun)\n",
    "num_classes = 1  # Apenas Gun (sem background)\n",
    "\n",
    "# Config do modelo EfficientDet-D1 (640x640)\n",
    "# OpÃ§Ãµes disponÃ­veis: 'tf_efficientdet_d0' (512px), 'tf_efficientdet_d1' (640px), 'tf_efficientdet_d2' (768px)\n",
    "config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "\n",
    "config.num_classes = num_classes\n",
    "config.image_size = (640, 640)\n",
    "config.norm_kwargs = dict(eps=1e-4)\n",
    "\n",
    "# Carregar modelo prÃ©-treinado (baixa automaticamente os pesos)\n",
    "# pretrained_backbone=True faz o download automÃ¡tico dos pesos do backbone\n",
    "model = EfficientDet(config, pretrained_backbone=True)\n",
    "\n",
    "# Substituir head para o nÃºmero correto de classes\n",
    "model.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "# Wrap para treinamento correto\n",
    "model = DetBenchTrain(model, config)\n",
    "\n",
    "# Mover para device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nâœ… Modelo EfficientDet-D1 configurado com sucesso!\")\n",
    "print(f\"   ğŸ“Š Classes: {num_classes} (Gun)\")\n",
    "print(f\"   ğŸ“ Tamanho da imagem: {config.image_size}\")\n",
    "print(f\"   ğŸ’» Device: {device}\")\n",
    "print(f\"   ğŸ“¦ Pesos prÃ©-treinados carregados automaticamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Device: cuda\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š CONFIGURAÃ‡ÃƒO DO TREINAMENTO\n",
      "============================================================\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Imagens de treino: 150\n",
      "   ğŸ“ Tamanho de Entrada: 640x640\n",
      "   ğŸ’» Device: cuda\n",
      "ğŸ“Š Usando subset de 150 imagens (de 15007 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 150\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 150\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 170\n",
      "\n",
      "âœ… Dataset de treino preparado:\n",
      "   ğŸ“Š Total de imagens: 150\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ”„ Batches por Ã©poca: 38\n"
     ]
    }
   ],
   "source": [
    "# ğŸš¨ DEFINIÃ‡ÃƒO DO MODEL_SIZE ğŸš¨\n",
    "MODEL_SIZE = 640 # Tamanho de entrada do EfficientDet-D1\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO DATASET DE TREINO (COM CORREÃ‡ÃƒO DE TAMANHO)\n",
    "# ================================================================\n",
    "\n",
    "# TransformaÃ§Ãµes para treino (com augmentations)\n",
    "train_transforms = A.Compose([\n",
    "    # ğŸš¨ CORREÃ‡ÃƒO CRUCIAL: Adiciona o redimensionamento obrigatÃ³rio ğŸš¨\n",
    "    A.Resize(height=MODEL_SIZE, width=MODEL_SIZE, p=1.0), \n",
    "    \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomGamma(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.3))\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "# Definir subset size\n",
    "TRAIN_SUBSET_SIZE = 1500\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š CONFIGURAÃ‡ÃƒO DO TREINAMENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Imagens de treino: {TRAIN_SUBSET_SIZE}\")\n",
    "print(f\"   ğŸ“ Tamanho de Entrada: {MODEL_SIZE}x{MODEL_SIZE}\")\n",
    "print(f\"   ğŸ’» Device: {device}\")\n",
    "\n",
    "train_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"train\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_train.json\"),\n",
    "    transforms=train_transforms,\n",
    "    subset_size=TRAIN_SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values e criar DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"FunÃ§Ã£o para agrupar amostras em batch, removendo None\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "# Criar DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # 0 funciona melhor no Windows/Jupyter\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Dataset de treino preparado:\")\n",
    "print(f\"   ğŸ“Š Total de imagens: {len(train_dataset)}\")\n",
    "print(f\"   ğŸ“¦ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ğŸ”„ Batches por Ã©poca: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Otimizador configurado:\n",
      "   ğŸ“Š Learning rate inicial: 0.001\n",
      "   ğŸ“Š Momentum: 0.9\n",
      "   ğŸ“Š Weight decay: 0.0005\n",
      "   ğŸ“Š Ã‰pocas: 20\n",
      "   ğŸ“Š Scheduler: StepLR (step_size=3, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO OTIMIZADOR E LEARNING RATE SCHEDULER\n",
    "# ================================================================\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# ParÃ¢metros de treinamento\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Otimizador\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(f\"âœ… Otimizador configurado:\")\n",
    "print(f\"   ğŸ“Š Learning rate inicial: {LEARNING_RATE}\")\n",
    "print(f\"   ğŸ“Š Momentum: {MOMENTUM}\")\n",
    "print(f\"   ğŸ“Š Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   ğŸ“Š Ã‰pocas: {NUM_EPOCHS}\")\n",
    "print(f\"   ğŸ“Š Scheduler: StepLR (step_size=3, gamma=0.1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MemÃ³ria CUDA limpa!\n",
      "   ğŸ’¾ MemÃ³ria alocada: 0.03 GB\n",
      "   ğŸ’¾ MemÃ³ria reservada: 0.04 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LIMPAR MEMÃ“RIA CUDA (Execute esta cÃ©lula se tiver OOM)\n",
    "# ================================================================\n",
    "\n",
    "# Limpar toda a memÃ³ria CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    print(\"âœ… MemÃ³ria CUDA limpa!\")\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria alocada: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria reservada: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA nÃ£o disponÃ­vel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUNÃ‡ÃƒO DE TREINAMENTO COM LOGS DETALHADOS\n",
    "# ================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    Treina o modelo por uma Ã©poca com logs detalhados a cada batch\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca {epoch+1}\", flush=True)\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"ğŸ”µ Modelo em modo train()\", flush=True)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_classifier = 0.0\n",
    "    loss_box_reg = 0.0\n",
    "    loss_objectness = 0.0\n",
    "    loss_rpn_box_reg = 0.0\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\", flush=True)\n",
    "    print(f\"ğŸš€ Ã‰POCA {epoch+1}/{num_epochs}\", flush=True)\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    print(f\"   ğŸ“Š Total de batches: {num_batches}\", flush=True)\n",
    "    print(f\"   ğŸ“¦ Batch size: {BATCH_SIZE}\", flush=True)\n",
    "    print(f\"   ğŸ“ˆ Learning rate: {optimizer.param_groups[0]['lr']:.6f}\", flush=True)\n",
    "    print(f\"   ğŸ• InÃ­cio: {datetime.now().strftime('%H:%M:%S')}\", flush=True)\n",
    "    print(f\"\\nğŸ”„ Iniciando treinamento...\\n\", flush=True)\n",
    "    \n",
    "    # Debug: verificar se o DataLoader estÃ¡ funcionando\n",
    "    print(f\"   ğŸ” Testando DataLoader...\", flush=True)\n",
    "    try:\n",
    "        first_batch = next(iter(data_loader))\n",
    "        print(f\"   âœ… DataLoader funcionando! Primeiro batch obtido.\", flush=True)\n",
    "        if first_batch[0] is None:\n",
    "            print(f\"   âš ï¸  ATENÃ‡ÃƒO: Primeiro batch Ã© None!\", flush=True)\n",
    "        else:\n",
    "            print(f\"   âœ… Primeiro batch tem {len(first_batch[0])} imagens\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ERRO ao obter primeiro batch: {e}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "    \n",
    "    # Reiniciar o iterador\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        try:\n",
    "            images, targets = next(data_loader_iter)\n",
    "        except StopIteration:\n",
    "            print(f\"   âš ï¸  DataLoader esgotado no batch {batch_idx+1}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ERRO ao obter batch {batch_idx+1}: {e}\")\n",
    "            break\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        if images is None or len(images) == 0:\n",
    "            print(f\"   âš ï¸  Batch {batch_idx+1} estÃ¡ vazio, pulando...\")\n",
    "            continue\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   ğŸ” Processando primeiro batch: {len(images)} imagens\")\n",
    "            \n",
    "        # ================================================================\n",
    "        # CONVERTER PARA FORMATO EFFICIENTDET\n",
    "        # ================================================================\n",
    "        # EfficientDet precisa de imagens empilhadas em tensor e targets em formato especÃ­fico\n",
    "        \n",
    "        # Mover imagens para device e empilhar em tensor [B, C, H, W]\n",
    "        images = torch.stack([img.to(device) for img in images], dim=0)\n",
    "        \n",
    "        # Mover targets para device\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Converter targets para formato EfficientDet\n",
    "        batch_bboxes = [t[\"boxes\"] for t in targets]\n",
    "        batch_labels = [t[\"labels\"] for t in targets]\n",
    "        \n",
    "        # NÃºmero mÃ¡ximo de boxes no batch (EfficientDet precisa de padding)\n",
    "        max_boxes = max(b.shape[0] for b in batch_bboxes) if len(batch_bboxes) > 0 else 1\n",
    "        \n",
    "        padded_boxes = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for b, l in zip(batch_bboxes, batch_labels):\n",
    "            num = b.shape[0]\n",
    "            if num < max_boxes:\n",
    "                # Padding com zeros\n",
    "                pad_b = torch.zeros((max_boxes - num, 4), device=b.device)\n",
    "                pad_l = torch.zeros((max_boxes - num,), device=l.device, dtype=l.dtype)\n",
    "                b = torch.cat([b, pad_b], dim=0)\n",
    "                l = torch.cat([l, pad_l], dim=0)\n",
    "            padded_boxes.append(b)\n",
    "            padded_labels.append(l)\n",
    "        \n",
    "        # Formato EfficientDet: {\"bbox\": [B, max_boxes, 4], \"cls\": [B, max_boxes]}\n",
    "        eff_target = {\n",
    "            \"bbox\": torch.stack(padded_boxes, dim=0),   # [B, max_boxes, 4]\n",
    "            \"cls\": torch.stack(padded_labels, dim=0),    # [B, max_boxes]\n",
    "        }\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   ğŸ” Imagens empilhadas, shape: {images.shape}\")\n",
    "            print(f\"   ğŸ” Targets convertidos para EfficientDet, bbox shape: {eff_target['bbox'].shape}\")\n",
    "            print(f\"   ğŸ” Iniciando forward pass...\")\n",
    "        \n",
    "        # Forward pass (EfficientDet retorna dict com \"loss\")\n",
    "        loss_dict = model(images, eff_target)\n",
    "        \n",
    "        # EfficientDet retorna dict com chave \"loss\"\n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = loss_dict.get(\"loss\", loss_dict.get(\"loss_total\", sum(loss_dict.values())))\n",
    "        else:\n",
    "            losses = loss_dict\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   âœ… Forward pass concluÃ­do, loss: {losses.item():.4f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular tempo do batch\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Acumular losses (EfficientDet retorna apenas loss total)\n",
    "        total_loss += losses.item()\n",
    "        # EfficientDet nÃ£o separa losses como Faster R-CNN\n",
    "        loss_classifier += 0.0  # NÃ£o disponÃ­vel separadamente\n",
    "        loss_box_reg += 0.0     # NÃ£o disponÃ­vel separadamente\n",
    "        loss_objectness += 0.0  # NÃ£o disponÃ­vel separadamente\n",
    "        loss_rpn_box_reg += 0.0 # NÃ£o disponÃ­vel separadamente\n",
    "        \n",
    "        # Calcular estatÃ­sticas\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress = ((batch_idx + 1) / num_batches) * 100\n",
    "        \n",
    "        # Estimar tempo restante\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        remaining_batches = num_batches - (batch_idx + 1)\n",
    "        estimated_remaining = avg_batch_time * remaining_batches\n",
    "        \n",
    "        # Log a cada batch\n",
    "        current_time = datetime.now().strftime('%H:%M:%S')\n",
    "        log_msg = (f\"   ğŸ“¦ [{batch_idx+1:3d}/{num_batches}] \"\n",
    "                   f\"| {progress:5.1f}% | \"\n",
    "                   f\"Loss: {losses.item():.4f} | \"\n",
    "                   f\"Avg: {avg_loss:.4f} | \"\n",
    "                   f\"Tempo: {batch_time:.2f}s | \"\n",
    "                   f\"Restante: ~{int(estimated_remaining//60)}m{int(estimated_remaining%60)}s | \"\n",
    "                   f\"ğŸ• {current_time}\")\n",
    "        print(log_msg, flush=True)  # flush=True garante que aparece imediatamente\n",
    "    \n",
    "    # Calcular mÃ©dias\n",
    "    total_time = time.time() - start_time\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_classifier = loss_classifier / num_batches\n",
    "    avg_box_reg = loss_box_reg / num_batches\n",
    "    avg_objectness = loss_objectness / num_batches\n",
    "    avg_rpn_box_reg = loss_rpn_box_reg / num_batches\n",
    "    \n",
    "    print(f\"\\nâœ… Ã‰poca {epoch+1} concluÃ­da!\")\n",
    "    print(f\"   ğŸ“Š Loss Total: {avg_total_loss:.4f}\")\n",
    "    print(f\"   âš ï¸  (EfficientDet nÃ£o separa losses como Faster R-CNN)\")\n",
    "    print(f\"   â±ï¸  Tempo total: {int(total_time//60)}m{int(total_time%60)}s\")\n",
    "    print(f\"   â±ï¸  Tempo mÃ©dio por batch: {sum(batch_times)/len(batch_times):.2f}s\")\n",
    "    \n",
    "    return avg_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DetBenchPredict configurado corretamente!\n"
     ]
    }
   ],
   "source": [
    "model_eval = DetBenchPredict(model.model)\n",
    "model_eval.eval()\n",
    "model_eval.to(device)\n",
    "\n",
    "\n",
    "print(\"âœ… DetBenchPredict configurado corretamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ã£o evaluate_coco_completo criada!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FUNÃ‡ÃƒO evaluate_coco MODIFICADA - RETORNA TODAS AS MÃ‰TRICAS\n",
    "# ================================================================\n",
    "# Esta funÃ§Ã£o estÃ¡ pronta para receber o modelo de inferÃªncia (DetBenchPredict)\n",
    "\n",
    "def evaluate_coco_completo(\n",
    "    predictor, dataset, ann_path, device=\"cuda\", \n",
    "    score_threshold=0.3, valid_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    AvaliaÃ§Ã£o COCO completa para EfficientDet usando DetBenchPredict.\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ” INÃCIO DA AVALIAÃ‡ÃƒO COCO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸ“‚ AnotaÃ§Ãµes: {ann_path}\")\n",
    "    print(f\"ğŸ¯ Score threshold: {score_threshold}\")\n",
    "    print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "    predictor.eval()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Carregar COCO GT\n",
    "    # ------------------------------------------------------------------\n",
    "    coco_gt = COCO(ann_path)\n",
    "\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        print(\"âš ï¸  Adicionando chave 'info' ao dataset COCO\")\n",
    "        coco_gt.dataset['info'] = {\n",
    "            \"description\": \"COCO Dataset - Gun Detection\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    expected_category_id = cat_ids[0] if len(cat_ids) else 1\n",
    "\n",
    "    print(f\"ğŸ“Š Categoria detectada no COCO: {cat_ids}\")\n",
    "    print(f\"ğŸ“‹ category_id usado para prediÃ§Ãµes: {expected_category_id}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. Preparar subset para avaliaÃ§Ã£o\n",
    "    # ------------------------------------------------------------------\n",
    "    indices_to_use = valid_indices if valid_indices is not None else list(range(len(dataset)))\n",
    "\n",
    "    print(f\"\\nğŸ”„ Processando {len(indices_to_use)} imagens...\")\n",
    "\n",
    "    results = []\n",
    "    subset_image_ids = []\n",
    "    processed = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Loop de prediÃ§Ã£o\n",
    "    # ------------------------------------------------------------------\n",
    "    for idx in indices_to_use:\n",
    "\n",
    "        sample = dataset[idx]\n",
    "        if sample is None:\n",
    "            continue\n",
    "\n",
    "        img, target = sample\n",
    "\n",
    "        # Tamanho da imagem REDIMENSIONADA (640x640)\n",
    "        h_resized, w_resized = img.shape[-2:]\n",
    "\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "        image_id = int(target[\"image_id\"].item())\n",
    "        subset_image_ids.append(image_id)\n",
    "\n",
    "        # ğŸ”¥ CORREÃ‡ÃƒO: Obter tamanho ORIGINAL da imagem do COCO\n",
    "        img_info = coco_gt.loadImgs(image_id)[0]\n",
    "        orig_w = img_info['width']\n",
    "        orig_h = img_info['height']\n",
    "        \n",
    "        # Calcular fatores de escala\n",
    "        scale_x = orig_w / w_resized\n",
    "        scale_y = orig_h / h_resized\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = predictor(img_tensor)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # EfficientDet retorna [N, 6]:\n",
    "        # [x1, y1, x2, y2, score, label]\n",
    "        # --------------------------------------------------------------\n",
    "        pred = output[0]\n",
    "\n",
    "        boxes  = pred[:, :4].detach().cpu().numpy()\n",
    "        scores = pred[:, 4].detach().cpu().numpy()\n",
    "        labels = pred[:, 5].detach().cpu().numpy().astype(int)\n",
    "        \n",
    "        # Debug: primeira imagem\n",
    "        if processed == 0:\n",
    "            print(f\"\\nğŸ” DEBUG - Primeira imagem (image_id={image_id}):\")\n",
    "            print(f\"   Total de prediÃ§Ãµes antes do filtro: {len(scores)}\")\n",
    "            if len(scores) > 0:\n",
    "                print(f\"   Scores: min={scores.min():.3f}, max={scores.max():.3f}, mean={scores.mean():.3f}\")\n",
    "                print(f\"   Labels Ãºnicos: {np.unique(labels)}\")\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # FILTRAR POR SCORE\n",
    "        # --------------------------------------------------------------\n",
    "        keep = scores >= score_threshold\n",
    "        boxes = boxes[keep]\n",
    "        scores = scores[keep]\n",
    "        labels = labels[keep]\n",
    "        \n",
    "        # Debug: primeira imagem\n",
    "        if processed == 0:\n",
    "            print(f\"   PrediÃ§Ãµes apÃ³s filtro (score>={score_threshold}): {len(scores)}\")\n",
    "        \n",
    "        # ğŸ”¥ CORREÃ‡ÃƒO: EfficientDet com num_classes=1 retorna labels como 0\n",
    "        # Converter de volta para category_id do COCO (0 -> 1)\n",
    "        if len(labels) > 0:\n",
    "            # Se todos os labels sÃ£o 0, converter para 1\n",
    "            if np.all(labels == 0):\n",
    "                labels = labels + 1  # Converter 0 -> 1\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # CONVERSÃƒO PARA COORDENADAS ORIGINAIS + CLIPPING + VALIDAÃ‡ÃƒO\n",
    "        # --------------------------------------------------------------\n",
    "        clipped_results = []\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            \n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "            # ğŸ”¥ CORREÃ‡ÃƒO: Converter de coordenadas redimensionadas (640x640) para originais\n",
    "            x1 = x1 * scale_x\n",
    "            y1 = y1 * scale_y\n",
    "            x2 = x2 * scale_x\n",
    "            y2 = y2 * scale_y\n",
    "\n",
    "            # Clamping dentro da imagem ORIGINAL\n",
    "            x1 = max(0, min(x1, orig_w-1))\n",
    "            y1 = max(0, min(y1, orig_h-1))\n",
    "            x2 = max(0, min(x2, orig_w-1))\n",
    "            y2 = max(0, min(y2, orig_h-1))\n",
    "\n",
    "            # Garantir ordem correta\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            w_box = x2 - x1\n",
    "            h_box = y2 - y1\n",
    "\n",
    "            if w_box <= 0 or h_box <= 0:\n",
    "                continue\n",
    "\n",
    "            clipped_results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": expected_category_id,\n",
    "                \"bbox\": [float(x1), float(y1), float(w_box), float(h_box)],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "        results.extend(clipped_results)\n",
    "        processed += 1\n",
    "\n",
    "        if processed % 10 == 0:\n",
    "            print(f\"â³ Processadas {processed}/{len(indices_to_use)} imagens...\")\n",
    "\n",
    "    print(\"\\nâœ… Processamento concluÃ­do!\")\n",
    "    print(f\"ğŸ“¸ Imagens avaliadas: {processed}\")\n",
    "    print(f\"ğŸ“Š PrediÃ§Ãµes vÃ¡lidas: {len(results)}\")\n",
    "    \n",
    "    # Debug: mostrar algumas prediÃ§Ãµes\n",
    "    if len(results) > 0:\n",
    "        print(f\"\\nğŸ” DEBUG - Primeiras 3 prediÃ§Ãµes:\")\n",
    "        for i, r in enumerate(results[:3]):\n",
    "            print(f\"   {i+1}. image_id={r['image_id']}, category_id={r['category_id']}, \"\n",
    "                  f\"bbox={r['bbox']}, score={r['score']:.3f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Nenhuma prediÃ§Ã£o gerada! Verifique se o modelo estÃ¡ gerando detecÃ§Ãµes.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Caso nÃ£o haja resultados\n",
    "    # ------------------------------------------------------------------\n",
    "    if len(results) == 0:\n",
    "        print(\"âš ï¸  Nenhuma prediÃ§Ã£o vÃ¡lida!\")\n",
    "        return {k: 0.0 for k in [\n",
    "            'map50_95','map50','map75','map50_95_small','map50_95_medium','map50_95_large',\n",
    "            'ar_all_1','ar_all_10','ar_all_100','ar_small','ar_medium','ar_large'\n",
    "        ]}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. AvaliaÃ§Ã£o COCO\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nğŸ“Š Calculando mÃ©tricas COCO...\")\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "\n",
    "    coco_eval.params.imgIds = subset_image_ids\n",
    "\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    stats = coco_eval.stats\n",
    "\n",
    "    metrics = {\n",
    "        'map50_95': stats[0],\n",
    "        'map50': stats[1],\n",
    "        'map75': stats[2],\n",
    "        'map50_95_small': stats[3],\n",
    "        'map50_95_medium': stats[4],\n",
    "        'map50_95_large': stats[5],\n",
    "        'ar_all_1': stats[6],\n",
    "        'ar_all_10': stats[7],\n",
    "        'ar_all_100': stats[8],\n",
    "        'ar_small': stats[9],\n",
    "        'ar_medium': stats[10],\n",
    "        'ar_large': stats[11]\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"âœ… FunÃ§Ã£o evaluate_coco_completo criada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ¯ INICIANDO TREINAMENTO\n",
      "============================================================\n",
      "   ğŸ“‚ DiretÃ³rio de salvamento: runs/aula9_coco_gun\n",
      "   ğŸ“Š Total de Ã©pocas: 20\n",
      "   ğŸ“¸ Imagens por Ã©poca: 150\n",
      "   ğŸ“¦ Batches por Ã©poca: 38\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 1/20 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 1\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 1/20\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 38\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.001000\n",
      "   ğŸ• InÃ­cio: 22:20:16\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens empilhadas, shape: torch.Size([4, 3, 640, 640])\n",
      "   ğŸ” Targets convertidos para EfficientDet, bbox shape: torch.Size([4, 2, 4])\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 323.5257\n",
      "   ğŸ“¦ [  1/38] |   2.6% | Loss: 323.5257 | Avg: 323.5257 | Tempo: 1.19s | Restante: ~0m44s | ğŸ• 22:20:17\n",
      "   ğŸ“¦ [  2/38] |   5.3% | Loss: 397.8240 | Avg: 360.6749 | Tempo: 0.67s | Restante: ~0m33s | ğŸ• 22:20:18\n",
      "   ğŸ“¦ [  3/38] |   7.9% | Loss: 347.4784 | Avg: 356.2760 | Tempo: 0.63s | Restante: ~0m29s | ğŸ• 22:20:19\n",
      "   ğŸ“¦ [  4/38] |  10.5% | Loss: 175.8240 | Avg: 311.1630 | Tempo: 0.62s | Restante: ~0m26s | ğŸ• 22:20:19\n",
      "   ğŸ“¦ [  5/38] |  13.2% | Loss: 108.4693 | Avg: 270.6243 | Tempo: 0.70s | Restante: ~0m25s | ğŸ• 22:20:20\n",
      "   ğŸ“¦ [  6/38] |  15.8% | Loss: 46.6998 | Avg: 233.3035 | Tempo: 0.63s | Restante: ~0m23s | ğŸ• 22:20:21\n",
      "   ğŸ“¦ [  7/38] |  18.4% | Loss: 18.2968 | Avg: 202.5883 | Tempo: 0.50s | Restante: ~0m21s | ğŸ• 22:20:21\n",
      "   ğŸ“¦ [  8/38] |  21.1% | Loss: 11.5580 | Avg: 178.7095 | Tempo: 0.63s | Restante: ~0m20s | ğŸ• 22:20:22\n",
      "   ğŸ“¦ [  9/38] |  23.7% | Loss: 5.2996 | Avg: 159.4417 | Tempo: 0.63s | Restante: ~0m19s | ğŸ• 22:20:23\n",
      "   ğŸ“¦ [ 10/38] |  26.3% | Loss: 3.5660 | Avg: 143.8542 | Tempo: 0.62s | Restante: ~0m19s | ğŸ• 22:20:23\n",
      "   ğŸ“¦ [ 11/38] |  28.9% | Loss: 4.3746 | Avg: 131.1742 | Tempo: 0.58s | Restante: ~0m18s | ğŸ• 22:20:24\n",
      "   ğŸ“¦ [ 12/38] |  31.6% | Loss: 4.0248 | Avg: 120.5784 | Tempo: 0.61s | Restante: ~0m17s | ğŸ• 22:20:25\n",
      "   ğŸ“¦ [ 13/38] |  34.2% | Loss: 3.9076 | Avg: 111.6038 | Tempo: 0.60s | Restante: ~0m16s | ğŸ• 22:20:25\n",
      "   ğŸ“¦ [ 14/38] |  36.8% | Loss: 3.6570 | Avg: 103.8933 | Tempo: 0.58s | Restante: ~0m15s | ğŸ• 22:20:26\n",
      "   ğŸ“¦ [ 15/38] |  39.5% | Loss: 3.2483 | Avg: 97.1836 | Tempo: 0.64s | Restante: ~0m15s | ğŸ• 22:20:27\n",
      "   ğŸ“¦ [ 16/38] |  42.1% | Loss: 3.2509 | Avg: 91.3128 | Tempo: 0.58s | Restante: ~0m14s | ğŸ• 22:20:27\n",
      "   ğŸ“¦ [ 17/38] |  44.7% | Loss: 3.9945 | Avg: 86.1764 | Tempo: 0.63s | Restante: ~0m13s | ğŸ• 22:20:28\n",
      "   ğŸ“¦ [ 18/38] |  47.4% | Loss: 4.0880 | Avg: 81.6160 | Tempo: 0.58s | Restante: ~0m12s | ğŸ• 22:20:29\n",
      "   ğŸ“¦ [ 19/38] |  50.0% | Loss: 4.2328 | Avg: 77.5432 | Tempo: 0.46s | Restante: ~0m12s | ğŸ• 22:20:29\n",
      "   ğŸ“¦ [ 20/38] |  52.6% | Loss: 3.9706 | Avg: 73.8645 | Tempo: 0.61s | Restante: ~0m11s | ğŸ• 22:20:30\n",
      "   ğŸ“¦ [ 21/38] |  55.3% | Loss: 4.8232 | Avg: 70.5769 | Tempo: 0.63s | Restante: ~0m10s | ğŸ• 22:20:31\n",
      "   ğŸ“¦ [ 22/38] |  57.9% | Loss: 3.5166 | Avg: 67.5287 | Tempo: 0.74s | Restante: ~0m10s | ğŸ• 22:20:31\n",
      "   ğŸ“¦ [ 23/38] |  60.5% | Loss: 4.8652 | Avg: 64.8042 | Tempo: 0.59s | Restante: ~0m9s | ğŸ• 22:20:32\n",
      "   ğŸ“¦ [ 24/38] |  63.2% | Loss: 3.5592 | Avg: 62.2523 | Tempo: 0.51s | Restante: ~0m8s | ğŸ• 22:20:33\n",
      "   ğŸ“¦ [ 25/38] |  65.8% | Loss: 3.9064 | Avg: 59.9185 | Tempo: 0.60s | Restante: ~0m8s | ğŸ• 22:20:33\n",
      "   ğŸ“¦ [ 26/38] |  68.4% | Loss: 3.1612 | Avg: 57.7355 | Tempo: 0.48s | Restante: ~0m7s | ğŸ• 22:20:34\n",
      "   ğŸ“¦ [ 27/38] |  71.1% | Loss: 6.0175 | Avg: 55.8200 | Tempo: 0.59s | Restante: ~0m6s | ğŸ• 22:20:34\n",
      "   ğŸ“¦ [ 28/38] |  73.7% | Loss: 4.5596 | Avg: 53.9893 | Tempo: 0.48s | Restante: ~0m6s | ğŸ• 22:20:35\n",
      "   ğŸ“¦ [ 29/38] |  76.3% | Loss: 3.8799 | Avg: 52.2614 | Tempo: 0.63s | Restante: ~0m5s | ğŸ• 22:20:36\n",
      "   ğŸ“¦ [ 30/38] |  78.9% | Loss: 4.3152 | Avg: 50.6632 | Tempo: 0.61s | Restante: ~0m4s | ğŸ• 22:20:36\n",
      "   ğŸ“¦ [ 31/38] |  81.6% | Loss: 3.5370 | Avg: 49.1430 | Tempo: 0.50s | Restante: ~0m4s | ğŸ• 22:20:37\n",
      "   ğŸ“¦ [ 32/38] |  84.2% | Loss: 3.8489 | Avg: 47.7275 | Tempo: 0.59s | Restante: ~0m3s | ğŸ• 22:20:38\n",
      "   ğŸ“¦ [ 33/38] |  86.8% | Loss: 3.5334 | Avg: 46.3883 | Tempo: 0.62s | Restante: ~0m3s | ğŸ• 22:20:38\n",
      "   ğŸ“¦ [ 34/38] |  89.5% | Loss: 4.4232 | Avg: 45.1540 | Tempo: 0.60s | Restante: ~0m2s | ğŸ• 22:20:39\n",
      "   ğŸ“¦ [ 35/38] |  92.1% | Loss: 3.8490 | Avg: 43.9739 | Tempo: 0.59s | Restante: ~0m1s | ğŸ• 22:20:40\n",
      "   ğŸ“¦ [ 36/38] |  94.7% | Loss: 3.7310 | Avg: 42.8560 | Tempo: 0.60s | Restante: ~0m1s | ğŸ• 22:20:40\n",
      "   ğŸ“¦ [ 37/38] |  97.4% | Loss: 4.4162 | Avg: 41.8171 | Tempo: 0.52s | Restante: ~0m0s | ğŸ• 22:20:41\n",
      "   ğŸ“¦ [ 38/38] | 100.0% | Loss: 3.7057 | Avg: 40.8142 | Tempo: 0.39s | Restante: ~0m0s | ğŸ• 22:20:41\n",
      "\n",
      "âœ… Ã‰poca 1 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 40.8142\n",
      "   âš ï¸  (EfficientDet nÃ£o separa losses como Faster R-CNN)\n",
      "   â±ï¸  Tempo total: 0m25s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.60s\n",
      "ğŸ”µ === FIM Ã‰POCA 1/20, Loss: 40.8142 ===\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š AVALIANDO MODELO NO DATASET VAL - Ã‰POCA 1\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ” INÃCIO DA AVALIAÃ‡ÃƒO COCO\n",
      "============================================================\n",
      "ğŸ“‚ AnotaÃ§Ãµes: dataset_final_coco\\coco_annotations_val.json\n",
      "ğŸ¯ Score threshold: 0.3\n",
      "ğŸ’» Device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "âš ï¸  Adicionando chave 'info' ao dataset COCO\n",
      "ğŸ“Š Categoria detectada no COCO: [1]\n",
      "ğŸ“‹ category_id usado para prediÃ§Ãµes: 1\n",
      "\n",
      "ğŸ”„ Processando 100 imagens...\n",
      "\n",
      "ğŸ” DEBUG - Primeira imagem (image_id=1):\n",
      "   Total de prediÃ§Ãµes antes do filtro: 100\n",
      "   Scores: min=0.034, max=0.067, mean=0.043\n",
      "   Labels Ãºnicos: [1]\n",
      "   PrediÃ§Ãµes apÃ³s filtro (score>=0.3): 0\n",
      "â³ Processadas 10/100 imagens...\n",
      "â³ Processadas 20/100 imagens...\n",
      "â³ Processadas 30/100 imagens...\n",
      "â³ Processadas 40/100 imagens...\n",
      "â³ Processadas 50/100 imagens...\n",
      "â³ Processadas 60/100 imagens...\n",
      "â³ Processadas 70/100 imagens...\n",
      "â³ Processadas 80/100 imagens...\n",
      "â³ Processadas 90/100 imagens...\n",
      "â³ Processadas 100/100 imagens...\n",
      "\n",
      "âœ… Processamento concluÃ­do!\n",
      "ğŸ“¸ Imagens avaliadas: 100\n",
      "ğŸ“Š PrediÃ§Ãµes vÃ¡lidas: 0\n",
      "âš ï¸  Nenhuma prediÃ§Ã£o gerada! Verifique se o modelo estÃ¡ gerando detecÃ§Ãµes.\n",
      "âš ï¸  Nenhuma prediÃ§Ã£o vÃ¡lida!\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š MÃ‰TRICAS DA Ã‰POCA 1:\n",
      "============================================================\n",
      "   âœ… mAP@0.5:      0.0000 (0.00%)\n",
      "   âœ… mAP@0.5:0.95: 0.0000 (0.00%)\n",
      "   âœ… mAP@0.75:     0.0000 (0.00%)\n",
      "   âœ… AR@100:       0.0000 (0.00%)\n",
      "\n",
      "   ğŸ“Š MÃ©tricas por tamanho:\n",
      "      â€¢ Small:   mAP=0.0000, AR=0.0000\n",
      "      â€¢ Medium:  mAP=0.0000, AR=0.0000\n",
      "      â€¢ Large:   mAP=0.0000, AR=0.0000\n",
      "============================================================\n",
      "\n",
      "ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_1.pth\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 2/20 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 2\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 2/20\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 38\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.001000\n",
      "   ğŸ• InÃ­cio: 22:20:51\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens empilhadas, shape: torch.Size([4, 3, 640, 640])\n",
      "   ğŸ” Targets convertidos para EfficientDet, bbox shape: torch.Size([4, 1, 4])\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 2.5507\n",
      "   ğŸ“¦ [  1/38] |   2.6% | Loss: 2.5507 | Avg: 2.5507 | Tempo: 0.33s | Restante: ~0m12s | ğŸ• 22:20:52\n",
      "   ğŸ“¦ [  2/38] |   5.3% | Loss: 2.9778 | Avg: 2.7642 | Tempo: 0.31s | Restante: ~0m11s | ğŸ• 22:20:52\n",
      "   ğŸ“¦ [  3/38] |   7.9% | Loss: 3.9755 | Avg: 3.1680 | Tempo: 0.29s | Restante: ~0m10s | ğŸ• 22:20:53\n",
      "   ğŸ“¦ [  4/38] |  10.5% | Loss: 3.3130 | Avg: 3.2042 | Tempo: 0.29s | Restante: ~0m10s | ğŸ• 22:20:54\n",
      "   ğŸ“¦ [  5/38] |  13.2% | Loss: 3.8658 | Avg: 3.3366 | Tempo: 0.38s | Restante: ~0m10s | ğŸ• 22:20:54\n",
      "   ğŸ“¦ [  6/38] |  15.8% | Loss: 6.1029 | Avg: 3.7976 | Tempo: 0.29s | Restante: ~0m10s | ğŸ• 22:20:55\n",
      "   ğŸ“¦ [  7/38] |  18.4% | Loss: 4.5803 | Avg: 3.9094 | Tempo: 0.29s | Restante: ~0m9s | ğŸ• 22:20:56\n",
      "   ğŸ“¦ [  8/38] |  21.1% | Loss: 3.8962 | Avg: 3.9078 | Tempo: 0.35s | Restante: ~0m9s | ğŸ• 22:20:56\n",
      "   ğŸ“¦ [  9/38] |  23.7% | Loss: 2.9761 | Avg: 3.8043 | Tempo: 0.48s | Restante: ~0m9s | ğŸ• 22:20:57\n",
      "   ğŸ“¦ [ 10/38] |  26.3% | Loss: 3.2159 | Avg: 3.7454 | Tempo: 0.34s | Restante: ~0m9s | ğŸ• 22:20:58\n",
      "   ğŸ“¦ [ 11/38] |  28.9% | Loss: 4.3763 | Avg: 3.8028 | Tempo: 0.29s | Restante: ~0m8s | ğŸ• 22:20:58\n",
      "   ğŸ“¦ [ 12/38] |  31.6% | Loss: 3.2344 | Avg: 3.7554 | Tempo: 0.29s | Restante: ~0m8s | ğŸ• 22:20:59\n",
      "   ğŸ“¦ [ 13/38] |  34.2% | Loss: 2.6344 | Avg: 3.6692 | Tempo: 0.28s | Restante: ~0m8s | ğŸ• 22:21:00\n",
      "   ğŸ“¦ [ 14/38] |  36.8% | Loss: 3.7688 | Avg: 3.6763 | Tempo: 0.28s | Restante: ~0m7s | ğŸ• 22:21:00\n",
      "   ğŸ“¦ [ 15/38] |  39.5% | Loss: 3.3441 | Avg: 3.6541 | Tempo: 0.28s | Restante: ~0m7s | ğŸ• 22:21:01\n",
      "   ğŸ“¦ [ 16/38] |  42.1% | Loss: 4.1091 | Avg: 3.6826 | Tempo: 0.29s | Restante: ~0m6s | ğŸ• 22:21:01\n",
      "   ğŸ“¦ [ 17/38] |  44.7% | Loss: 3.3676 | Avg: 3.6641 | Tempo: 0.29s | Restante: ~0m6s | ğŸ• 22:21:02\n",
      "   ğŸ“¦ [ 18/38] |  47.4% | Loss: 4.2986 | Avg: 3.6993 | Tempo: 0.29s | Restante: ~0m6s | ğŸ• 22:21:03\n",
      "   ğŸ“¦ [ 19/38] |  50.0% | Loss: 4.2589 | Avg: 3.7288 | Tempo: 0.29s | Restante: ~0m5s | ğŸ• 22:21:03\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LOOP DE TREINAMENTO (CORRIGIDO)\n",
    "# ================================================================\n",
    "\n",
    "save_dir = \"runs/aula9_coco_gun\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"map50_95_small\": [],\n",
    "    \"map50_95_medium\": [],\n",
    "    \"map50_95_large\": [],\n",
    "    \"ar_all_1\": [],\n",
    "    \"ar_all_10\": [],\n",
    "    \"ar_all_100\": [],\n",
    "    \"ar_small\": [],\n",
    "    \"ar_medium\": [],\n",
    "    \"ar_large\": []\n",
    "}\n",
    "\n",
    "best_map = 0.0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¯ INICIANDO TREINAMENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“‚ DiretÃ³rio de salvamento: {save_dir}\")\n",
    "print(f\"   ğŸ“Š Total de Ã©pocas: {NUM_EPOCHS}\")\n",
    "print(f\"   ğŸ“¸ Imagens por Ã©poca: {len(train_dataset)}\")\n",
    "print(f\"   ğŸ“¦ Batches por Ã©poca: {len(train_loader)}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ================================================================\n",
    "# LOOP\n",
    "# ================================================================\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print(f\"\\nğŸ”µ === INÃCIO Ã‰POCA {epoch+1}/{NUM_EPOCHS} ===\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # TREINO\n",
    "    # ---------------------------\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, NUM_EPOCHS)\n",
    "    print(f\"ğŸ”µ === FIM Ã‰POCA {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f} ===\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # ================================================================\n",
    "    # AVALIAÃ‡ÃƒO\n",
    "    # ================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š AVALIANDO MODELO NO DATASET VAL - Ã‰POCA {epoch+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # ğŸ”¥ RECRIAR modelo de prediÃ§Ã£o (obrigatÃ³rio!)\n",
    "    model_eval = DetBenchPredict(model.model)\n",
    "    model_eval.to(device)\n",
    "    model_eval.eval()\n",
    "\n",
    "    ann_path = os.path.join(root_dir, \"coco_annotations_val.json\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metrics = evaluate_coco_completo(\n",
    "            model_eval,\n",
    "            val_dataset,\n",
    "            ann_path,\n",
    "            device=device,\n",
    "            score_threshold=0.3,\n",
    "            valid_indices=valid_indices\n",
    "        )\n",
    "\n",
    "    # limpar memÃ³ria\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # voltar ao train mode\n",
    "    model.train()\n",
    "\n",
    "    # imprimir mÃ©tricas\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š MÃ‰TRICAS DA Ã‰POCA {epoch+1}:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   âœ… mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "    print(f\"   âœ… mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "    print(f\"   âœ… mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "    print(f\"   âœ… AR@100:       {metrics['ar_all_100']:.4f} ({metrics['ar_all_100']*100:.2f}%)\")\n",
    "    print(f\"\\n   ğŸ“Š MÃ©tricas por tamanho:\")\n",
    "    print(f\"      â€¢ Small:   mAP={metrics['map50_95_small']:.4f}, AR={metrics['ar_small']:.4f}\")\n",
    "    print(f\"      â€¢ Medium:  mAP={metrics['map50_95_medium']:.4f}, AR={metrics['ar_medium']:.4f}\")\n",
    "    print(f\"      â€¢ Large:   mAP={metrics['map50_95_large']:.4f}, AR={metrics['ar_large']:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # salvar histÃ³rico (TODAS as mÃ©tricas)\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    history[\"map50_95\"].append(metrics['map50_95'])\n",
    "    history[\"map50\"].append(metrics['map50'])\n",
    "    history[\"map75\"].append(metrics['map75'])\n",
    "    history[\"map50_95_small\"].append(metrics['map50_95_small'])\n",
    "    history[\"map50_95_medium\"].append(metrics['map50_95_medium'])\n",
    "    history[\"map50_95_large\"].append(metrics['map50_95_large'])\n",
    "    history[\"ar_all_1\"].append(metrics['ar_all_1'])\n",
    "    history[\"ar_all_10\"].append(metrics['ar_all_10'])\n",
    "    history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "    history[\"ar_small\"].append(metrics['ar_small'])\n",
    "    history[\"ar_medium\"].append(metrics['ar_medium'])\n",
    "    history[\"ar_large\"].append(metrics['ar_large'])\n",
    "\n",
    "    # salvar melhor modelo\n",
    "    if metrics['map50_95'] > best_map:\n",
    "        best_map = metrics['map50_95']\n",
    "        model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"ğŸ’¾ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "        print(f\"ğŸ“‚ Caminho: {model_path}\")\n",
    "\n",
    "    # salvar checkpoint\n",
    "    ckpt_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'history': history\n",
    "    }, ckpt_path)\n",
    "\n",
    "    print(f\"ğŸ’¾ Checkpoint salvo: {ckpt_path}\")\n",
    "\n",
    "# salvar histÃ³rico\n",
    "history_path = os.path.join(save_dir, \"history.json\")\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ… TREINAMENTO CONCLUÃDO!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Melhor mAP@0.5:0.95: {best_map:.4f}\")\n",
    "print(f\"ğŸ“‚ Modelo salvo: {model_path}\")\n",
    "print(f\"ğŸ“‚ HistÃ³rico salvo: {history_path}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VISUALIZAR HISTÃ“RICO DE TREINAMENTO\n",
    "# ================================================================\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, \"history.json\")):\n",
    "    with open(os.path.join(save_dir, \"history.json\"), 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "        # Criar figura com mÃºltiplos subplots\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train Loss', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Learning Rate\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history[\"epoch\"], history[\"lr\"], 'r-', linewidth=2, label='Learning Rate', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: mAP principais\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=2, label='mAP@0.5:0.95', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=2, label='mAP@0.5', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"map75\"], 'r-', linewidth=2, label='mAP@0.75', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.title('Average Precision (AP) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: mAP@0.5:0.95 (principal)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=3, label='mAP@0.5:0.95', marker='o', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP@0.5:0.95')\n",
    "    plt.title('mAP@0.5:0.95 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 5: mAP@0.5\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=3, label='mAP@0.5', marker='s', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP@0.5')\n",
    "    plt.title('mAP@0.5 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 6: AR@100\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_100\"], 'm-', linewidth=3, label='AR@100', marker='d', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('AR@100')\n",
    "    plt.title('Average Recall (AR@100) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_history.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"âœ… GrÃ¡ficos salvos em: {os.path.join(save_dir, 'training_history.png')}\")\n",
    "else:\n",
    "    print(\"âš ï¸  HistÃ³rico nÃ£o encontrado. Execute o treinamento primeiro.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CÃ“DIGO MODIFICADO PARA O LOOP DE TREINAMENTO\n",
    "# ================================================================\n",
    "# SUBSTITUA O HISTÃ“RICO E O LOOP DE TREINAMENTO NA CÃ‰LULA 9 POR ESTE CÃ“DIGO\n",
    "\n",
    "# HistÃ³rico de treinamento (MODIFICADO - adicionar todas as mÃ©tricas)\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"map50_95_small\": [],\n",
    "    \"map50_95_medium\": [],\n",
    "    \"map50_95_large\": [],\n",
    "    \"ar_all_1\": [],\n",
    "    \"ar_all_10\": [],\n",
    "    \"ar_all_100\": [],\n",
    "    \"ar_small\": [],\n",
    "    \"ar_medium\": [],\n",
    "    \"ar_large\": []\n",
    "}\n",
    "\n",
    "# Melhor mAP (usaremos mAP@0.5:0.95 como critÃ©rio)\n",
    "best_map = 0.0\n",
    "\n",
    "print(\"âœ… HistÃ³rico modificado para incluir todas as mÃ©tricas COCO\")\n",
    "print(\"   ğŸ“ Copie este cÃ³digo para substituir o histÃ³rico na cÃ©lula 9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š AVALIANDO MODELO NO DATASET DE TESTE {epoch+1}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model_eval = DetBenchPredict(model.model)\n",
    "model_eval.eval()\n",
    "model_eval.to(device)\n",
    "\n",
    "# Avaliar no dataset de teste\n",
    "ann_path = os.path.join(root_dir, \"coco_annotations_test.json\")\n",
    "metrics = evaluate_coco_completo( \n",
    "    model_eval,\n",
    "    test_dataset,\n",
    "    ann_path,\n",
    "    device=device,\n",
    "    score_threshold=0.3,\n",
    "    valid_indices=valid_indices\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š MÃ‰TRICAS DA Ã‰POCA {epoch+1}:\")\n",
    "print(f\"   âœ… mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "print(f\"   âœ… mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "print(f\"   âœ… mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Salvar histÃ³rico (ADICIONAR TODAS AS MÃ‰TRICAS)\n",
    "history[\"epoch\"].append(epoch + 1)\n",
    "history[\"train_loss\"].append(avg_loss)\n",
    "history[\"lr\"].append(current_lr)\n",
    "history[\"map50_95\"].append(metrics['map50_95'])\n",
    "history[\"map50\"].append(metrics['map50'])\n",
    "history[\"map75\"].append(metrics['map75'])\n",
    "history[\"map50_95_small\"].append(metrics['map50_95_small'])\n",
    "history[\"map50_95_medium\"].append(metrics['map50_95_medium'])\n",
    "history[\"map50_95_large\"].append(metrics['map50_95_large'])\n",
    "history[\"ar_all_1\"].append(metrics['ar_all_1'])\n",
    "history[\"ar_all_10\"].append(metrics['ar_all_10'])\n",
    "history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "history[\"ar_small\"].append(metrics['ar_small'])\n",
    "history[\"ar_medium\"].append(metrics['ar_medium'])\n",
    "history[\"ar_large\"].append(metrics['ar_large'])\n",
    "\n",
    "# Salvar melhor modelo (baseado em mAP@0.5:0.95)\n",
    "current_map = metrics['map50_95']\n",
    "if current_map > best_map:\n",
    "    best_map = current_map\n",
    "    model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\n   ğŸ’¾ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "    print(f\"      ğŸ“‚ Caminho: {model_path}\")\n",
    "\n",
    "print(\"âœ… CÃ³digo de avaliaÃ§Ã£o durante treinamento criado!\")\n",
    "print(\"   ğŸ“ Adicione este cÃ³digo no loop de treinamento (cÃ©lula 9) apÃ³s atualizar o learning rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# VISUALIZAÃ‡ÃƒO DE PREDIÃ‡Ã•ES E CÃLCULO DE TEMPO DE INFERÃŠNCIA\n",
    "# ================================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def denormalize_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Desnormaliza imagem tensor para visualizaÃ§Ã£o\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_images=6, score_threshold=0.3):\n",
    "    model.eval()\n",
    "    \n",
    "    valid_indices = [i for i in range(len(dataset)) if dataset[i] is not None]\n",
    "    selected_indices = np.random.choice(valid_indices, min(num_images, len(valid_indices)), replace=False)\n",
    "    \n",
    "    inference_times = []\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(16, 4*num_images))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(selected_indices):\n",
    "        image_tensor, target = dataset[sample_idx]\n",
    "        \n",
    "        image_np = denormalize_image(image_tensor).permute(1, 2, 0).cpu().numpy()\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "        \n",
    "        # ğŸ”¥ CORREÃ‡ÃƒO 1 â€” converter GT para float\n",
    "        gt_boxes = target['boxes'].float().cpu().numpy()\n",
    "        gt_labels = target['labels'].cpu().numpy()\n",
    "        \n",
    "        image_batch = image_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_batch)[0]   # continua igual\n",
    "        inference_time = time.time() - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        \n",
    "        # ğŸ”¥ ğŸ”¥ ğŸ”¥ CORREÃ‡ÃƒO â€” EfficientDet saÃ­da: Nx6 (x1,y1,x2,y2,score,class)\n",
    "        preds = predictions[0] if isinstance(predictions, list) else predictions\n",
    "        \n",
    "        pred_boxes  = preds[:, 0:4].float().cpu().numpy()\n",
    "        pred_scores = preds[:, 4].float().cpu().numpy()\n",
    "        pred_labels = preds[:, 5].cpu().numpy().astype(int)\n",
    "        \n",
    "        # Aplicar filtro de score\n",
    "        mask = pred_scores >= score_threshold\n",
    "        pred_boxes = pred_boxes[mask]\n",
    "        pred_scores = pred_scores[mask]\n",
    "        pred_labels = pred_labels[mask]\n",
    "        \n",
    "        # Plot Ground Truth (esquerda)\n",
    "        ax_gt = axes[idx, 0]\n",
    "        ax_gt.imshow(image_np)\n",
    "        ax_gt.set_title(f'Ground Truth (Imagem {sample_idx})', fontsize=12, fontweight='bold')\n",
    "        ax_gt.axis('off')\n",
    "        \n",
    "        # Desenhar caixas do ground truth (verde)\n",
    "        for box, label in zip(gt_boxes, gt_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height,\n",
    "                linewidth=2, edgecolor='green', facecolor='none'\n",
    "            )\n",
    "            ax_gt.add_patch(rect)\n",
    "            ax_gt.text(x1, y1-5, 'Gun (GT)', color='green', fontsize=10, fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Plot PrediÃ§Ãµes (direita)\n",
    "        ax_pred = axes[idx, 1]\n",
    "        ax_pred.imshow(image_np)\n",
    "        ax_pred.set_title(f'PrediÃ§Ãµes (Scoreâ‰¥{score_threshold}) - {inference_time*1000:.1f}ms', \n",
    "                          fontsize=12, fontweight='bold')\n",
    "        ax_pred.axis('off')\n",
    "        \n",
    "        # Desenhar caixas das prediÃ§Ãµes (vermelho)\n",
    "        for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height,\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax_pred.add_patch(rect)\n",
    "            ax_pred.text(x1, y1-5, f'Gun {score:.2f}', color='red', fontsize=10, fontweight='bold',\n",
    "                         bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Adicionar contadores\n",
    "        ax_gt.text(0.02, 0.98, f'GT: {len(gt_boxes)} objetos', \n",
    "                   transform=ax_gt.transAxes, fontsize=10, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
    "        ax_pred.text(0.02, 0.98, f'Pred: {len(pred_boxes)} objetos', \n",
    "                     transform=ax_pred.transAxes, fontsize=10, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular estatÃ­sticas de tempo\n",
    "    avg_time = np.mean(inference_times)\n",
    "    std_time = np.std(inference_times)\n",
    "    min_time = np.min(inference_times)\n",
    "    max_time = np.max(inference_times)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"â±ï¸  ESTATÃSTICAS DE TEMPO DE INFERÃŠNCIA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ğŸ“Š NÃºmero de imagens testadas: {len(inference_times)}\")\n",
    "    print(f\"   â±ï¸  Tempo mÃ©dio: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"   ğŸ“ˆ Desvio padrÃ£o: {std_time*1000:.2f} ms\")\n",
    "    print(f\"   â¬‡ï¸  Tempo mÃ­nimo: {min_time*1000:.2f} ms\")\n",
    "    print(f\"   â¬†ï¸  Tempo mÃ¡ximo: {max_time*1000:.2f} ms\")\n",
    "    print(f\"   ğŸš€ FPS mÃ©dio: {1/avg_time:.2f} frames/segundo\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return inference_times\n",
    "\n",
    "# Carregar melhor modelo se ainda nÃ£o estiver carregado\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"ğŸ“‚ Carregando melhor modelo...\")\n",
    "    # Verificar se save_dir estÃ¡ definida\n",
    "    if 'save_dir' in locals():\n",
    "        model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    else:\n",
    "        # Tentar caminho padrÃ£o\n",
    "        model_path = \"runs/aula9_coco_gun/best_model.pth\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))\n",
    "        print(f\"âœ… Modelo carregado de: {model_path}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Modelo nÃ£o encontrado em: {model_path}\")\n",
    "        print(\"   Usando modelo atual (se disponÃ­vel)\")\n",
    "\n",
    "# Garantir que o modelo estÃ¡ no device correto\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ¨ VISUALIZANDO PREDIÃ‡Ã•ES DO MODELO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“¸ NÃºmero de imagens: 6\")\n",
    "print(f\"   ğŸ¯ Score threshold: 0.3\")\n",
    "print(f\"   ğŸ’» Device: {device}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "model_eval = DetBenchPredict(model.model)\n",
    "model_eval.eval()\n",
    "model_eval.to(device)\n",
    "\n",
    "# Visualizar prediÃ§Ãµes\n",
    "inference_times = visualize_predictions(\n",
    "    model=model_eval,\n",
    "    dataset=test_dataset,\n",
    "    device=device,\n",
    "    num_images=6,\n",
    "    score_threshold=0.3\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
