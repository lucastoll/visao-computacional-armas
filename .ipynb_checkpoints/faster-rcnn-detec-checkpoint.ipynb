{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e AvaliaÃ§Ã£o - Dataset COCO Final - Classe Gun\n",
    "\n",
    "Este notebook treina e avalia um modelo Faster R-CNN usando o dataset COCO final.\n",
    "\n",
    "**Dataset:** `dataset_final_coco`\n",
    "**Classe:** Gun (apenas uma classe)\n",
    "**Treinamento:** 5% dos dados (750 imagens)\n",
    "**AvaliaÃ§Ã£o:** Subset do dataset de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ CORREÃ‡Ã•ES PARA EVITAR OOM (Out of Memory)\n",
    "\n",
    "**Problemas identificados e soluÃ§Ãµes aplicadas:**\n",
    "\n",
    "1. âœ… **Batch size reduzido**: De 12 para 4 (cÃ©lula criada acima)\n",
    "2. âœ… **ConfiguraÃ§Ã£o de memÃ³ria CUDA**: `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`\n",
    "3. âœ… **Limpeza de cache**: Adicionado `torch.cuda.empty_cache()` e `gc.collect()`\n",
    "\n",
    "**Ainda precisa ser aplicado manualmente:**\n",
    "\n",
    "1. Na cÃ©lula do DataLoader: alterar `pin_memory=True` para `pin_memory=False`\n",
    "2. Na funÃ§Ã£o `train_one_epoch`: adicionar limpeza de memÃ³ria apÃ³s cada batch\n",
    "3. No loop de treinamento: adicionar tratamento de erro OOM\n",
    "\n",
    "Veja as prÃ³ximas cÃ©lulas para cÃ³digo completo das correÃ§Ãµes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision matplotlib pillow pycocotools albumentations -q\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cache CUDA limpo. MemÃ³ria disponÃ­vel: 6.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DE GERENCIAMENTO DE MEMÃ“RIA CUDA\n",
    "# ================================================================\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configurar gerenciamento de memÃ³ria CUDA\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Limpar cache CUDA no inÃ­cio\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"âœ… Cache CUDA limpo. MemÃ³ria disponÃ­vel: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoAlbumentationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None, subset_size=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "        # Agrupar anotaÃ§Ãµes por imagem\n",
    "        self.annotations = {}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        self.ids = list(self.images.keys())\n",
    "        \n",
    "        # Limitar a um subset se especificado\n",
    "        if subset_size is not None and subset_size < len(self.ids):\n",
    "            self.ids = self.ids[:subset_size]\n",
    "            print(f\"ğŸ“Š Usando subset de {len(self.ids)} imagens (de {len(self.images)} total)\")\n",
    "        \n",
    "        # VerificaÃ§Ã£o de anotaÃ§Ãµes\n",
    "        total_images = len(self.ids)\n",
    "        images_with_anns = sum(1 for img_id in self.ids if img_id in self.annotations and len(self.annotations[img_id]) > 0)\n",
    "        total_anns = sum(len(self.annotations.get(img_id, [])) for img_id in self.ids)\n",
    "        \n",
    "        print(f\"âœ… Dataset carregado:\")\n",
    "        print(f\"   ğŸ“Š Imagens no subset: {total_images}\")\n",
    "        print(f\"   ğŸ“Š Imagens com anotaÃ§Ãµes: {images_with_anns}\")\n",
    "        print(f\"   ğŸ“Š Total de anotaÃ§Ãµes: {total_anns}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        info = self.images[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, info[\"file_name\"])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        annots = self.annotations.get(img_id, [])\n",
    "\n",
    "        # Se nÃ£o tem boxes, retorna None\n",
    "        if len(annots) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annots:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Device: cuda\n",
      "\n",
      "ğŸ“Š ConfiguraÃ§Ã£o:\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Subset size: 50 imagens\n",
      "ğŸ“Š Usando subset de 50 imagens (de 2145 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 50\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 50\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Roaming\\Python\\Python312\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset preparado: 50 imagens vÃ¡lidas\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraÃ§Ãµes\n",
    "root_dir = \"dataset_final_coco\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"ğŸ’» Device: {device}\")\n",
    "\n",
    "# Definir subset size (avaliar apenas uma parte do dataset)\n",
    "SUBSET_SIZE = 50  # Avaliar apenas 50 imagens para ver as mÃ©tricas rapidamente\n",
    "print(f\"\\nğŸ“Š ConfiguraÃ§Ã£o:\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Subset size: {SUBSET_SIZE} imagens\")\n",
    "\n",
    "# TransformaÃ§Ãµes para validaÃ§Ã£o (sem augmentations)\n",
    "val_transforms = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Carregar dataset de VALIDAÃ‡ÃƒO (usado durante treinamento)\n",
    "val_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"val\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_val.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values\n",
    "valid_indices = [i for i in range(len(val_dataset)) if val_dataset[i] is not None]\n",
    "print(f\"\\nâœ… Dataset de VALIDAÃ‡ÃƒO preparado: {len(valid_indices)} imagens vÃ¡lidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Batch size ajustado para: 4 (para evitar OOM)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# REDUZIR BATCH SIZE PARA EVITAR OOM\n",
    "# ================================================================\n",
    "\n",
    "# Reduzir batch size de 12 para 4 (GPU de 6GB)\n",
    "BATCH_SIZE = 4\n",
    "print(f\"ğŸ“¦ Batch size ajustado para: {BATCH_SIZE} (para evitar OOM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo configurado para 2 classes (background + gun)\n",
      "   ğŸ’» Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO MODELO\n",
    "# ================================================================\n",
    "\n",
    "# Carregar modelo Faster R-CNN prÃ©-treinado\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Modificar o classificador para 2 classes (background + gun)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)  # 2 classes: background + gun\n",
    "\n",
    "model.to(device)\n",
    "print(f\"âœ… Modelo configurado para 2 classes (background + gun)\")\n",
    "print(f\"   ğŸ’» Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š CONFIGURAÃ‡ÃƒO DO TREINAMENTO\n",
      "============================================================\n",
      "   ğŸ“‚ Dataset: dataset_final_coco\n",
      "   ğŸ“¸ Imagens de treino: 750 (5% dos dados)\n",
      "   ğŸ’» Device: cuda\n",
      "ğŸ“Š Usando subset de 750 imagens (de 15007 total)\n",
      "âœ… Dataset carregado:\n",
      "   ğŸ“Š Imagens no subset: 750\n",
      "   ğŸ“Š Imagens com anotaÃ§Ãµes: 750\n",
      "   ğŸ“Š Total de anotaÃ§Ãµes: 937\n",
      "\n",
      "âœ… Dataset de treino preparado:\n",
      "   ğŸ“Š Total de imagens: 750\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ”„ Batches por Ã©poca: 188\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO DATASET DE TREINO (5% = 750 imagens)\n",
    "# ================================================================\n",
    "\n",
    "# TransformaÃ§Ãµes para treino (com augmentations)\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RandomGamma(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.3))\n",
    "\n",
    "# Carregar dataset de treino com 750 imagens (5% dos dados)\n",
    "TRAIN_SUBSET_SIZE = 750\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š CONFIGURAÃ‡ÃƒO DO TREINAMENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“‚ Dataset: {root_dir}\")\n",
    "print(f\"   ğŸ“¸ Imagens de treino: {TRAIN_SUBSET_SIZE} (5% dos dados)\")\n",
    "print(f\"   ğŸ’» Device: {device}\")\n",
    "\n",
    "train_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"train\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_train.json\"),\n",
    "    transforms=train_transforms,\n",
    "    subset_size=TRAIN_SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Filtrar None values e criar DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"FunÃ§Ã£o para agrupar amostras em batch, removendo None\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "# Criar DataLoader\n",
    "BATCH_SIZE = 4  # Aumentado para acelerar treinamento\n",
    "# num_workers=0 no Windows/Jupyter evita travamentos\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # 0 funciona melhor no Windows/Jupyter\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Dataset de treino preparado:\")\n",
    "print(f\"   ğŸ“Š Total de imagens: {len(train_dataset)}\")\n",
    "print(f\"   ğŸ“¦ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ğŸ”„ Batches por Ã©poca: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Otimizador configurado:\n",
      "   ğŸ“Š Learning rate inicial: 0.005\n",
      "   ğŸ“Š Momentum: 0.9\n",
      "   ğŸ“Š Weight decay: 0.0005\n",
      "   ğŸ“Š Ã‰pocas: 10\n",
      "   ğŸ“Š Scheduler: StepLR (step_size=3, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DO OTIMIZADOR E LEARNING RATE SCHEDULER\n",
    "# ================================================================\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# ParÃ¢metros de treinamento\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Otimizador\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(f\"âœ… Otimizador configurado:\")\n",
    "print(f\"   ğŸ“Š Learning rate inicial: {LEARNING_RATE}\")\n",
    "print(f\"   ğŸ“Š Momentum: {MOMENTUM}\")\n",
    "print(f\"   ğŸ“Š Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   ğŸ“Š Ã‰pocas: {NUM_EPOCHS}\")\n",
    "print(f\"   ğŸ“Š Scheduler: StepLR (step_size=3, gamma=0.1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MemÃ³ria CUDA limpa!\n",
      "   ğŸ’¾ MemÃ³ria alocada: 0.16 GB\n",
      "   ğŸ’¾ MemÃ³ria reservada: 0.17 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LIMPAR MEMÃ“RIA CUDA (Execute esta cÃ©lula se tiver OOM)\n",
    "# ================================================================\n",
    "\n",
    "# Limpar toda a memÃ³ria CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    print(\"âœ… MemÃ³ria CUDA limpa!\")\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria alocada: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria reservada: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA nÃ£o disponÃ­vel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUNÃ‡ÃƒO DE TREINAMENTO COM LOGS DETALHADOS\n",
    "# ================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    Treina o modelo por uma Ã©poca com logs detalhados a cada batch\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca {epoch+1}\", flush=True)\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"ğŸ”µ Modelo em modo train()\", flush=True)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_classifier = 0.0\n",
    "    loss_box_reg = 0.0\n",
    "    loss_objectness = 0.0\n",
    "    loss_rpn_box_reg = 0.0\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\", flush=True)\n",
    "    print(f\"ğŸš€ Ã‰POCA {epoch+1}/{num_epochs}\", flush=True)\n",
    "    print(f\"{'='*60}\", flush=True)\n",
    "    print(f\"   ğŸ“Š Total de batches: {num_batches}\", flush=True)\n",
    "    print(f\"   ğŸ“¦ Batch size: {BATCH_SIZE}\", flush=True)\n",
    "    print(f\"   ğŸ“ˆ Learning rate: {optimizer.param_groups[0]['lr']:.6f}\", flush=True)\n",
    "    print(f\"   ğŸ• InÃ­cio: {datetime.now().strftime('%H:%M:%S')}\", flush=True)\n",
    "    print(f\"\\nğŸ”„ Iniciando treinamento...\\n\", flush=True)\n",
    "    \n",
    "    # Debug: verificar se o DataLoader estÃ¡ funcionando\n",
    "    print(f\"   ğŸ” Testando DataLoader...\", flush=True)\n",
    "    try:\n",
    "        first_batch = next(iter(data_loader))\n",
    "        print(f\"   âœ… DataLoader funcionando! Primeiro batch obtido.\", flush=True)\n",
    "        if first_batch[0] is None:\n",
    "            print(f\"   âš ï¸  ATENÃ‡ÃƒO: Primeiro batch Ã© None!\", flush=True)\n",
    "        else:\n",
    "            print(f\"   âœ… Primeiro batch tem {len(first_batch[0])} imagens\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ERRO ao obter primeiro batch: {e}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0\n",
    "    \n",
    "    # Reiniciar o iterador\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        try:\n",
    "            images, targets = next(data_loader_iter)\n",
    "        except StopIteration:\n",
    "            print(f\"   âš ï¸  DataLoader esgotado no batch {batch_idx+1}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ERRO ao obter batch {batch_idx+1}: {e}\")\n",
    "            break\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        if images is None or len(images) == 0:\n",
    "            print(f\"   âš ï¸  Batch {batch_idx+1} estÃ¡ vazio, pulando...\")\n",
    "            continue\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   ğŸ” Processando primeiro batch: {len(images)} imagens\")\n",
    "            \n",
    "        # Mover para device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   ğŸ” Imagens movidas para {device}, shape: {[img.shape for img in images[:2]]}\")\n",
    "            print(f\"   ğŸ” Iniciando forward pass...\")\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Debug: primeiro batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"   âœ… Forward pass concluÃ­do, loss: {losses.item():.4f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular tempo do batch\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Acumular losses\n",
    "        total_loss += losses.item()\n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0).item()\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0).item()\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0).item()\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0).item()\n",
    "        \n",
    "        # Calcular estatÃ­sticas\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress = ((batch_idx + 1) / num_batches) * 100\n",
    "        \n",
    "        # Estimar tempo restante\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        remaining_batches = num_batches - (batch_idx + 1)\n",
    "        estimated_remaining = avg_batch_time * remaining_batches\n",
    "        \n",
    "        # Log a cada batch\n",
    "        current_time = datetime.now().strftime('%H:%M:%S')\n",
    "        log_msg = (f\"   ğŸ“¦ [{batch_idx+1:3d}/{num_batches}] \"\n",
    "                   f\"| {progress:5.1f}% | \"\n",
    "                   f\"Loss: {losses.item():.4f} | \"\n",
    "                   f\"Avg: {avg_loss:.4f} | \"\n",
    "                   f\"Tempo: {batch_time:.2f}s | \"\n",
    "                   f\"Restante: ~{int(estimated_remaining//60)}m{int(estimated_remaining%60)}s | \"\n",
    "                   f\"ğŸ• {current_time}\")\n",
    "        print(log_msg, flush=True)  # flush=True garante que aparece imediatamente\n",
    "    \n",
    "    # Calcular mÃ©dias\n",
    "    total_time = time.time() - start_time\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_classifier = loss_classifier / num_batches\n",
    "    avg_box_reg = loss_box_reg / num_batches\n",
    "    avg_objectness = loss_objectness / num_batches\n",
    "    avg_rpn_box_reg = loss_rpn_box_reg / num_batches\n",
    "    \n",
    "    print(f\"\\nâœ… Ã‰poca {epoch+1} concluÃ­da!\")\n",
    "    print(f\"   ğŸ“Š Loss Total: {avg_total_loss:.4f}\")\n",
    "    print(f\"   ğŸ“Š Loss Classifier: {avg_classifier:.4f}\")\n",
    "    print(f\"   ğŸ“Š Loss Box Reg: {avg_box_reg:.4f}\")\n",
    "    print(f\"   ğŸ“Š Loss Objectness: {avg_objectness:.4f}\")\n",
    "    print(f\"   ğŸ“Š Loss RPN Box Reg: {avg_rpn_box_reg:.4f}\")\n",
    "    print(f\"   â±ï¸  Tempo total: {int(total_time//60)}m{int(total_time%60)}s\")\n",
    "    print(f\"   â±ï¸  Tempo mÃ©dio por batch: {sum(batch_times)/len(batch_times):.2f}s\")\n",
    "    \n",
    "    return avg_total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ¯ INICIANDO TREINAMENTO\n",
      "============================================================\n",
      "   ğŸ“‚ DiretÃ³rio de salvamento: runs/aula9_coco_gun\n",
      "   ğŸ“Š Total de Ã©pocas: 10\n",
      "   ğŸ“¸ Imagens por Ã©poca: 750\n",
      "   ğŸ“¦ Batches por Ã©poca: 188\n",
      "============================================================\n",
      "\n",
      "ğŸ”µ Iniciando loop de treinamento...\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 1/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 1\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 1/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.005000\n",
      "   ğŸ• InÃ­cio: 23:21:38\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.8708\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.8708 | Avg: 0.8708 | Tempo: 1.51s | Restante: ~4m42s | ğŸ• 23:21:39\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.3196 | Avg: 0.5952 | Tempo: 1.08s | Restante: ~4m0s | ğŸ• 23:21:41\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.4217 | Avg: 0.5374 | Tempo: 0.83s | Restante: ~3m31s | ğŸ• 23:21:43\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.3755 | Avg: 0.4969 | Tempo: 0.49s | Restante: ~3m0s | ğŸ• 23:21:44\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.5249 | Avg: 0.5025 | Tempo: 0.48s | Restante: ~2m40s | ğŸ• 23:21:45\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.3281 | Avg: 0.4734 | Tempo: 0.85s | Restante: ~2m39s | ğŸ• 23:21:47\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.4120 | Avg: 0.4647 | Tempo: 0.49s | Restante: ~2m28s | ğŸ• 23:21:48\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.1896 | Avg: 0.4303 | Tempo: 0.48s | Restante: ~2m19s | ğŸ• 23:21:49\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.2978 | Avg: 0.4156 | Tempo: 1.03s | Restante: ~2m23s | ğŸ• 23:21:50\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.3099 | Avg: 0.4050 | Tempo: 0.49s | Restante: ~2m17s | ğŸ• 23:21:51\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.2838 | Avg: 0.3940 | Tempo: 0.47s | Restante: ~2m11s | ğŸ• 23:21:53\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.4747 | Avg: 0.4007 | Tempo: 0.50s | Restante: ~2m7s | ğŸ• 23:21:54\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.4521 | Avg: 0.4047 | Tempo: 0.92s | Restante: ~2m9s | ğŸ• 23:21:55\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.2752 | Avg: 0.3954 | Tempo: 0.48s | Restante: ~2m5s | ğŸ• 23:21:56\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.2538 | Avg: 0.3860 | Tempo: 0.49s | Restante: ~2m2s | ğŸ• 23:21:57\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.2694 | Avg: 0.3787 | Tempo: 0.48s | Restante: ~1m58s | ğŸ• 23:21:58\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.1907 | Avg: 0.3676 | Tempo: 0.48s | Restante: ~1m56s | ğŸ• 23:22:00\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.2496 | Avg: 0.3611 | Tempo: 0.48s | Restante: ~1m53s | ğŸ• 23:22:01\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.1811 | Avg: 0.3516 | Tempo: 0.49s | Restante: ~1m51s | ğŸ• 23:22:02\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.3213 | Avg: 0.3501 | Tempo: 0.48s | Restante: ~1m49s | ğŸ• 23:22:03\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.6182 | Avg: 0.3629 | Tempo: 0.49s | Restante: ~1m47s | ğŸ• 23:22:04\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.1624 | Avg: 0.3537 | Tempo: 0.49s | Restante: ~1m45s | ğŸ• 23:22:05\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.1947 | Avg: 0.3468 | Tempo: 0.48s | Restante: ~1m43s | ğŸ• 23:22:07\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.1639 | Avg: 0.3392 | Tempo: 0.48s | Restante: ~1m42s | ğŸ• 23:22:08\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.2471 | Avg: 0.3355 | Tempo: 0.49s | Restante: ~1m40s | ğŸ• 23:22:09\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.2550 | Avg: 0.3324 | Tempo: 0.48s | Restante: ~1m39s | ğŸ• 23:22:10\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.1821 | Avg: 0.3269 | Tempo: 1.83s | Restante: ~1m45s | ğŸ• 23:22:13\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.2132 | Avg: 0.3228 | Tempo: 0.58s | Restante: ~1m44s | ğŸ• 23:22:14\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.2373 | Avg: 0.3199 | Tempo: 0.49s | Restante: ~1m43s | ğŸ• 23:22:16\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.1967 | Avg: 0.3157 | Tempo: 0.48s | Restante: ~1m41s | ğŸ• 23:22:17\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.1682 | Avg: 0.3110 | Tempo: 0.49s | Restante: ~1m40s | ğŸ• 23:22:18\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.2491 | Avg: 0.3091 | Tempo: 0.50s | Restante: ~1m38s | ğŸ• 23:22:19\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.2276 | Avg: 0.3066 | Tempo: 0.48s | Restante: ~1m37s | ğŸ• 23:22:20\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.3105 | Avg: 0.3067 | Tempo: 0.49s | Restante: ~1m36s | ğŸ• 23:22:22\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.2385 | Avg: 0.3048 | Tempo: 0.49s | Restante: ~1m34s | ğŸ• 23:22:23\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1607 | Avg: 0.3008 | Tempo: 0.63s | Restante: ~1m34s | ğŸ• 23:22:25\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.1901 | Avg: 0.2978 | Tempo: 0.49s | Restante: ~1m33s | ğŸ• 23:22:26\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.2018 | Avg: 0.2952 | Tempo: 0.47s | Restante: ~1m32s | ğŸ• 23:22:27\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.1546 | Avg: 0.2916 | Tempo: 0.47s | Restante: ~1m30s | ğŸ• 23:22:28\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.5738 | Avg: 0.2987 | Tempo: 0.48s | Restante: ~1m29s | ğŸ• 23:22:30\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.2751 | Avg: 0.2981 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:22:31\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.2256 | Avg: 0.2964 | Tempo: 0.49s | Restante: ~1m27s | ğŸ• 23:22:32\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.2121 | Avg: 0.2944 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:22:33\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 1.1347 | Avg: 0.3135 | Tempo: 0.75s | Restante: ~1m26s | ğŸ• 23:22:35\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.1824 | Avg: 0.3106 | Tempo: 0.51s | Restante: ~1m25s | ğŸ• 23:22:37\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1845 | Avg: 0.3079 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:22:38\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.1496 | Avg: 0.3045 | Tempo: 0.85s | Restante: ~1m24s | ğŸ• 23:22:39\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.2485 | Avg: 0.3033 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:22:40\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.1513 | Avg: 0.3002 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:22:41\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.1758 | Avg: 0.2977 | Tempo: 0.48s | Restante: ~1m22s | ğŸ• 23:22:42\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.1754 | Avg: 0.2953 | Tempo: 0.48s | Restante: ~1m21s | ğŸ• 23:22:44\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.1200 | Avg: 0.2920 | Tempo: 0.48s | Restante: ~1m20s | ğŸ• 23:22:45\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.1734 | Avg: 0.2897 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:22:46\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.0916 | Avg: 0.2861 | Tempo: 0.51s | Restante: ~1m18s | ğŸ• 23:22:47\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.2968 | Avg: 0.2863 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:22:48\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.2425 | Avg: 0.2855 | Tempo: 0.50s | Restante: ~1m17s | ğŸ• 23:22:50\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.1279 | Avg: 0.2827 | Tempo: 0.48s | Restante: ~1m16s | ğŸ• 23:22:51\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.1712 | Avg: 0.2808 | Tempo: 0.50s | Restante: ~1m15s | ğŸ• 23:22:52\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.1854 | Avg: 0.2792 | Tempo: 0.47s | Restante: ~1m14s | ğŸ• 23:22:53\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.1515 | Avg: 0.2770 | Tempo: 0.48s | Restante: ~1m13s | ğŸ• 23:22:54\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.2328 | Avg: 0.2763 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:22:55\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.2514 | Avg: 0.2759 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:22:57\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1644 | Avg: 0.2741 | Tempo: 0.50s | Restante: ~1m11s | ğŸ• 23:22:58\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.1702 | Avg: 0.2725 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:22:59\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.1446 | Avg: 0.2706 | Tempo: 0.49s | Restante: ~1m10s | ğŸ• 23:23:00\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.2126 | Avg: 0.2697 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:23:01\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.1075 | Avg: 0.2673 | Tempo: 0.50s | Restante: ~1m8s | ğŸ• 23:23:02\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.2783 | Avg: 0.2674 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:23:04\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.1501 | Avg: 0.2657 | Tempo: 0.51s | Restante: ~1m7s | ğŸ• 23:23:05\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.1933 | Avg: 0.2647 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:23:06\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0993 | Avg: 0.2624 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:23:07\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.1508 | Avg: 0.2608 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:23:08\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.1647 | Avg: 0.2595 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:23:09\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.1643 | Avg: 0.2582 | Tempo: 0.49s | Restante: ~1m4s | ğŸ• 23:23:11\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.1844 | Avg: 0.2572 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:23:12\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.3677 | Avg: 0.2587 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:23:13\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.1840 | Avg: 0.2577 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:23:14\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.2168 | Avg: 0.2572 | Tempo: 0.50s | Restante: ~1m1s | ğŸ• 23:23:15\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1594 | Avg: 0.2559 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:23:17\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.1732 | Avg: 0.2549 | Tempo: 0.48s | Restante: ~1m0s | ğŸ• 23:23:18\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.1173 | Avg: 0.2532 | Tempo: 0.66s | Restante: ~0m59s | ğŸ• 23:23:20\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.1532 | Avg: 0.2520 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:23:21\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.1987 | Avg: 0.2513 | Tempo: 0.78s | Restante: ~0m58s | ğŸ• 23:23:22\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.1095 | Avg: 0.2497 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:23:24\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.1870 | Avg: 0.2489 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:23:25\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.4544 | Avg: 0.2513 | Tempo: 0.65s | Restante: ~0m56s | ğŸ• 23:23:27\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.2430 | Avg: 0.2512 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:23:28\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.1781 | Avg: 0.2504 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:23:29\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.1234 | Avg: 0.2490 | Tempo: 1.06s | Restante: ~0m55s | ğŸ• 23:23:31\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.1472 | Avg: 0.2478 | Tempo: 0.47s | Restante: ~0m55s | ğŸ• 23:23:32\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.1423 | Avg: 0.2467 | Tempo: 0.47s | Restante: ~0m54s | ğŸ• 23:23:33\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.2554 | Avg: 0.2468 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:23:34\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0981 | Avg: 0.2452 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:23:35\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.1988 | Avg: 0.2447 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:23:36\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.2638 | Avg: 0.2449 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:23:38\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.1589 | Avg: 0.2440 | Tempo: 0.53s | Restante: ~0m51s | ğŸ• 23:23:39\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.1125 | Avg: 0.2426 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:23:40\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.1750 | Avg: 0.2419 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:23:41\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.2183 | Avg: 0.2417 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:23:42\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.1508 | Avg: 0.2408 | Tempo: 0.48s | Restante: ~0m48s | ğŸ• 23:23:43\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.1469 | Avg: 0.2399 | Tempo: 0.48s | Restante: ~0m48s | ğŸ• 23:23:45\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.1523 | Avg: 0.2390 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:23:46\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.1123 | Avg: 0.2378 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:23:47\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.3925 | Avg: 0.2393 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:23:48\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.1434 | Avg: 0.2383 | Tempo: 0.50s | Restante: ~0m45s | ğŸ• 23:23:49\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.2285 | Avg: 0.2382 | Tempo: 0.48s | Restante: ~0m45s | ğŸ• 23:23:50\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1876 | Avg: 0.2378 | Tempo: 0.48s | Restante: ~0m44s | ğŸ• 23:23:51\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.2658 | Avg: 0.2380 | Tempo: 0.50s | Restante: ~0m43s | ğŸ• 23:23:53\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.1209 | Avg: 0.2370 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:23:54\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.1312 | Avg: 0.2360 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:23:55\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.1109 | Avg: 0.2349 | Tempo: 0.48s | Restante: ~0m42s | ğŸ• 23:23:56\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.1641 | Avg: 0.2342 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:23:57\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.1700 | Avg: 0.2337 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:23:59\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.1349 | Avg: 0.2328 | Tempo: 0.47s | Restante: ~0m40s | ğŸ• 23:24:00\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.1679 | Avg: 0.2322 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:24:01\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.1239 | Avg: 0.2313 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:24:02\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.1516 | Avg: 0.2306 | Tempo: 2.40s | Restante: ~0m39s | ğŸ• 23:24:05\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0716 | Avg: 0.2293 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:24:06\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.1360 | Avg: 0.2285 | Tempo: 0.50s | Restante: ~0m38s | ğŸ• 23:24:07\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.1759 | Avg: 0.2281 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:24:08\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.1624 | Avg: 0.2275 | Tempo: 0.47s | Restante: ~0m37s | ğŸ• 23:24:10\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.1151 | Avg: 0.2266 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:24:11\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.2731 | Avg: 0.2270 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:24:12\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.2027 | Avg: 0.2268 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:24:13\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.1857 | Avg: 0.2264 | Tempo: 0.50s | Restante: ~0m35s | ğŸ• 23:24:14\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.1192 | Avg: 0.2256 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:24:15\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.1607 | Avg: 0.2251 | Tempo: 0.50s | Restante: ~0m33s | ğŸ• 23:24:17\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.1070 | Avg: 0.2242 | Tempo: 0.47s | Restante: ~0m33s | ğŸ• 23:24:18\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1142 | Avg: 0.2233 | Tempo: 0.49s | Restante: ~0m32s | ğŸ• 23:24:19\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.1838 | Avg: 0.2230 | Tempo: 0.50s | Restante: ~0m32s | ğŸ• 23:24:20\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.1073 | Avg: 0.2221 | Tempo: 0.48s | Restante: ~0m31s | ğŸ• 23:24:21\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.1110 | Avg: 0.2213 | Tempo: 0.51s | Restante: ~0m30s | ğŸ• 23:24:22\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.3215 | Avg: 0.2220 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:24:24\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.1637 | Avg: 0.2216 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:24:25\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0800 | Avg: 0.2205 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:24:26\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.1518 | Avg: 0.2200 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:24:27\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.1935 | Avg: 0.2198 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:24:28\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1357 | Avg: 0.2192 | Tempo: 0.97s | Restante: ~0m27s | ğŸ• 23:24:29\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.2490 | Avg: 0.2195 | Tempo: 0.49s | Restante: ~0m27s | ğŸ• 23:24:31\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.2460 | Avg: 0.2196 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:24:32\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0997 | Avg: 0.2188 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:24:33\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.1259 | Avg: 0.2181 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:24:34\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.5184 | Avg: 0.2202 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:24:35\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.2008 | Avg: 0.2201 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:24:36\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.3592 | Avg: 0.2211 | Tempo: 0.80s | Restante: ~0m23s | ğŸ• 23:24:38\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.1352 | Avg: 0.2205 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:24:39\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.1676 | Avg: 0.2201 | Tempo: 0.91s | Restante: ~0m22s | ğŸ• 23:24:40\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.1860 | Avg: 0.2199 | Tempo: 0.49s | Restante: ~0m22s | ğŸ• 23:24:41\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.2066 | Avg: 0.2198 | Tempo: 0.70s | Restante: ~0m21s | ğŸ• 23:24:44\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.1667 | Avg: 0.2194 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:24:45\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.3259 | Avg: 0.2201 | Tempo: 0.50s | Restante: ~0m20s | ğŸ• 23:24:46\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0980 | Avg: 0.2193 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:24:48\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.1517 | Avg: 0.2189 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:24:49\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.1912 | Avg: 0.2187 | Tempo: 0.48s | Restante: ~0m18s | ğŸ• 23:24:50\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.1023 | Avg: 0.2180 | Tempo: 0.99s | Restante: ~0m18s | ğŸ• 23:24:51\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.1605 | Avg: 0.2176 | Tempo: 0.47s | Restante: ~0m17s | ğŸ• 23:24:52\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.4497 | Avg: 0.2191 | Tempo: 0.50s | Restante: ~0m17s | ğŸ• 23:24:53\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.2211 | Avg: 0.2191 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:24:55\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.1190 | Avg: 0.2185 | Tempo: 0.80s | Restante: ~0m16s | ğŸ• 23:24:56\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.1280 | Avg: 0.2179 | Tempo: 0.50s | Restante: ~0m15s | ğŸ• 23:24:57\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.1229 | Avg: 0.2173 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:24:58\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.1462 | Avg: 0.2169 | Tempo: 0.85s | Restante: ~0m14s | ğŸ• 23:24:59\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0959 | Avg: 0.2161 | Tempo: 0.49s | Restante: ~0m13s | ğŸ• 23:25:00\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.1774 | Avg: 0.2159 | Tempo: 0.50s | Restante: ~0m13s | ğŸ• 23:25:01\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.2205 | Avg: 0.2159 | Tempo: 0.48s | Restante: ~0m12s | ğŸ• 23:25:03\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.1086 | Avg: 0.2153 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:25:04\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.1618 | Avg: 0.2150 | Tempo: 0.50s | Restante: ~0m11s | ğŸ• 23:25:05\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0833 | Avg: 0.2142 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:25:06\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.1698 | Avg: 0.2139 | Tempo: 0.47s | Restante: ~0m10s | ğŸ• 23:25:07\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.1375 | Avg: 0.2135 | Tempo: 0.50s | Restante: ~0m9s | ğŸ• 23:25:08\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.1615 | Avg: 0.2132 | Tempo: 0.57s | Restante: ~0m9s | ğŸ• 23:25:10\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.1322 | Avg: 0.2127 | Tempo: 0.48s | Restante: ~0m8s | ğŸ• 23:25:11\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0912 | Avg: 0.2120 | Tempo: 0.48s | Restante: ~0m8s | ğŸ• 23:25:12\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0875 | Avg: 0.2113 | Tempo: 0.47s | Restante: ~0m7s | ğŸ• 23:25:13\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.1040 | Avg: 0.2107 | Tempo: 0.51s | Restante: ~0m7s | ğŸ• 23:25:14\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0960 | Avg: 0.2100 | Tempo: 0.50s | Restante: ~0m6s | ğŸ• 23:25:15\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.1590 | Avg: 0.2097 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:25:17\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.1822 | Avg: 0.2096 | Tempo: 0.50s | Restante: ~0m5s | ğŸ• 23:25:18\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.1252 | Avg: 0.2091 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:25:19\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0700 | Avg: 0.2083 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:25:20\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.1969 | Avg: 0.2083 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:25:21\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.1643 | Avg: 0.2080 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:25:22\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.2612 | Avg: 0.2083 | Tempo: 0.80s | Restante: ~0m2s | ğŸ• 23:25:24\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.2070 | Avg: 0.2083 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:25:25\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.2136 | Avg: 0.2083 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:25:26\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.1076 | Avg: 0.2078 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:25:27\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0900 | Avg: 0.2071 | Tempo: 0.48s | Restante: ~0m0s | ğŸ• 23:25:28\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.1658 | Avg: 0.2069 | Tempo: 0.27s | Restante: ~0m0s | ğŸ• 23:25:29\n",
      "\n",
      "âœ… Ã‰poca 1 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.2069\n",
      "   ğŸ“Š Loss Classifier: 0.0776\n",
      "   ğŸ“Š Loss Box Reg: 0.0917\n",
      "   ğŸ“Š Loss Objectness: 0.0295\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0081\n",
      "   â±ï¸  Tempo total: 3m51s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.55s\n",
      "ğŸ”µ === FIM Ã‰POCA 1/10, Loss: 0.2069 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.2069)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_1.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 2/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 2\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 2/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.005000\n",
      "   ğŸ• InÃ­cio: 23:25:30\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 640, 640])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.2958\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.2958 | Avg: 0.2958 | Tempo: 0.50s | Restante: ~1m32s | ğŸ• 23:25:32\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.1622 | Avg: 0.2290 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:25:33\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.4092 | Avg: 0.2890 | Tempo: 0.50s | Restante: ~1m30s | ğŸ• 23:25:34\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.1018 | Avg: 0.2422 | Tempo: 0.50s | Restante: ~1m30s | ğŸ• 23:25:35\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.1491 | Avg: 0.2236 | Tempo: 0.49s | Restante: ~1m30s | ğŸ• 23:25:36\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.1595 | Avg: 0.2129 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:25:37\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.1403 | Avg: 0.2025 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:25:38\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0892 | Avg: 0.1884 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:25:39\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.1478 | Avg: 0.1839 | Tempo: 0.54s | Restante: ~1m28s | ğŸ• 23:25:41\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.1729 | Avg: 0.1828 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:25:42\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.3271 | Avg: 0.1959 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:25:43\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.2613 | Avg: 0.2013 | Tempo: 0.51s | Restante: ~1m27s | ğŸ• 23:25:45\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.1349 | Avg: 0.1962 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:25:46\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.2061 | Avg: 0.1969 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:25:47\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.1037 | Avg: 0.1907 | Tempo: 0.51s | Restante: ~1m25s | ğŸ• 23:25:48\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.1416 | Avg: 0.1876 | Tempo: 0.51s | Restante: ~1m25s | ğŸ• 23:25:49\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.1282 | Avg: 0.1841 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:25:50\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.1397 | Avg: 0.1817 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:25:51\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.1839 | Avg: 0.1818 | Tempo: 0.50s | Restante: ~1m23s | ğŸ• 23:25:53\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0952 | Avg: 0.1775 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:25:54\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0839 | Avg: 0.1730 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:25:55\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.1339 | Avg: 0.1712 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:25:56\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.1368 | Avg: 0.1697 | Tempo: 0.48s | Restante: ~1m21s | ğŸ• 23:25:57\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.1280 | Avg: 0.1680 | Tempo: 0.87s | Restante: ~1m23s | ğŸ• 23:25:58\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.1133 | Avg: 0.1658 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:25:59\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.1392 | Avg: 0.1648 | Tempo: 0.50s | Restante: ~1m22s | ğŸ• 23:26:00\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0956 | Avg: 0.1622 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:26:02\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0576 | Avg: 0.1585 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:26:03\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.1278 | Avg: 0.1574 | Tempo: 0.48s | Restante: ~1m20s | ğŸ• 23:26:04\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.0930 | Avg: 0.1553 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:26:05\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.1503 | Avg: 0.1551 | Tempo: 0.89s | Restante: ~1m21s | ğŸ• 23:26:06\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0946 | Avg: 0.1532 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:26:07\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0906 | Avg: 0.1513 | Tempo: 0.51s | Restante: ~1m19s | ğŸ• 23:26:08\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0953 | Avg: 0.1497 | Tempo: 0.52s | Restante: ~1m19s | ğŸ• 23:26:09\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.1327 | Avg: 0.1492 | Tempo: 0.50s | Restante: ~1m18s | ğŸ• 23:26:10\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1093 | Avg: 0.1481 | Tempo: 0.47s | Restante: ~1m18s | ğŸ• 23:26:12\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.1358 | Avg: 0.1477 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:26:13\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0957 | Avg: 0.1464 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:26:14\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.1671 | Avg: 0.1469 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:26:15\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.1181 | Avg: 0.1462 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:26:16\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.1156 | Avg: 0.1454 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:26:17\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.1056 | Avg: 0.1445 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:26:18\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.1402 | Avg: 0.1444 | Tempo: 0.48s | Restante: ~1m13s | ğŸ• 23:26:19\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.1261 | Avg: 0.1440 | Tempo: 0.47s | Restante: ~1m13s | ğŸ• 23:26:21\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.4350 | Avg: 0.1504 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:26:22\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1077 | Avg: 0.1495 | Tempo: 0.90s | Restante: ~1m13s | ğŸ• 23:26:23\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.1150 | Avg: 0.1488 | Tempo: 0.50s | Restante: ~1m12s | ğŸ• 23:26:24\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.1086 | Avg: 0.1479 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:26:25\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.1021 | Avg: 0.1470 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:26:26\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.1125 | Avg: 0.1463 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:26:27\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.1038 | Avg: 0.1455 | Tempo: 0.49s | Restante: ~1m10s | ğŸ• 23:26:28\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.1338 | Avg: 0.1453 | Tempo: 0.50s | Restante: ~1m9s | ğŸ• 23:26:30\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0934 | Avg: 0.1443 | Tempo: 0.47s | Restante: ~1m9s | ğŸ• 23:26:31\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.1016 | Avg: 0.1435 | Tempo: 0.47s | Restante: ~1m8s | ğŸ• 23:26:32\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0855 | Avg: 0.1424 | Tempo: 0.48s | Restante: ~1m8s | ğŸ• 23:26:33\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.1544 | Avg: 0.1427 | Tempo: 0.71s | Restante: ~1m8s | ğŸ• 23:26:36\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.2874 | Avg: 0.1452 | Tempo: 0.49s | Restante: ~1m7s | ğŸ• 23:26:37\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.1653 | Avg: 0.1455 | Tempo: 0.71s | Restante: ~1m7s | ğŸ• 23:26:40\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.1652 | Avg: 0.1459 | Tempo: 10.49s | Restante: ~1m28s | ğŸ• 23:26:51\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.2583 | Avg: 0.1477 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:26:52\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.1665 | Avg: 0.1481 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:26:53\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.1721 | Avg: 0.1484 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:26:54\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1028 | Avg: 0.1477 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:26:55\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0980 | Avg: 0.1469 | Tempo: 0.50s | Restante: ~1m23s | ğŸ• 23:26:56\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.1442 | Avg: 0.1469 | Tempo: 0.48s | Restante: ~1m22s | ğŸ• 23:26:58\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.1249 | Avg: 0.1466 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:26:59\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0572 | Avg: 0.1452 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:27:00\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.1212 | Avg: 0.1449 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:27:01\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0756 | Avg: 0.1439 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:27:02\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.1137 | Avg: 0.1434 | Tempo: 0.57s | Restante: ~1m17s | ğŸ• 23:27:03\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.1148 | Avg: 0.1430 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:27:04\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.1004 | Avg: 0.1424 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:27:05\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.1680 | Avg: 0.1428 | Tempo: 0.50s | Restante: ~1m14s | ğŸ• 23:27:07\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.1128 | Avg: 0.1424 | Tempo: 0.47s | Restante: ~1m13s | ğŸ• 23:27:08\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.1338 | Avg: 0.1423 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:27:09\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.1722 | Avg: 0.1427 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:27:10\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0837 | Avg: 0.1419 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:27:11\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.1291 | Avg: 0.1417 | Tempo: 0.48s | Restante: ~1m10s | ğŸ• 23:27:12\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1311 | Avg: 0.1416 | Tempo: 0.48s | Restante: ~1m9s | ğŸ• 23:27:13\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.1451 | Avg: 0.1416 | Tempo: 0.50s | Restante: ~1m8s | ğŸ• 23:27:14\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0636 | Avg: 0.1407 | Tempo: 0.49s | Restante: ~1m7s | ğŸ• 23:27:15\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.1009 | Avg: 0.1402 | Tempo: 0.49s | Restante: ~1m7s | ğŸ• 23:27:17\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.1321 | Avg: 0.1401 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:27:18\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.1062 | Avg: 0.1397 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:27:19\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.2201 | Avg: 0.1406 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:27:20\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.1063 | Avg: 0.1402 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:27:21\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0865 | Avg: 0.1396 | Tempo: 0.47s | Restante: ~1m2s | ğŸ• 23:27:22\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.1528 | Avg: 0.1398 | Tempo: 0.48s | Restante: ~1m2s | ğŸ• 23:27:23\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.1436 | Avg: 0.1398 | Tempo: 0.49s | Restante: ~1m1s | ğŸ• 23:27:24\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.2339 | Avg: 0.1409 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:27:26\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.1064 | Avg: 0.1405 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:27:27\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.1689 | Avg: 0.1408 | Tempo: 0.50s | Restante: ~0m59s | ğŸ• 23:27:28\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.1323 | Avg: 0.1407 | Tempo: 0.49s | Restante: ~0m58s | ğŸ• 23:27:29\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.1308 | Avg: 0.1406 | Tempo: 0.50s | Restante: ~0m57s | ğŸ• 23:27:30\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.2961 | Avg: 0.1422 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:27:31\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.1339 | Avg: 0.1421 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:27:32\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0906 | Avg: 0.1416 | Tempo: 0.50s | Restante: ~0m55s | ğŸ• 23:27:33\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.1375 | Avg: 0.1416 | Tempo: 1.03s | Restante: ~0m55s | ğŸ• 23:27:35\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.1292 | Avg: 0.1415 | Tempo: 0.49s | Restante: ~0m54s | ğŸ• 23:27:36\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.1653 | Avg: 0.1417 | Tempo: 0.66s | Restante: ~0m53s | ğŸ• 23:27:38\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.1059 | Avg: 0.1413 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:27:39\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.1617 | Avg: 0.1415 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:27:40\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.1380 | Avg: 0.1415 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:27:41\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.1386 | Avg: 0.1415 | Tempo: 0.66s | Restante: ~0m51s | ğŸ• 23:27:44\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.1304 | Avg: 0.1414 | Tempo: 0.48s | Restante: ~0m50s | ğŸ• 23:27:45\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.1345 | Avg: 0.1413 | Tempo: 0.48s | Restante: ~0m49s | ğŸ• 23:27:46\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1608 | Avg: 0.1415 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:27:47\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.2028 | Avg: 0.1421 | Tempo: 0.48s | Restante: ~0m48s | ğŸ• 23:27:48\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0701 | Avg: 0.1414 | Tempo: 0.65s | Restante: ~0m47s | ğŸ• 23:27:50\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.1478 | Avg: 0.1415 | Tempo: 0.50s | Restante: ~0m47s | ğŸ• 23:27:51\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.1283 | Avg: 0.1413 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:27:53\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.1643 | Avg: 0.1415 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:27:54\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.0865 | Avg: 0.1411 | Tempo: 0.48s | Restante: ~0m45s | ğŸ• 23:27:55\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.1013 | Avg: 0.1407 | Tempo: 0.48s | Restante: ~0m44s | ğŸ• 23:27:56\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.0679 | Avg: 0.1401 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:27:57\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.1281 | Avg: 0.1400 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:27:58\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0989 | Avg: 0.1396 | Tempo: 0.50s | Restante: ~0m42s | ğŸ• 23:27:59\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.2225 | Avg: 0.1403 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:28:00\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.2076 | Avg: 0.1409 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:28:02\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.1563 | Avg: 0.1410 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:28:03\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.1990 | Avg: 0.1415 | Tempo: 0.65s | Restante: ~0m39s | ğŸ• 23:28:05\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.1653 | Avg: 0.1417 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:28:06\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.1780 | Avg: 0.1420 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:28:07\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.1387 | Avg: 0.1420 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:28:08\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.1707 | Avg: 0.1422 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:28:09\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0956 | Avg: 0.1418 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:28:10\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.1100 | Avg: 0.1416 | Tempo: 0.48s | Restante: ~0m35s | ğŸ• 23:28:12\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.1129 | Avg: 0.1413 | Tempo: 1.09s | Restante: ~0m35s | ğŸ• 23:28:13\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1125 | Avg: 0.1411 | Tempo: 0.50s | Restante: ~0m35s | ğŸ• 23:28:14\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.1328 | Avg: 0.1411 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:28:15\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0890 | Avg: 0.1407 | Tempo: 0.50s | Restante: ~0m33s | ğŸ• 23:28:16\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0863 | Avg: 0.1402 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:28:17\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.2168 | Avg: 0.1408 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:28:18\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0952 | Avg: 0.1405 | Tempo: 0.51s | Restante: ~0m31s | ğŸ• 23:28:19\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0704 | Avg: 0.1400 | Tempo: 0.47s | Restante: ~0m31s | ğŸ• 23:28:21\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.1292 | Avg: 0.1399 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:28:22\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.1666 | Avg: 0.1401 | Tempo: 0.78s | Restante: ~0m30s | ğŸ• 23:28:23\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1709 | Avg: 0.1403 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:28:24\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0998 | Avg: 0.1400 | Tempo: 0.51s | Restante: ~0m28s | ğŸ• 23:28:25\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0817 | Avg: 0.1396 | Tempo: 0.48s | Restante: ~0m28s | ğŸ• 23:28:26\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.1044 | Avg: 0.1393 | Tempo: 0.49s | Restante: ~0m27s | ğŸ• 23:28:27\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.1493 | Avg: 0.1394 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:28:28\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.1064 | Avg: 0.1392 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:28:30\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.1105 | Avg: 0.1390 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:28:31\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.1196 | Avg: 0.1388 | Tempo: 0.50s | Restante: ~0m25s | ğŸ• 23:28:32\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.1403 | Avg: 0.1389 | Tempo: 0.56s | Restante: ~0m24s | ğŸ• 23:28:33\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.2633 | Avg: 0.1397 | Tempo: 0.50s | Restante: ~0m23s | ğŸ• 23:28:34\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.1444 | Avg: 0.1397 | Tempo: 1.19s | Restante: ~0m23s | ğŸ• 23:28:35\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0785 | Avg: 0.1393 | Tempo: 0.50s | Restante: ~0m22s | ğŸ• 23:28:36\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.1042 | Avg: 0.1391 | Tempo: 0.50s | Restante: ~0m22s | ğŸ• 23:28:38\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0846 | Avg: 0.1387 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:28:39\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0887 | Avg: 0.1384 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:28:40\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.1494 | Avg: 0.1385 | Tempo: 0.50s | Restante: ~0m20s | ğŸ• 23:28:41\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.1324 | Avg: 0.1384 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:28:42\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.1241 | Avg: 0.1383 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:28:43\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0862 | Avg: 0.1380 | Tempo: 0.49s | Restante: ~0m18s | ğŸ• 23:28:44\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0789 | Avg: 0.1376 | Tempo: 0.51s | Restante: ~0m18s | ğŸ• 23:28:45\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.0504 | Avg: 0.1371 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:28:46\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.1061 | Avg: 0.1369 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:28:48\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.1896 | Avg: 0.1372 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:28:49\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.1959 | Avg: 0.1376 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:28:50\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.3734 | Avg: 0.1390 | Tempo: 0.49s | Restante: ~0m15s | ğŸ• 23:28:51\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0924 | Avg: 0.1387 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:28:52\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0948 | Avg: 0.1385 | Tempo: 0.50s | Restante: ~0m13s | ğŸ• 23:28:53\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.1418 | Avg: 0.1385 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:28:54\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.1383 | Avg: 0.1385 | Tempo: 0.48s | Restante: ~0m12s | ğŸ• 23:28:55\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.1006 | Avg: 0.1383 | Tempo: 0.50s | Restante: ~0m12s | ğŸ• 23:28:57\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0890 | Avg: 0.1380 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:28:58\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.1513 | Avg: 0.1381 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:28:59\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.1250 | Avg: 0.1380 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:29:00\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0852 | Avg: 0.1377 | Tempo: 0.50s | Restante: ~0m9s | ğŸ• 23:29:01\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.1459 | Avg: 0.1377 | Tempo: 0.52s | Restante: ~0m9s | ğŸ• 23:29:02\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0980 | Avg: 0.1375 | Tempo: 0.51s | Restante: ~0m8s | ğŸ• 23:29:03\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0689 | Avg: 0.1371 | Tempo: 0.47s | Restante: ~0m8s | ğŸ• 23:29:04\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0658 | Avg: 0.1367 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:29:05\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.1447 | Avg: 0.1367 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:29:07\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.1247 | Avg: 0.1367 | Tempo: 0.50s | Restante: ~0m6s | ğŸ• 23:29:08\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.2264 | Avg: 0.1372 | Tempo: 0.52s | Restante: ~0m5s | ğŸ• 23:29:09\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.3342 | Avg: 0.1383 | Tempo: 0.67s | Restante: ~0m5s | ğŸ• 23:29:11\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0923 | Avg: 0.1380 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:29:12\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0922 | Avg: 0.1378 | Tempo: 0.50s | Restante: ~0m3s | ğŸ• 23:29:13\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.1075 | Avg: 0.1376 | Tempo: 0.95s | Restante: ~0m3s | ğŸ• 23:29:14\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0684 | Avg: 0.1372 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:29:16\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.1472 | Avg: 0.1373 | Tempo: 0.50s | Restante: ~0m2s | ğŸ• 23:29:17\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0543 | Avg: 0.1368 | Tempo: 0.50s | Restante: ~0m1s | ğŸ• 23:29:18\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0859 | Avg: 0.1366 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:29:19\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0687 | Avg: 0.1362 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:29:20\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0557 | Avg: 0.1358 | Tempo: 0.24s | Restante: ~0m0s | ğŸ• 23:29:21\n",
      "\n",
      "âœ… Ã‰poca 2 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.1358\n",
      "   ğŸ“Š Loss Classifier: 0.0502\n",
      "   ğŸ“Š Loss Box Reg: 0.0706\n",
      "   ğŸ“Š Loss Objectness: 0.0095\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0056\n",
      "   â±ï¸  Tempo total: 3m50s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.57s\n",
      "ğŸ”µ === FIM Ã‰POCA 2/10, Loss: 0.1358 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.1358)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_2.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 3/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 3\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 3/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.005000\n",
      "   ğŸ• InÃ­cio: 23:29:22\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.1465\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.1465 | Avg: 0.1465 | Tempo: 0.49s | Restante: ~1m32s | ğŸ• 23:29:23\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.1818 | Avg: 0.1642 | Tempo: 0.50s | Restante: ~1m31s | ğŸ• 23:29:24\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.1498 | Avg: 0.1594 | Tempo: 0.49s | Restante: ~1m30s | ğŸ• 23:29:25\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.1153 | Avg: 0.1484 | Tempo: 0.56s | Restante: ~1m33s | ğŸ• 23:29:26\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0773 | Avg: 0.1341 | Tempo: 0.48s | Restante: ~1m32s | ğŸ• 23:29:27\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.1238 | Avg: 0.1324 | Tempo: 0.49s | Restante: ~1m31s | ğŸ• 23:29:29\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0928 | Avg: 0.1268 | Tempo: 0.49s | Restante: ~1m30s | ğŸ• 23:29:30\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.1153 | Avg: 0.1253 | Tempo: 0.50s | Restante: ~1m29s | ğŸ• 23:29:31\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0892 | Avg: 0.1213 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:29:32\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.1722 | Avg: 0.1264 | Tempo: 0.50s | Restante: ~1m28s | ğŸ• 23:29:33\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.1492 | Avg: 0.1285 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:29:34\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.1674 | Avg: 0.1317 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:29:35\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.1047 | Avg: 0.1296 | Tempo: 0.77s | Restante: ~1m30s | ğŸ• 23:29:36\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0908 | Avg: 0.1269 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:29:38\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.1120 | Avg: 0.1259 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:29:39\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.1401 | Avg: 0.1268 | Tempo: 0.49s | Restante: ~1m27s | ğŸ• 23:29:40\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.3049 | Avg: 0.1373 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:29:41\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0605 | Avg: 0.1330 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:29:42\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0761 | Avg: 0.1300 | Tempo: 0.85s | Restante: ~1m28s | ğŸ• 23:29:43\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.1071 | Avg: 0.1289 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:29:44\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0617 | Avg: 0.1257 | Tempo: 0.57s | Restante: ~1m27s | ğŸ• 23:29:45\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0979 | Avg: 0.1244 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:29:47\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.1263 | Avg: 0.1245 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:29:48\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0895 | Avg: 0.1230 | Tempo: 0.51s | Restante: ~1m25s | ğŸ• 23:29:49\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.1043 | Avg: 0.1223 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:29:50\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0844 | Avg: 0.1208 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:29:51\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0746 | Avg: 0.1191 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:29:52\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.1272 | Avg: 0.1194 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:29:53\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0727 | Avg: 0.1178 | Tempo: 0.50s | Restante: ~1m21s | ğŸ• 23:29:54\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.1604 | Avg: 0.1192 | Tempo: 0.47s | Restante: ~1m21s | ğŸ• 23:29:55\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0973 | Avg: 0.1185 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:29:57\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.1542 | Avg: 0.1196 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:29:58\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0955 | Avg: 0.1189 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:29:59\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0853 | Avg: 0.1179 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:30:00\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.1092 | Avg: 0.1176 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:30:01\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.0902 | Avg: 0.1169 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:30:02\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0950 | Avg: 0.1163 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:30:03\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0681 | Avg: 0.1150 | Tempo: 0.50s | Restante: ~1m16s | ğŸ• 23:30:04\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0920 | Avg: 0.1144 | Tempo: 0.52s | Restante: ~1m15s | ğŸ• 23:30:06\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.1482 | Avg: 0.1153 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:30:07\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.1236 | Avg: 0.1155 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:30:08\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.1239 | Avg: 0.1157 | Tempo: 0.65s | Restante: ~1m14s | ğŸ• 23:30:10\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0901 | Avg: 0.1151 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:30:11\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.2260 | Avg: 0.1176 | Tempo: 0.50s | Restante: ~1m13s | ğŸ• 23:30:12\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.1590 | Avg: 0.1185 | Tempo: 0.50s | Restante: ~1m12s | ğŸ• 23:30:13\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1342 | Avg: 0.1189 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:30:15\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0671 | Avg: 0.1178 | Tempo: 0.65s | Restante: ~1m12s | ğŸ• 23:30:17\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0710 | Avg: 0.1168 | Tempo: 0.50s | Restante: ~1m11s | ğŸ• 23:30:18\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0771 | Avg: 0.1160 | Tempo: 0.50s | Restante: ~1m11s | ğŸ• 23:30:19\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0850 | Avg: 0.1154 | Tempo: 0.48s | Restante: ~1m10s | ğŸ• 23:30:20\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0590 | Avg: 0.1143 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:30:21\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0856 | Avg: 0.1137 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:30:22\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.1517 | Avg: 0.1144 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:30:24\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.1052 | Avg: 0.1143 | Tempo: 0.51s | Restante: ~1m8s | ğŸ• 23:30:25\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.2216 | Avg: 0.1162 | Tempo: 0.50s | Restante: ~1m7s | ğŸ• 23:30:26\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.1863 | Avg: 0.1175 | Tempo: 0.48s | Restante: ~1m7s | ğŸ• 23:30:27\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0856 | Avg: 0.1169 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:30:28\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.1158 | Avg: 0.1169 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:30:29\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.1063 | Avg: 0.1167 | Tempo: 0.50s | Restante: ~1m5s | ğŸ• 23:30:30\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0903 | Avg: 0.1163 | Tempo: 0.48s | Restante: ~1m5s | ğŸ• 23:30:31\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.1090 | Avg: 0.1161 | Tempo: 0.50s | Restante: ~1m4s | ğŸ• 23:30:33\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.1433 | Avg: 0.1166 | Tempo: 1.83s | Restante: ~1m6s | ğŸ• 23:30:35\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1191 | Avg: 0.1166 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:30:36\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.1211 | Avg: 0.1167 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:30:37\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.1063 | Avg: 0.1165 | Tempo: 0.50s | Restante: ~1m4s | ğŸ• 23:30:38\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.1011 | Avg: 0.1163 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:30:39\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.1387 | Avg: 0.1166 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:30:40\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.1282 | Avg: 0.1168 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:30:42\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0681 | Avg: 0.1161 | Tempo: 0.48s | Restante: ~1m2s | ğŸ• 23:30:43\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.1481 | Avg: 0.1166 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:30:44\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.1383 | Avg: 0.1169 | Tempo: 0.87s | Restante: ~1m1s | ğŸ• 23:30:45\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.1418 | Avg: 0.1172 | Tempo: 0.51s | Restante: ~1m1s | ğŸ• 23:30:46\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.1112 | Avg: 0.1171 | Tempo: 0.50s | Restante: ~1m0s | ğŸ• 23:30:47\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0869 | Avg: 0.1167 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:30:48\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0927 | Avg: 0.1164 | Tempo: 0.48s | Restante: ~0m59s | ğŸ• 23:30:49\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0758 | Avg: 0.1159 | Tempo: 0.47s | Restante: ~0m58s | ğŸ• 23:30:50\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.1386 | Avg: 0.1162 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:30:52\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0961 | Avg: 0.1159 | Tempo: 0.48s | Restante: ~0m57s | ğŸ• 23:30:53\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1124 | Avg: 0.1159 | Tempo: 0.50s | Restante: ~0m57s | ğŸ• 23:30:54\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0774 | Avg: 0.1154 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:30:55\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0693 | Avg: 0.1148 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:30:56\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0706 | Avg: 0.1143 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:30:57\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0596 | Avg: 0.1136 | Tempo: 0.49s | Restante: ~0m54s | ğŸ• 23:30:58\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.1033 | Avg: 0.1135 | Tempo: 0.47s | Restante: ~0m54s | ğŸ• 23:30:59\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.1407 | Avg: 0.1138 | Tempo: 0.55s | Restante: ~0m53s | ğŸ• 23:31:01\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0587 | Avg: 0.1132 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:31:02\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0981 | Avg: 0.1130 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:31:03\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0665 | Avg: 0.1125 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:31:04\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.1126 | Avg: 0.1125 | Tempo: 0.50s | Restante: ~0m51s | ğŸ• 23:31:05\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.1243 | Avg: 0.1126 | Tempo: 0.65s | Restante: ~0m51s | ğŸ• 23:31:07\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0611 | Avg: 0.1120 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:31:08\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.1344 | Avg: 0.1123 | Tempo: 0.48s | Restante: ~0m50s | ğŸ• 23:31:10\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0640 | Avg: 0.1118 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:31:11\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0673 | Avg: 0.1113 | Tempo: 0.48s | Restante: ~0m49s | ğŸ• 23:31:12\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.1201 | Avg: 0.1114 | Tempo: 0.47s | Restante: ~0m48s | ğŸ• 23:31:13\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.1361 | Avg: 0.1116 | Tempo: 0.57s | Restante: ~0m47s | ğŸ• 23:31:14\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.1036 | Avg: 0.1115 | Tempo: 0.58s | Restante: ~0m47s | ğŸ• 23:31:15\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.1635 | Avg: 0.1121 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:31:16\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0653 | Avg: 0.1116 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:31:17\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.1290 | Avg: 0.1118 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:31:19\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.3424 | Avg: 0.1141 | Tempo: 0.50s | Restante: ~0m45s | ğŸ• 23:31:20\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0603 | Avg: 0.1135 | Tempo: 0.51s | Restante: ~0m44s | ğŸ• 23:31:21\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0710 | Avg: 0.1131 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:31:22\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.0896 | Avg: 0.1129 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:31:23\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0815 | Avg: 0.1126 | Tempo: 0.72s | Restante: ~0m43s | ğŸ• 23:31:24\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.1097 | Avg: 0.1126 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:31:25\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1978 | Avg: 0.1134 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:31:26\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.2802 | Avg: 0.1149 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:31:27\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.1429 | Avg: 0.1152 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:31:29\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.1279 | Avg: 0.1153 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:31:30\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.0870 | Avg: 0.1150 | Tempo: 0.50s | Restante: ~0m40s | ğŸ• 23:31:31\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.2632 | Avg: 0.1163 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:31:32\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.0581 | Avg: 0.1158 | Tempo: 0.47s | Restante: ~0m38s | ğŸ• 23:31:33\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0592 | Avg: 0.1153 | Tempo: 0.89s | Restante: ~0m38s | ğŸ• 23:31:34\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.1395 | Avg: 0.1155 | Tempo: 0.53s | Restante: ~0m38s | ğŸ• 23:31:35\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.2303 | Avg: 0.1165 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:31:36\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0978 | Avg: 0.1164 | Tempo: 0.50s | Restante: ~0m37s | ğŸ• 23:31:38\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0446 | Avg: 0.1158 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:31:39\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.1227 | Avg: 0.1158 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:31:40\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.1003 | Avg: 0.1157 | Tempo: 0.50s | Restante: ~0m35s | ğŸ• 23:31:41\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.0967 | Avg: 0.1155 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:31:42\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0594 | Avg: 0.1151 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:31:43\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0607 | Avg: 0.1146 | Tempo: 0.55s | Restante: ~0m33s | ğŸ• 23:31:44\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.1147 | Avg: 0.1146 | Tempo: 0.50s | Restante: ~0m33s | ğŸ• 23:31:45\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.1122 | Avg: 0.1146 | Tempo: 0.50s | Restante: ~0m32s | ğŸ• 23:31:46\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0935 | Avg: 0.1145 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:31:48\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.1356 | Avg: 0.1146 | Tempo: 0.48s | Restante: ~0m31s | ğŸ• 23:31:49\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.1442 | Avg: 0.1148 | Tempo: 0.49s | Restante: ~0m31s | ğŸ• 23:31:50\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1396 | Avg: 0.1150 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:31:51\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.4430 | Avg: 0.1176 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:31:52\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0782 | Avg: 0.1173 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:31:53\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0578 | Avg: 0.1168 | Tempo: 0.81s | Restante: ~0m29s | ğŸ• 23:31:54\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.0814 | Avg: 0.1165 | Tempo: 0.50s | Restante: ~0m28s | ğŸ• 23:31:55\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.1570 | Avg: 0.1168 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:31:57\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.1039 | Avg: 0.1168 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:31:58\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.1359 | Avg: 0.1169 | Tempo: 0.49s | Restante: ~0m27s | ğŸ• 23:31:59\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.1446 | Avg: 0.1171 | Tempo: 0.47s | Restante: ~0m26s | ğŸ• 23:32:00\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1218 | Avg: 0.1171 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:32:01\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0585 | Avg: 0.1167 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:32:02\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0587 | Avg: 0.1163 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:32:03\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0690 | Avg: 0.1160 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:32:04\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.1155 | Avg: 0.1160 | Tempo: 0.61s | Restante: ~0m23s | ğŸ• 23:32:05\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.1398 | Avg: 0.1161 | Tempo: 0.52s | Restante: ~0m23s | ğŸ• 23:32:07\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.1533 | Avg: 0.1164 | Tempo: 0.71s | Restante: ~0m22s | ğŸ• 23:32:10\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.1032 | Avg: 0.1163 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:32:11\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.1087 | Avg: 0.1162 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:32:12\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.1173 | Avg: 0.1162 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:32:13\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.2282 | Avg: 0.1170 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:32:14\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.1580 | Avg: 0.1173 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:32:15\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.0772 | Avg: 0.1170 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:32:16\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.1916 | Avg: 0.1175 | Tempo: 0.51s | Restante: ~0m19s | ğŸ• 23:32:17\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.1155 | Avg: 0.1175 | Tempo: 0.48s | Restante: ~0m18s | ğŸ• 23:32:18\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0982 | Avg: 0.1174 | Tempo: 0.56s | Restante: ~0m18s | ğŸ• 23:32:20\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0767 | Avg: 0.1171 | Tempo: 0.50s | Restante: ~0m17s | ğŸ• 23:32:21\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.1797 | Avg: 0.1175 | Tempo: 0.51s | Restante: ~0m17s | ğŸ• 23:32:22\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0732 | Avg: 0.1172 | Tempo: 0.49s | Restante: ~0m16s | ğŸ• 23:32:23\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.1972 | Avg: 0.1177 | Tempo: 0.49s | Restante: ~0m16s | ğŸ• 23:32:24\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.1185 | Avg: 0.1177 | Tempo: 0.49s | Restante: ~0m15s | ğŸ• 23:32:25\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0950 | Avg: 0.1176 | Tempo: 0.52s | Restante: ~0m15s | ğŸ• 23:32:26\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.1114 | Avg: 0.1176 | Tempo: 0.47s | Restante: ~0m14s | ğŸ• 23:32:27\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0918 | Avg: 0.1174 | Tempo: 0.89s | Restante: ~0m14s | ğŸ• 23:32:29\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0934 | Avg: 0.1172 | Tempo: 0.55s | Restante: ~0m13s | ğŸ• 23:32:30\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.1127 | Avg: 0.1172 | Tempo: 0.62s | Restante: ~0m13s | ğŸ• 23:32:32\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0580 | Avg: 0.1169 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:32:34\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.1452 | Avg: 0.1170 | Tempo: 0.70s | Restante: ~0m12s | ğŸ• 23:32:36\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0823 | Avg: 0.1168 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:32:38\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.1910 | Avg: 0.1173 | Tempo: 0.51s | Restante: ~0m10s | ğŸ• 23:32:39\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.1506 | Avg: 0.1175 | Tempo: 0.50s | Restante: ~0m10s | ğŸ• 23:32:40\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0753 | Avg: 0.1172 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:32:41\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0761 | Avg: 0.1170 | Tempo: 0.50s | Restante: ~0m9s | ğŸ• 23:32:42\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0793 | Avg: 0.1167 | Tempo: 0.50s | Restante: ~0m8s | ğŸ• 23:32:43\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0806 | Avg: 0.1165 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:32:44\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0988 | Avg: 0.1164 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:32:46\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0891 | Avg: 0.1163 | Tempo: 0.50s | Restante: ~0m7s | ğŸ• 23:32:47\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0916 | Avg: 0.1161 | Tempo: 0.52s | Restante: ~0m6s | ğŸ• 23:32:48\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.1332 | Avg: 0.1162 | Tempo: 0.66s | Restante: ~0m6s | ğŸ• 23:32:50\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0919 | Avg: 0.1161 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:32:51\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0934 | Avg: 0.1160 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:32:52\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0982 | Avg: 0.1159 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:32:53\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0949 | Avg: 0.1158 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:32:55\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0577 | Avg: 0.1154 | Tempo: 0.51s | Restante: ~0m3s | ğŸ• 23:32:56\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.0773 | Avg: 0.1152 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:32:57\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.1925 | Avg: 0.1156 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:32:58\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.1129 | Avg: 0.1156 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:32:59\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0975 | Avg: 0.1155 | Tempo: 0.51s | Restante: ~0m1s | ğŸ• 23:33:00\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0759 | Avg: 0.1153 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:33:01\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.1311 | Avg: 0.1154 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:33:02\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0812 | Avg: 0.1152 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:33:03\n",
      "\n",
      "âœ… Ã‰poca 3 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.1152\n",
      "   ğŸ“Š Loss Classifier: 0.0421\n",
      "   ğŸ“Š Loss Box Reg: 0.0636\n",
      "   ğŸ“Š Loss Objectness: 0.0051\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0045\n",
      "   â±ï¸  Tempo total: 3m41s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.52s\n",
      "ğŸ”µ === FIM Ã‰POCA 3/10, Loss: 0.1152 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.1152)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_3.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 4/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 4\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 4/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000500\n",
      "   ğŸ• InÃ­cio: 23:33:04\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.1286\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.1286 | Avg: 0.1286 | Tempo: 0.50s | Restante: ~1m33s | ğŸ• 23:33:05\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0969 | Avg: 0.1128 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:33:06\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0794 | Avg: 0.1016 | Tempo: 0.51s | Restante: ~1m31s | ğŸ• 23:33:08\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0617 | Avg: 0.0917 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:33:09\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.1490 | Avg: 0.1031 | Tempo: 0.69s | Restante: ~1m37s | ğŸ• 23:33:11\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.1600 | Avg: 0.1126 | Tempo: 0.50s | Restante: ~1m35s | ğŸ• 23:33:12\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.1078 | Avg: 0.1119 | Tempo: 0.48s | Restante: ~1m34s | ğŸ• 23:33:13\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0618 | Avg: 0.1057 | Tempo: 0.48s | Restante: ~1m32s | ğŸ• 23:33:14\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0753 | Avg: 0.1023 | Tempo: 0.49s | Restante: ~1m31s | ğŸ• 23:33:16\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0844 | Avg: 0.1005 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:33:17\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.1020 | Avg: 0.1006 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:33:18\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.1614 | Avg: 0.1057 | Tempo: 1.03s | Restante: ~1m36s | ğŸ• 23:33:19\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0921 | Avg: 0.1047 | Tempo: 0.53s | Restante: ~1m36s | ğŸ• 23:33:20\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0786 | Avg: 0.1028 | Tempo: 0.48s | Restante: ~1m34s | ğŸ• 23:33:21\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0939 | Avg: 0.1022 | Tempo: 0.54s | Restante: ~1m34s | ğŸ• 23:33:23\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.1600 | Avg: 0.1058 | Tempo: 0.49s | Restante: ~1m33s | ğŸ• 23:33:24\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0673 | Avg: 0.1035 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:33:25\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0727 | Avg: 0.1018 | Tempo: 0.50s | Restante: ~1m30s | ğŸ• 23:33:26\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0417 | Avg: 0.0987 | Tempo: 0.53s | Restante: ~1m30s | ğŸ• 23:33:28\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0749 | Avg: 0.0975 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:33:29\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.1182 | Avg: 0.0985 | Tempo: 0.72s | Restante: ~1m30s | ğŸ• 23:33:32\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0951 | Avg: 0.0983 | Tempo: 0.51s | Restante: ~1m29s | ğŸ• 23:33:33\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0884 | Avg: 0.0979 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:33:34\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.1787 | Avg: 0.1013 | Tempo: 0.50s | Restante: ~1m27s | ğŸ• 23:33:35\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.1023 | Avg: 0.1013 | Tempo: 0.49s | Restante: ~1m27s | ğŸ• 23:33:36\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.1467 | Avg: 0.1030 | Tempo: 0.57s | Restante: ~1m26s | ğŸ• 23:33:37\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.1072 | Avg: 0.1032 | Tempo: 0.50s | Restante: ~1m26s | ğŸ• 23:33:38\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0722 | Avg: 0.1021 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:33:39\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0329 | Avg: 0.0997 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:33:40\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.1515 | Avg: 0.1014 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:33:42\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0680 | Avg: 0.1004 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:33:43\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0388 | Avg: 0.0984 | Tempo: 0.52s | Restante: ~1m22s | ğŸ• 23:33:44\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0929 | Avg: 0.0983 | Tempo: 0.53s | Restante: ~1m21s | ğŸ• 23:33:45\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0826 | Avg: 0.0978 | Tempo: 1.05s | Restante: ~1m23s | ğŸ• 23:33:46\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0793 | Avg: 0.0973 | Tempo: 1.06s | Restante: ~1m25s | ğŸ• 23:33:47\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1153 | Avg: 0.0978 | Tempo: 1.07s | Restante: ~1m26s | ğŸ• 23:33:49\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.1464 | Avg: 0.0991 | Tempo: 0.50s | Restante: ~1m26s | ğŸ• 23:33:50\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0638 | Avg: 0.0982 | Tempo: 0.50s | Restante: ~1m25s | ğŸ• 23:33:51\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0807 | Avg: 0.0977 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:33:52\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0395 | Avg: 0.0963 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:33:53\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.1064 | Avg: 0.0965 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:33:54\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.0925 | Avg: 0.0964 | Tempo: 0.99s | Restante: ~1m23s | ğŸ• 23:33:55\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0827 | Avg: 0.0961 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:33:56\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0530 | Avg: 0.0951 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:33:58\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0838 | Avg: 0.0949 | Tempo: 0.51s | Restante: ~1m21s | ğŸ• 23:33:59\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1025 | Avg: 0.0950 | Tempo: 0.48s | Restante: ~1m20s | ğŸ• 23:34:00\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0549 | Avg: 0.0942 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:34:01\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.1058 | Avg: 0.0944 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:34:02\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.2446 | Avg: 0.0975 | Tempo: 0.52s | Restante: ~1m18s | ğŸ• 23:34:03\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0730 | Avg: 0.0970 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:34:04\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0688 | Avg: 0.0964 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:34:05\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0879 | Avg: 0.0963 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:34:07\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0792 | Avg: 0.0959 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:34:08\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.0514 | Avg: 0.0951 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:34:09\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.2257 | Avg: 0.0975 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:34:10\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.1057 | Avg: 0.0976 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:34:11\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0618 | Avg: 0.0970 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:34:12\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0872 | Avg: 0.0968 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:34:13\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.2894 | Avg: 0.1001 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:34:14\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0630 | Avg: 0.0995 | Tempo: 0.49s | Restante: ~1m10s | ğŸ• 23:34:15\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0813 | Avg: 0.0992 | Tempo: 0.51s | Restante: ~1m9s | ğŸ• 23:34:17\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0800 | Avg: 0.0989 | Tempo: 0.71s | Restante: ~1m9s | ğŸ• 23:34:19\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.0506 | Avg: 0.0981 | Tempo: 0.48s | Restante: ~1m8s | ğŸ• 23:34:21\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0829 | Avg: 0.0979 | Tempo: 0.48s | Restante: ~1m7s | ğŸ• 23:34:22\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0976 | Avg: 0.0979 | Tempo: 0.50s | Restante: ~1m7s | ğŸ• 23:34:23\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0879 | Avg: 0.0977 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:34:24\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.1017 | Avg: 0.0978 | Tempo: 0.66s | Restante: ~1m6s | ğŸ• 23:34:26\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.0886 | Avg: 0.0976 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:34:27\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0964 | Avg: 0.0976 | Tempo: 1.01s | Restante: ~1m5s | ğŸ• 23:34:28\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.0564 | Avg: 0.0970 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:34:30\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0701 | Avg: 0.0967 | Tempo: 0.51s | Restante: ~1m4s | ğŸ• 23:34:31\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0349 | Avg: 0.0958 | Tempo: 0.92s | Restante: ~1m4s | ğŸ• 23:34:32\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.1229 | Avg: 0.0962 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:34:33\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0702 | Avg: 0.0958 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:34:34\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0948 | Avg: 0.0958 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:34:35\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0930 | Avg: 0.0958 | Tempo: 0.50s | Restante: ~1m2s | ğŸ• 23:34:36\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.1172 | Avg: 0.0960 | Tempo: 0.66s | Restante: ~1m1s | ğŸ• 23:34:39\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.1024 | Avg: 0.0961 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:34:40\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1110 | Avg: 0.0963 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:34:41\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0763 | Avg: 0.0961 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:34:42\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0710 | Avg: 0.0958 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:34:43\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0770 | Avg: 0.0955 | Tempo: 0.49s | Restante: ~0m58s | ğŸ• 23:34:44\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.1996 | Avg: 0.0968 | Tempo: 0.53s | Restante: ~0m57s | ğŸ• 23:34:45\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0785 | Avg: 0.0966 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:34:47\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0477 | Avg: 0.0960 | Tempo: 0.51s | Restante: ~0m56s | ğŸ• 23:34:48\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0490 | Avg: 0.0954 | Tempo: 0.48s | Restante: ~0m55s | ğŸ• 23:34:49\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.1169 | Avg: 0.0957 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:34:50\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0622 | Avg: 0.0953 | Tempo: 0.49s | Restante: ~0m54s | ğŸ• 23:34:51\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.1114 | Avg: 0.0955 | Tempo: 0.49s | Restante: ~0m54s | ğŸ• 23:34:52\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.1155 | Avg: 0.0957 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:34:53\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0468 | Avg: 0.0952 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:34:54\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0929 | Avg: 0.0952 | Tempo: 0.50s | Restante: ~0m52s | ğŸ• 23:34:55\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0903 | Avg: 0.0951 | Tempo: 0.81s | Restante: ~0m52s | ğŸ• 23:34:57\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0660 | Avg: 0.0948 | Tempo: 0.49s | Restante: ~0m51s | ğŸ• 23:34:58\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0836 | Avg: 0.0947 | Tempo: 0.48s | Restante: ~0m50s | ğŸ• 23:34:59\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.0853 | Avg: 0.0946 | Tempo: 0.48s | Restante: ~0m50s | ğŸ• 23:35:00\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0630 | Avg: 0.0942 | Tempo: 0.50s | Restante: ~0m49s | ğŸ• 23:35:01\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0380 | Avg: 0.0937 | Tempo: 0.48s | Restante: ~0m49s | ğŸ• 23:35:02\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0688 | Avg: 0.0934 | Tempo: 0.48s | Restante: ~0m48s | ğŸ• 23:35:03\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0910 | Avg: 0.0934 | Tempo: 0.53s | Restante: ~0m47s | ğŸ• 23:35:05\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0702 | Avg: 0.0932 | Tempo: 0.50s | Restante: ~0m47s | ğŸ• 23:35:06\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0773 | Avg: 0.0930 | Tempo: 0.59s | Restante: ~0m46s | ğŸ• 23:35:07\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0561 | Avg: 0.0927 | Tempo: 0.50s | Restante: ~0m46s | ğŸ• 23:35:08\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.0493 | Avg: 0.0922 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:35:09\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0621 | Avg: 0.0920 | Tempo: 0.60s | Restante: ~0m45s | ğŸ• 23:35:10\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.0590 | Avg: 0.0916 | Tempo: 0.57s | Restante: ~0m44s | ğŸ• 23:35:11\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.0684 | Avg: 0.0914 | Tempo: 0.48s | Restante: ~0m43s | ğŸ• 23:35:12\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.2442 | Avg: 0.0928 | Tempo: 0.51s | Restante: ~0m43s | ğŸ• 23:35:14\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0684 | Avg: 0.0926 | Tempo: 0.90s | Restante: ~0m43s | ğŸ• 23:35:15\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0917 | Avg: 0.0926 | Tempo: 1.00s | Restante: ~0m42s | ğŸ• 23:35:16\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.0656 | Avg: 0.0924 | Tempo: 0.53s | Restante: ~0m42s | ğŸ• 23:35:17\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.0978 | Avg: 0.0924 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:35:18\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.1066 | Avg: 0.0925 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:35:19\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0848 | Avg: 0.0925 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:35:20\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.0487 | Avg: 0.0921 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:35:21\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.0642 | Avg: 0.0918 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:35:23\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0923 | Avg: 0.0919 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:35:24\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0566 | Avg: 0.0916 | Tempo: 0.49s | Restante: ~0m38s | ğŸ• 23:35:25\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.1372 | Avg: 0.0919 | Tempo: 0.50s | Restante: ~0m37s | ğŸ• 23:35:26\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.0600 | Avg: 0.0917 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:35:27\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.1240 | Avg: 0.0919 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:35:28\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0881 | Avg: 0.0919 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:35:29\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0515 | Avg: 0.0916 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:35:30\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.0657 | Avg: 0.0914 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:35:32\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.1018 | Avg: 0.0915 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:35:33\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0898 | Avg: 0.0914 | Tempo: 0.50s | Restante: ~0m33s | ğŸ• 23:35:34\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.1169 | Avg: 0.0916 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:35:35\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.0602 | Avg: 0.0914 | Tempo: 0.49s | Restante: ~0m32s | ğŸ• 23:35:36\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1256 | Avg: 0.0917 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:35:37\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.2642 | Avg: 0.0930 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:35:38\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0792 | Avg: 0.0929 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:35:39\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0638 | Avg: 0.0927 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:35:41\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.0792 | Avg: 0.0926 | Tempo: 0.48s | Restante: ~0m29s | ğŸ• 23:35:42\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.1148 | Avg: 0.0927 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:35:43\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0617 | Avg: 0.0925 | Tempo: 1.01s | Restante: ~0m28s | ğŸ• 23:35:44\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.1097 | Avg: 0.0926 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:35:45\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0428 | Avg: 0.0923 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:35:46\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.0689 | Avg: 0.0921 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:35:47\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0699 | Avg: 0.0919 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:35:48\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0451 | Avg: 0.0916 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:35:50\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0772 | Avg: 0.0915 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:35:51\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.0895 | Avg: 0.0915 | Tempo: 0.47s | Restante: ~0m24s | ğŸ• 23:35:52\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.1131 | Avg: 0.0916 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:35:53\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.0732 | Avg: 0.0915 | Tempo: 1.06s | Restante: ~0m23s | ğŸ• 23:35:54\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0671 | Avg: 0.0913 | Tempo: 0.49s | Restante: ~0m23s | ğŸ• 23:35:55\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0865 | Avg: 0.0913 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:35:56\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0716 | Avg: 0.0912 | Tempo: 0.49s | Restante: ~0m22s | ğŸ• 23:35:57\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.2036 | Avg: 0.0919 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:35:59\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0997 | Avg: 0.0920 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:36:00\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.0667 | Avg: 0.0918 | Tempo: 0.61s | Restante: ~0m20s | ğŸ• 23:36:02\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.1081 | Avg: 0.0919 | Tempo: 0.65s | Restante: ~0m20s | ğŸ• 23:36:04\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0740 | Avg: 0.0918 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:36:05\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.1457 | Avg: 0.0921 | Tempo: 0.51s | Restante: ~0m18s | ğŸ• 23:36:06\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0541 | Avg: 0.0919 | Tempo: 0.49s | Restante: ~0m18s | ğŸ• 23:36:07\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.0790 | Avg: 0.0918 | Tempo: 0.50s | Restante: ~0m17s | ğŸ• 23:36:08\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0539 | Avg: 0.0916 | Tempo: 0.49s | Restante: ~0m17s | ğŸ• 23:36:10\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.1087 | Avg: 0.0917 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:36:11\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.1287 | Avg: 0.0919 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:36:12\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0861 | Avg: 0.0919 | Tempo: 0.50s | Restante: ~0m15s | ğŸ• 23:36:13\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0882 | Avg: 0.0919 | Tempo: 0.99s | Restante: ~0m15s | ğŸ• 23:36:14\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0767 | Avg: 0.0918 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:36:15\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.1622 | Avg: 0.0922 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:36:16\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0981 | Avg: 0.0922 | Tempo: 0.49s | Restante: ~0m13s | ğŸ• 23:36:18\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.1223 | Avg: 0.0924 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:36:19\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.1252 | Avg: 0.0926 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:36:20\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.2806 | Avg: 0.0938 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:36:21\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.0719 | Avg: 0.0936 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:36:22\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0765 | Avg: 0.0935 | Tempo: 0.48s | Restante: ~0m10s | ğŸ• 23:36:23\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0846 | Avg: 0.0935 | Tempo: 0.91s | Restante: ~0m10s | ğŸ• 23:36:24\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0702 | Avg: 0.0933 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:36:25\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0803 | Avg: 0.0933 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:36:26\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0783 | Avg: 0.0932 | Tempo: 0.66s | Restante: ~0m8s | ğŸ• 23:36:29\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0764 | Avg: 0.0931 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:36:30\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0443 | Avg: 0.0928 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:36:31\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0953 | Avg: 0.0928 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:36:32\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0810 | Avg: 0.0927 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:36:33\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0825 | Avg: 0.0927 | Tempo: 0.50s | Restante: ~0m5s | ğŸ• 23:36:34\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0856 | Avg: 0.0926 | Tempo: 0.50s | Restante: ~0m5s | ğŸ• 23:36:35\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.1912 | Avg: 0.0932 | Tempo: 0.65s | Restante: ~0m4s | ğŸ• 23:36:38\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0607 | Avg: 0.0930 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:36:39\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.1036 | Avg: 0.0931 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:36:40\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.0849 | Avg: 0.0930 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:36:41\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0378 | Avg: 0.0927 | Tempo: 0.50s | Restante: ~0m2s | ğŸ• 23:36:42\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0464 | Avg: 0.0925 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:36:43\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.1206 | Avg: 0.0926 | Tempo: 0.50s | Restante: ~0m1s | ğŸ• 23:36:44\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0782 | Avg: 0.0925 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:36:46\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0716 | Avg: 0.0924 | Tempo: 0.48s | Restante: ~0m0s | ğŸ• 23:36:47\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0978 | Avg: 0.0925 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:36:47\n",
      "\n",
      "âœ… Ã‰poca 4 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0925\n",
      "   ğŸ“Š Loss Classifier: 0.0323\n",
      "   ğŸ“Š Loss Box Reg: 0.0533\n",
      "   ğŸ“Š Loss Objectness: 0.0030\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0038\n",
      "   â±ï¸  Tempo total: 3m43s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.54s\n",
      "ğŸ”µ === FIM Ã‰POCA 4/10, Loss: 0.0925 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0925)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_4.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 5/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 5\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 5/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000500\n",
      "   ğŸ• InÃ­cio: 23:36:49\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0641\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0641 | Avg: 0.0641 | Tempo: 0.50s | Restante: ~1m34s | ğŸ• 23:36:51\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0519 | Avg: 0.0580 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:36:52\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0729 | Avg: 0.0629 | Tempo: 0.49s | Restante: ~1m31s | ğŸ• 23:36:53\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0678 | Avg: 0.0641 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:36:54\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0736 | Avg: 0.0660 | Tempo: 0.50s | Restante: ~1m30s | ğŸ• 23:36:55\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.0954 | Avg: 0.0709 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:36:56\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0728 | Avg: 0.0712 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:36:57\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.1044 | Avg: 0.0753 | Tempo: 0.50s | Restante: ~1m28s | ğŸ• 23:36:58\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0610 | Avg: 0.0737 | Tempo: 0.50s | Restante: ~1m28s | ğŸ• 23:36:59\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0550 | Avg: 0.0719 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:37:01\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.0622 | Avg: 0.0710 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:37:02\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.2076 | Avg: 0.0824 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:37:03\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0432 | Avg: 0.0794 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:37:04\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0702 | Avg: 0.0787 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:37:05\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0817 | Avg: 0.0789 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:37:06\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.0600 | Avg: 0.0777 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:37:07\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0766 | Avg: 0.0776 | Tempo: 0.47s | Restante: ~1m23s | ğŸ• 23:37:08\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.1494 | Avg: 0.0816 | Tempo: 0.65s | Restante: ~1m24s | ğŸ• 23:37:11\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0993 | Avg: 0.0826 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:37:12\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0896 | Avg: 0.0829 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:37:13\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0700 | Avg: 0.0823 | Tempo: 0.50s | Restante: ~1m22s | ğŸ• 23:37:14\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0562 | Avg: 0.0811 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:37:15\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0690 | Avg: 0.0806 | Tempo: 0.50s | Restante: ~1m21s | ğŸ• 23:37:16\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0621 | Avg: 0.0798 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:37:17\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.0524 | Avg: 0.0787 | Tempo: 0.50s | Restante: ~1m20s | ğŸ• 23:37:19\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0322 | Avg: 0.0769 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:37:20\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0624 | Avg: 0.0764 | Tempo: 0.51s | Restante: ~1m19s | ğŸ• 23:37:21\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0828 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:37:22\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0776 | Avg: 0.0767 | Tempo: 0.48s | Restante: ~1m18s | ğŸ• 23:37:23\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.1019 | Avg: 0.0775 | Tempo: 0.50s | Restante: ~1m18s | ğŸ• 23:37:24\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0903 | Avg: 0.0779 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:37:25\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0750 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:37:27\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0663 | Avg: 0.0775 | Tempo: 1.03s | Restante: ~1m19s | ğŸ• 23:37:28\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.1043 | Avg: 0.0783 | Tempo: 0.51s | Restante: ~1m18s | ğŸ• 23:37:29\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0826 | Avg: 0.0784 | Tempo: 0.50s | Restante: ~1m18s | ğŸ• 23:37:30\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.0508 | Avg: 0.0776 | Tempo: 0.50s | Restante: ~1m17s | ğŸ• 23:37:31\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0763 | Avg: 0.0776 | Tempo: 0.50s | Restante: ~1m17s | ğŸ• 23:37:32\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0851 | Avg: 0.0778 | Tempo: 0.50s | Restante: ~1m16s | ğŸ• 23:37:33\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0792 | Avg: 0.0778 | Tempo: 0.66s | Restante: ~1m16s | ğŸ• 23:37:36\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0787 | Avg: 0.0778 | Tempo: 0.50s | Restante: ~1m16s | ğŸ• 23:37:37\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.0965 | Avg: 0.0783 | Tempo: 0.50s | Restante: ~1m15s | ğŸ• 23:37:38\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.1194 | Avg: 0.0793 | Tempo: 0.48s | Restante: ~1m14s | ğŸ• 23:37:39\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0495 | Avg: 0.0786 | Tempo: 0.51s | Restante: ~1m14s | ğŸ• 23:37:40\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0562 | Avg: 0.0781 | Tempo: 0.50s | Restante: ~1m13s | ğŸ• 23:37:41\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0715 | Avg: 0.0779 | Tempo: 0.50s | Restante: ~1m13s | ğŸ• 23:37:42\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1430 | Avg: 0.0793 | Tempo: 0.50s | Restante: ~1m12s | ğŸ• 23:37:44\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0978 | Avg: 0.0797 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:37:45\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0822 | Avg: 0.0798 | Tempo: 0.94s | Restante: ~1m12s | ğŸ• 23:37:46\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0585 | Avg: 0.0793 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:37:47\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0605 | Avg: 0.0790 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:37:48\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0420 | Avg: 0.0782 | Tempo: 0.50s | Restante: ~1m11s | ğŸ• 23:37:49\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0745 | Avg: 0.0782 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:37:50\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0620 | Avg: 0.0779 | Tempo: 0.52s | Restante: ~1m9s | ğŸ• 23:37:52\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.0597 | Avg: 0.0775 | Tempo: 0.51s | Restante: ~1m9s | ğŸ• 23:37:53\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.1222 | Avg: 0.0783 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:37:54\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.0401 | Avg: 0.0777 | Tempo: 0.58s | Restante: ~1m8s | ğŸ• 23:37:55\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0458 | Avg: 0.0771 | Tempo: 0.50s | Restante: ~1m7s | ğŸ• 23:37:56\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0489 | Avg: 0.0766 | Tempo: 0.50s | Restante: ~1m7s | ğŸ• 23:37:57\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.0605 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:37:58\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0604 | Avg: 0.0761 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:37:59\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0970 | Avg: 0.0764 | Tempo: 0.50s | Restante: ~1m5s | ğŸ• 23:38:01\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0533 | Avg: 0.0760 | Tempo: 0.50s | Restante: ~1m5s | ğŸ• 23:38:02\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1087 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~1m4s | ğŸ• 23:38:03\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0676 | Avg: 0.0764 | Tempo: 0.99s | Restante: ~1m4s | ğŸ• 23:38:04\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0298 | Avg: 0.0757 | Tempo: 0.50s | Restante: ~1m4s | ğŸ• 23:38:05\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0435 | Avg: 0.0752 | Tempo: 0.50s | Restante: ~1m3s | ğŸ• 23:38:06\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0586 | Avg: 0.0750 | Tempo: 0.54s | Restante: ~1m3s | ğŸ• 23:38:08\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.0580 | Avg: 0.0747 | Tempo: 0.54s | Restante: ~1m2s | ğŸ• 23:38:09\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0793 | Avg: 0.0748 | Tempo: 0.48s | Restante: ~1m2s | ğŸ• 23:38:10\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.0744 | Avg: 0.0748 | Tempo: 0.50s | Restante: ~1m1s | ğŸ• 23:38:11\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0796 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~1m1s | ğŸ• 23:38:12\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0832 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:38:14\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0791 | Avg: 0.0750 | Tempo: 0.48s | Restante: ~0m59s | ğŸ• 23:38:15\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0416 | Avg: 0.0746 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:38:16\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.1265 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~0m58s | ğŸ• 23:38:17\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0969 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:38:18\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0783 | Avg: 0.0756 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:38:19\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0930 | Avg: 0.0758 | Tempo: 0.50s | Restante: ~0m57s | ğŸ• 23:38:20\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.0743 | Avg: 0.0758 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:38:21\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0626 | Avg: 0.0756 | Tempo: 0.50s | Restante: ~0m55s | ğŸ• 23:38:23\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0438 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:38:24\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0985 | Avg: 0.0755 | Tempo: 2.55s | Restante: ~0m57s | ğŸ• 23:38:27\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0730 | Avg: 0.0755 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:38:28\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0412 | Avg: 0.0751 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:38:29\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0854 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:38:30\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0576 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:38:31\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0532 | Avg: 0.0747 | Tempo: 0.50s | Restante: ~0m54s | ğŸ• 23:38:32\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.1108 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:38:33\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.0799 | Avg: 0.0752 | Tempo: 0.47s | Restante: ~0m53s | ğŸ• 23:38:34\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.0602 | Avg: 0.0750 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:38:36\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0856 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:38:37\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0796 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:38:38\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0668 | Avg: 0.0751 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:38:39\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0993 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:38:40\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0400 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:38:41\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.0760 | Avg: 0.0750 | Tempo: 1.08s | Restante: ~0m49s | ğŸ• 23:38:42\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.2177 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:38:43\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.1236 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m48s | ğŸ• 23:38:45\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0701 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~0m47s | ğŸ• 23:38:46\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0510 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:38:47\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0660 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:38:48\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0632 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:38:49\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0682 | Avg: 0.0763 | Tempo: 0.48s | Restante: ~0m45s | ğŸ• 23:38:50\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.1248 | Avg: 0.0768 | Tempo: 0.47s | Restante: ~0m45s | ğŸ• 23:38:51\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0500 | Avg: 0.0765 | Tempo: 0.50s | Restante: ~0m44s | ğŸ• 23:38:52\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.0378 | Avg: 0.0762 | Tempo: 0.50s | Restante: ~0m43s | ğŸ• 23:38:53\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.0784 | Avg: 0.0762 | Tempo: 0.50s | Restante: ~0m43s | ğŸ• 23:38:55\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0517 | Avg: 0.0760 | Tempo: 0.62s | Restante: ~0m42s | ğŸ• 23:38:57\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0894 | Avg: 0.0761 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:38:58\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0520 | Avg: 0.0759 | Tempo: 0.47s | Restante: ~0m41s | ğŸ• 23:38:59\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.0658 | Avg: 0.0758 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:39:00\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.0582 | Avg: 0.0756 | Tempo: 0.47s | Restante: ~0m40s | ğŸ• 23:39:01\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.0798 | Avg: 0.0756 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:39:02\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0567 | Avg: 0.0755 | Tempo: 0.50s | Restante: ~0m39s | ğŸ• 23:39:03\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.0366 | Avg: 0.0751 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:39:05\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.0842 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~0m38s | ğŸ• 23:39:06\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0969 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:39:07\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0544 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:39:08\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0806 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:39:09\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.0729 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:39:10\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.2399 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:39:11\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0560 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:39:12\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0434 | Avg: 0.0762 | Tempo: 1.08s | Restante: ~0m34s | ğŸ• 23:39:13\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.0796 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:39:15\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.0992 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:39:16\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.1124 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:39:17\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.0694 | Avg: 0.0766 | Tempo: 0.51s | Restante: ~0m32s | ğŸ• 23:39:18\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.1278 | Avg: 0.0770 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:39:19\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1054 | Avg: 0.0772 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:39:20\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.0498 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:39:21\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0590 | Avg: 0.0769 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:39:22\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0646 | Avg: 0.0768 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:39:24\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.1089 | Avg: 0.0770 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:39:25\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0582 | Avg: 0.0769 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:39:26\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0504 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:39:27\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.0698 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m27s | ğŸ• 23:39:28\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0598 | Avg: 0.0765 | Tempo: 0.47s | Restante: ~0m27s | ğŸ• 23:39:29\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.0975 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:39:30\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0834 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:39:31\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0611 | Avg: 0.0766 | Tempo: 0.57s | Restante: ~0m25s | ğŸ• 23:39:33\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0641 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:39:34\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.1142 | Avg: 0.0768 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:39:35\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.1079 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m23s | ğŸ• 23:39:36\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.1305 | Avg: 0.0774 | Tempo: 0.49s | Restante: ~0m23s | ğŸ• 23:39:37\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.1412 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m22s | ğŸ• 23:39:38\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.1265 | Avg: 0.0782 | Tempo: 0.47s | Restante: ~0m22s | ğŸ• 23:39:39\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0659 | Avg: 0.0781 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:39:40\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.0959 | Avg: 0.0782 | Tempo: 0.70s | Restante: ~0m21s | ğŸ• 23:39:43\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0752 | Avg: 0.0782 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:39:45\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.0418 | Avg: 0.0779 | Tempo: 0.48s | Restante: ~0m20s | ğŸ• 23:39:46\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0773 | Avg: 0.0779 | Tempo: 0.47s | Restante: ~0m19s | ğŸ• 23:39:47\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0803 | Avg: 0.0779 | Tempo: 0.47s | Restante: ~0m18s | ğŸ• 23:39:48\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0608 | Avg: 0.0778 | Tempo: 0.48s | Restante: ~0m18s | ğŸ• 23:39:49\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0646 | Avg: 0.0777 | Tempo: 0.49s | Restante: ~0m17s | ğŸ• 23:39:50\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.1518 | Avg: 0.0782 | Tempo: 0.49s | Restante: ~0m17s | ğŸ• 23:39:51\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.1054 | Avg: 0.0784 | Tempo: 0.49s | Restante: ~0m16s | ğŸ• 23:39:52\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0714 | Avg: 0.0784 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:39:53\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.2800 | Avg: 0.0796 | Tempo: 0.49s | Restante: ~0m15s | ğŸ• 23:39:55\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0616 | Avg: 0.0795 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:39:56\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0677 | Avg: 0.0794 | Tempo: 0.65s | Restante: ~0m14s | ğŸ• 23:39:58\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.1923 | Avg: 0.0801 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:39:59\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0928 | Avg: 0.0802 | Tempo: 0.47s | Restante: ~0m13s | ğŸ• 23:40:00\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.1193 | Avg: 0.0805 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:40:01\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.1003 | Avg: 0.0806 | Tempo: 0.65s | Restante: ~0m12s | ğŸ• 23:40:04\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.0661 | Avg: 0.0805 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:40:05\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0793 | Avg: 0.0805 | Tempo: 0.48s | Restante: ~0m11s | ğŸ• 23:40:06\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.0664 | Avg: 0.0804 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:40:07\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0948 | Avg: 0.0805 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:40:08\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.1004 | Avg: 0.0806 | Tempo: 0.47s | Restante: ~0m9s | ğŸ• 23:40:09\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0733 | Avg: 0.0806 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:40:10\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.1161 | Avg: 0.0808 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:40:11\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0651 | Avg: 0.0807 | Tempo: 0.47s | Restante: ~0m8s | ğŸ• 23:40:12\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.1480 | Avg: 0.0811 | Tempo: 0.66s | Restante: ~0m7s | ğŸ• 23:40:15\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0813 | Avg: 0.0811 | Tempo: 1.01s | Restante: ~0m7s | ğŸ• 23:40:16\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0619 | Avg: 0.0810 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:40:17\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0678 | Avg: 0.0809 | Tempo: 0.51s | Restante: ~0m6s | ğŸ• 23:40:18\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0937 | Avg: 0.0810 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:40:19\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0605 | Avg: 0.0808 | Tempo: 0.49s | Restante: ~0m5s | ğŸ• 23:40:20\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.1200 | Avg: 0.0811 | Tempo: 0.66s | Restante: ~0m4s | ğŸ• 23:40:23\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.1195 | Avg: 0.0813 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:40:24\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0917 | Avg: 0.0813 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:40:25\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.0734 | Avg: 0.0813 | Tempo: 0.91s | Restante: ~0m3s | ğŸ• 23:40:26\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0941 | Avg: 0.0814 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:40:27\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.1169 | Avg: 0.0816 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:40:28\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.1322 | Avg: 0.0818 | Tempo: 0.51s | Restante: ~0m1s | ğŸ• 23:40:29\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0457 | Avg: 0.0816 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:40:30\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.2249 | Avg: 0.0824 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:40:32\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0707 | Avg: 0.0823 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:40:32\n",
      "\n",
      "âœ… Ã‰poca 5 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0823\n",
      "   ğŸ“Š Loss Classifier: 0.0285\n",
      "   ğŸ“Š Loss Box Reg: 0.0481\n",
      "   ğŸ“Š Loss Objectness: 0.0024\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0034\n",
      "   â±ï¸  Tempo total: 3m42s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.53s\n",
      "ğŸ”µ === FIM Ã‰POCA 5/10, Loss: 0.0823 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0823)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_5.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 6/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 6\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 6/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000500\n",
      "   ğŸ• InÃ­cio: 23:40:33\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 640, 640]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0625\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0625 | Avg: 0.0625 | Tempo: 0.50s | Restante: ~1m32s | ğŸ• 23:40:35\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0432 | Avg: 0.0529 | Tempo: 0.49s | Restante: ~1m31s | ğŸ• 23:40:36\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0915 | Avg: 0.0657 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:40:37\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0622 | Avg: 0.0649 | Tempo: 0.48s | Restante: ~1m29s | ğŸ• 23:40:38\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0751 | Avg: 0.0669 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:40:39\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.0708 | Avg: 0.0676 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:40:40\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0641 | Avg: 0.0671 | Tempo: 0.51s | Restante: ~1m28s | ğŸ• 23:40:41\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0669 | Avg: 0.0670 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:40:42\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0678 | Avg: 0.0671 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:40:44\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0390 | Avg: 0.0643 | Tempo: 0.49s | Restante: ~1m27s | ğŸ• 23:40:45\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.0820 | Avg: 0.0659 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:40:46\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.0449 | Avg: 0.0642 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:40:47\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0599 | Avg: 0.0638 | Tempo: 0.50s | Restante: ~1m25s | ğŸ• 23:40:48\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0609 | Avg: 0.0636 | Tempo: 0.66s | Restante: ~1m27s | ğŸ• 23:40:50\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0720 | Avg: 0.0642 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:40:51\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.0630 | Avg: 0.0641 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:40:53\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.2296 | Avg: 0.0738 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:40:54\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0802 | Avg: 0.0742 | Tempo: 0.70s | Restante: ~1m26s | ğŸ• 23:40:57\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0394 | Avg: 0.0724 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:40:58\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0893 | Avg: 0.0732 | Tempo: 0.50s | Restante: ~1m25s | ğŸ• 23:40:59\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.1177 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:41:00\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0542 | Avg: 0.0744 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:41:01\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0451 | Avg: 0.0731 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:41:02\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0556 | Avg: 0.0724 | Tempo: 0.47s | Restante: ~1m22s | ğŸ• 23:41:03\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.0541 | Avg: 0.0716 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:41:04\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.1229 | Avg: 0.0736 | Tempo: 0.65s | Restante: ~1m22s | ğŸ• 23:41:07\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0362 | Avg: 0.0722 | Tempo: 0.47s | Restante: ~1m21s | ğŸ• 23:41:08\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0501 | Avg: 0.0714 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:41:09\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0791 | Avg: 0.0717 | Tempo: 0.47s | Restante: ~1m20s | ğŸ• 23:41:10\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.1024 | Avg: 0.0727 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:41:11\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.1089 | Avg: 0.0739 | Tempo: 0.66s | Restante: ~1m20s | ğŸ• 23:41:13\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0991 | Avg: 0.0747 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:41:15\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0469 | Avg: 0.0738 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:41:16\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0534 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~1m18s | ğŸ• 23:41:17\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0511 | Avg: 0.0726 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:41:18\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1349 | Avg: 0.0743 | Tempo: 0.50s | Restante: ~1m17s | ğŸ• 23:41:19\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0330 | Avg: 0.0732 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:41:20\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0589 | Avg: 0.0728 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:41:21\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0691 | Avg: 0.0727 | Tempo: 0.56s | Restante: ~1m15s | ğŸ• 23:41:22\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0643 | Avg: 0.0725 | Tempo: 0.50s | Restante: ~1m15s | ğŸ• 23:41:24\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.1362 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:41:25\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.0631 | Avg: 0.0738 | Tempo: 0.48s | Restante: ~1m13s | ğŸ• 23:41:26\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0408 | Avg: 0.0731 | Tempo: 0.50s | Restante: ~1m13s | ğŸ• 23:41:27\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0851 | Avg: 0.0733 | Tempo: 0.47s | Restante: ~1m12s | ğŸ• 23:41:28\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0791 | Avg: 0.0735 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:41:29\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.0792 | Avg: 0.0736 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:41:30\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0722 | Avg: 0.0736 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:41:31\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0829 | Avg: 0.0738 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:41:32\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0634 | Avg: 0.0735 | Tempo: 0.50s | Restante: ~1m9s | ğŸ• 23:41:34\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0669 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:41:35\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0620 | Avg: 0.0732 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:41:36\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0710 | Avg: 0.0731 | Tempo: 0.51s | Restante: ~1m8s | ğŸ• 23:41:37\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.1749 | Avg: 0.0751 | Tempo: 0.48s | Restante: ~1m7s | ğŸ• 23:41:38\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.1028 | Avg: 0.0756 | Tempo: 0.49s | Restante: ~1m7s | ğŸ• 23:41:39\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0673 | Avg: 0.0754 | Tempo: 0.47s | Restante: ~1m6s | ğŸ• 23:41:40\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.1001 | Avg: 0.0759 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:41:41\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.1301 | Avg: 0.0768 | Tempo: 0.81s | Restante: ~1m6s | ğŸ• 23:41:43\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0985 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:41:44\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.0718 | Avg: 0.0771 | Tempo: 0.48s | Restante: ~1m5s | ğŸ• 23:41:45\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0513 | Avg: 0.0767 | Tempo: 0.52s | Restante: ~1m4s | ğŸ• 23:41:46\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0539 | Avg: 0.0763 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:41:47\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0645 | Avg: 0.0761 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:41:48\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.0443 | Avg: 0.0756 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:41:49\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0672 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~1m2s | ğŸ• 23:41:50\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0641 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:41:52\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0907 | Avg: 0.0755 | Tempo: 0.65s | Restante: ~1m1s | ğŸ• 23:41:53\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0708 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:41:54\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.1350 | Avg: 0.0763 | Tempo: 0.52s | Restante: ~1m0s | ğŸ• 23:41:55\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0654 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~1m0s | ğŸ• 23:41:56\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.1007 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:41:57\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0976 | Avg: 0.0768 | Tempo: 0.48s | Restante: ~0m59s | ğŸ• 23:41:58\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0538 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m58s | ğŸ• 23:41:59\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0524 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:42:01\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0507 | Avg: 0.0758 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:42:02\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0892 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m57s | ğŸ• 23:42:03\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0728 | Avg: 0.0760 | Tempo: 0.52s | Restante: ~0m56s | ğŸ• 23:42:04\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0597 | Avg: 0.0758 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:42:05\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0583 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~0m55s | ğŸ• 23:42:06\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.0782 | Avg: 0.0756 | Tempo: 0.51s | Restante: ~0m54s | ğŸ• 23:42:07\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0671 | Avg: 0.0755 | Tempo: 0.49s | Restante: ~0m54s | ğŸ• 23:42:08\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0671 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:42:10\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0984 | Avg: 0.0756 | Tempo: 0.56s | Restante: ~0m53s | ğŸ• 23:42:11\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0596 | Avg: 0.0754 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:42:12\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.1670 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:42:14\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0283 | Avg: 0.0760 | Tempo: 0.50s | Restante: ~0m51s | ğŸ• 23:42:15\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0386 | Avg: 0.0755 | Tempo: 0.66s | Restante: ~0m51s | ğŸ• 23:42:17\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0974 | Avg: 0.0758 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:42:18\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0814 | Avg: 0.0758 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:42:19\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.1219 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:42:20\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.1074 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:42:21\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0607 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m48s | ğŸ• 23:42:23\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0985 | Avg: 0.0768 | Tempo: 0.51s | Restante: ~0m48s | ğŸ• 23:42:24\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0752 | Avg: 0.0768 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:42:25\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0747 | Avg: 0.0767 | Tempo: 0.89s | Restante: ~0m47s | ğŸ• 23:42:26\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0519 | Avg: 0.0765 | Tempo: 0.83s | Restante: ~0m47s | ğŸ• 23:42:27\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.1050 | Avg: 0.0768 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:42:28\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0726 | Avg: 0.0767 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:42:29\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0603 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:42:30\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.1158 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:42:32\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0511 | Avg: 0.0767 | Tempo: 0.51s | Restante: ~0m44s | ğŸ• 23:42:33\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0366 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:42:34\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0978 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:42:35\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0531 | Avg: 0.0763 | Tempo: 0.48s | Restante: ~0m43s | ğŸ• 23:42:36\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.0728 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:42:37\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0703 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:42:38\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.0734 | Avg: 0.0762 | Tempo: 0.47s | Restante: ~0m41s | ğŸ• 23:42:39\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1075 | Avg: 0.0765 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:42:41\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0934 | Avg: 0.0766 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:42:42\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.1652 | Avg: 0.0774 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:42:43\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0953 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:42:44\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.1285 | Avg: 0.0780 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:42:45\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.0918 | Avg: 0.0782 | Tempo: 0.56s | Restante: ~0m38s | ğŸ• 23:42:46\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.1026 | Avg: 0.0784 | Tempo: 0.47s | Restante: ~0m38s | ğŸ• 23:42:47\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0910 | Avg: 0.0785 | Tempo: 0.50s | Restante: ~0m37s | ğŸ• 23:42:48\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.2451 | Avg: 0.0799 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:42:49\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.0633 | Avg: 0.0798 | Tempo: 0.50s | Restante: ~0m36s | ğŸ• 23:42:51\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0559 | Avg: 0.0796 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:42:52\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0530 | Avg: 0.0794 | Tempo: 0.50s | Restante: ~0m35s | ğŸ• 23:42:53\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0365 | Avg: 0.0790 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:42:54\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.2709 | Avg: 0.0806 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:42:55\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.0680 | Avg: 0.0805 | Tempo: 0.51s | Restante: ~0m34s | ğŸ• 23:42:56\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0432 | Avg: 0.0802 | Tempo: 0.48s | Restante: ~0m33s | ğŸ• 23:42:57\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.1109 | Avg: 0.0805 | Tempo: 0.49s | Restante: ~0m32s | ğŸ• 23:42:58\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.1394 | Avg: 0.0809 | Tempo: 0.49s | Restante: ~0m32s | ğŸ• 23:43:00\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.0585 | Avg: 0.0807 | Tempo: 0.49s | Restante: ~0m31s | ğŸ• 23:43:01\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.1180 | Avg: 0.0810 | Tempo: 0.48s | Restante: ~0m31s | ğŸ• 23:43:02\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.0559 | Avg: 0.0808 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:43:03\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.0618 | Avg: 0.0807 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:43:04\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.0412 | Avg: 0.0804 | Tempo: 0.51s | Restante: ~0m29s | ğŸ• 23:43:05\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.0596 | Avg: 0.0802 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:43:06\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0509 | Avg: 0.0800 | Tempo: 0.53s | Restante: ~0m28s | ğŸ• 23:43:07\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.1200 | Avg: 0.0803 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:43:09\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.1430 | Avg: 0.0808 | Tempo: 0.65s | Restante: ~0m27s | ğŸ• 23:43:11\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0795 | Avg: 0.0808 | Tempo: 0.47s | Restante: ~0m27s | ğŸ• 23:43:12\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0693 | Avg: 0.0807 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:43:13\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.0799 | Avg: 0.0807 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:43:14\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.1111 | Avg: 0.0809 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:43:15\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1059 | Avg: 0.0811 | Tempo: 0.70s | Restante: ~0m25s | ğŸ• 23:43:18\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0711 | Avg: 0.0810 | Tempo: 0.98s | Restante: ~0m25s | ğŸ• 23:43:19\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0871 | Avg: 0.0811 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:43:20\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0933 | Avg: 0.0811 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:43:22\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.0430 | Avg: 0.0809 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:43:23\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.1154 | Avg: 0.0811 | Tempo: 0.65s | Restante: ~0m23s | ğŸ• 23:43:25\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.1658 | Avg: 0.0817 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:43:26\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0790 | Avg: 0.0817 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:43:27\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0561 | Avg: 0.0815 | Tempo: 0.92s | Restante: ~0m21s | ğŸ• 23:43:28\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0417 | Avg: 0.0812 | Tempo: 0.50s | Restante: ~0m21s | ğŸ• 23:43:29\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.1217 | Avg: 0.0815 | Tempo: 0.54s | Restante: ~0m20s | ğŸ• 23:43:31\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0437 | Avg: 0.0813 | Tempo: 0.48s | Restante: ~0m20s | ğŸ• 23:43:32\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.0674 | Avg: 0.0812 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:43:33\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0647 | Avg: 0.0811 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:43:34\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0554 | Avg: 0.0809 | Tempo: 0.50s | Restante: ~0m18s | ğŸ• 23:43:35\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0489 | Avg: 0.0807 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:43:36\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0517 | Avg: 0.0805 | Tempo: 0.49s | Restante: ~0m17s | ğŸ• 23:43:37\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.0659 | Avg: 0.0804 | Tempo: 0.50s | Restante: ~0m16s | ğŸ• 23:43:38\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0765 | Avg: 0.0804 | Tempo: 0.49s | Restante: ~0m16s | ğŸ• 23:43:40\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0361 | Avg: 0.0801 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:43:41\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.0501 | Avg: 0.0799 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:43:42\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0712 | Avg: 0.0798 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:43:43\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0512 | Avg: 0.0797 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:43:44\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0502 | Avg: 0.0795 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:43:45\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0739 | Avg: 0.0794 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:43:46\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0547 | Avg: 0.0793 | Tempo: 0.48s | Restante: ~0m12s | ğŸ• 23:43:47\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0605 | Avg: 0.0792 | Tempo: 0.50s | Restante: ~0m12s | ğŸ• 23:43:49\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.0998 | Avg: 0.0793 | Tempo: 0.60s | Restante: ~0m11s | ğŸ• 23:43:51\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0581 | Avg: 0.0792 | Tempo: 0.50s | Restante: ~0m11s | ğŸ• 23:43:52\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.1741 | Avg: 0.0797 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:43:53\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0741 | Avg: 0.0797 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:43:54\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0465 | Avg: 0.0795 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:43:55\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0550 | Avg: 0.0794 | Tempo: 0.48s | Restante: ~0m9s | ğŸ• 23:43:56\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0348 | Avg: 0.0791 | Tempo: 0.50s | Restante: ~0m8s | ğŸ• 23:43:57\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0684 | Avg: 0.0791 | Tempo: 0.88s | Restante: ~0m8s | ğŸ• 23:43:59\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0661 | Avg: 0.0790 | Tempo: 0.96s | Restante: ~0m7s | ğŸ• 23:44:00\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0631 | Avg: 0.0789 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:44:01\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.1119 | Avg: 0.0791 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:44:02\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0545 | Avg: 0.0789 | Tempo: 0.48s | Restante: ~0m6s | ğŸ• 23:44:03\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0624 | Avg: 0.0788 | Tempo: 0.49s | Restante: ~0m5s | ğŸ• 23:44:04\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0367 | Avg: 0.0786 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:44:05\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0729 | Avg: 0.0786 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:44:06\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.1093 | Avg: 0.0787 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:44:08\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0497 | Avg: 0.0786 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:44:09\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.0808 | Avg: 0.0786 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:44:10\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0678 | Avg: 0.0785 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:44:11\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0717 | Avg: 0.0785 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:44:12\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0758 | Avg: 0.0785 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:44:13\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0484 | Avg: 0.0783 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:44:14\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0504 | Avg: 0.0782 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:44:15\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.1303 | Avg: 0.0785 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:44:16\n",
      "\n",
      "âœ… Ã‰poca 6 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0785\n",
      "   ğŸ“Š Loss Classifier: 0.0267\n",
      "   ğŸ“Š Loss Box Reg: 0.0464\n",
      "   ğŸ“Š Loss Objectness: 0.0019\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0034\n",
      "   â±ï¸  Tempo total: 3m42s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.51s\n",
      "ğŸ”µ === FIM Ã‰POCA 6/10, Loss: 0.0785 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0785)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_6.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 7/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 7\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 7/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000050\n",
      "   ğŸ• InÃ­cio: 23:44:17\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0697\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0697 | Avg: 0.0697 | Tempo: 0.50s | Restante: ~1m32s | ğŸ• 23:44:19\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0521 | Avg: 0.0609 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:44:20\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0584 | Avg: 0.0601 | Tempo: 0.86s | Restante: ~1m53s | ğŸ• 23:44:21\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.1423 | Avg: 0.0806 | Tempo: 0.72s | Restante: ~1m57s | ğŸ• 23:44:24\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0577 | Avg: 0.0760 | Tempo: 0.73s | Restante: ~2m0s | ğŸ• 23:44:25\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.0744 | Avg: 0.0758 | Tempo: 0.50s | Restante: ~1m54s | ğŸ• 23:44:26\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0768 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~1m50s | ğŸ• 23:44:27\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0515 | Avg: 0.0729 | Tempo: 0.49s | Restante: ~1m47s | ğŸ• 23:44:28\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.1029 | Avg: 0.0762 | Tempo: 0.65s | Restante: ~1m47s | ğŸ• 23:44:30\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0831 | Avg: 0.0769 | Tempo: 0.50s | Restante: ~1m45s | ğŸ• 23:44:32\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.1041 | Avg: 0.0794 | Tempo: 0.49s | Restante: ~1m43s | ğŸ• 23:44:33\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.0501 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~1m41s | ğŸ• 23:44:34\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.1833 | Avg: 0.0851 | Tempo: 0.48s | Restante: ~1m39s | ğŸ• 23:44:35\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0538 | Avg: 0.0829 | Tempo: 0.49s | Restante: ~1m37s | ğŸ• 23:44:36\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0783 | Avg: 0.0826 | Tempo: 0.66s | Restante: ~1m38s | ğŸ• 23:44:38\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.0674 | Avg: 0.0816 | Tempo: 0.50s | Restante: ~1m36s | ğŸ• 23:44:39\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0561 | Avg: 0.0801 | Tempo: 0.49s | Restante: ~1m35s | ğŸ• 23:44:41\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0472 | Avg: 0.0783 | Tempo: 0.48s | Restante: ~1m34s | ğŸ• 23:44:42\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0632 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~1m33s | ğŸ• 23:44:43\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0893 | Avg: 0.0781 | Tempo: 0.50s | Restante: ~1m32s | ğŸ• 23:44:44\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0589 | Avg: 0.0772 | Tempo: 0.50s | Restante: ~1m31s | ğŸ• 23:44:45\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0905 | Avg: 0.0778 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:44:46\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0660 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:44:47\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0448 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:44:48\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.0503 | Avg: 0.0749 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:44:50\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0439 | Avg: 0.0737 | Tempo: 0.50s | Restante: ~1m26s | ğŸ• 23:44:51\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0832 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:44:52\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0639 | Avg: 0.0737 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:44:53\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0591 | Avg: 0.0732 | Tempo: 0.49s | Restante: ~1m24s | ğŸ• 23:44:54\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.0793 | Avg: 0.0734 | Tempo: 0.48s | Restante: ~1m23s | ğŸ• 23:44:55\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0589 | Avg: 0.0729 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:44:56\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0399 | Avg: 0.0719 | Tempo: 0.48s | Restante: ~1m22s | ğŸ• 23:44:57\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0586 | Avg: 0.0715 | Tempo: 0.48s | Restante: ~1m21s | ğŸ• 23:44:59\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.1365 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:45:00\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0601 | Avg: 0.0730 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:45:01\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.0629 | Avg: 0.0727 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:45:02\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0757 | Avg: 0.0728 | Tempo: 0.87s | Restante: ~1m20s | ğŸ• 23:45:03\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0447 | Avg: 0.0721 | Tempo: 0.77s | Restante: ~1m20s | ğŸ• 23:45:04\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0617 | Avg: 0.0718 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:45:05\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0947 | Avg: 0.0724 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:45:06\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.0838 | Avg: 0.0727 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:45:08\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.0858 | Avg: 0.0730 | Tempo: 0.50s | Restante: ~1m17s | ğŸ• 23:45:09\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0695 | Avg: 0.0729 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:45:10\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0404 | Avg: 0.0721 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:45:11\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0445 | Avg: 0.0715 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:45:12\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.0494 | Avg: 0.0711 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:45:13\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.1179 | Avg: 0.0721 | Tempo: 0.66s | Restante: ~1m14s | ğŸ• 23:45:15\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0946 | Avg: 0.0725 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:45:17\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0525 | Avg: 0.0721 | Tempo: 0.48s | Restante: ~1m13s | ğŸ• 23:45:18\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.2358 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~1m12s | ğŸ• 23:45:19\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0473 | Avg: 0.0748 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:45:20\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0877 | Avg: 0.0751 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:45:21\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0507 | Avg: 0.0746 | Tempo: 1.11s | Restante: ~1m12s | ğŸ• 23:45:22\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.2876 | Avg: 0.0786 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:45:23\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0443 | Avg: 0.0779 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:45:25\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.1264 | Avg: 0.0788 | Tempo: 0.49s | Restante: ~1m10s | ğŸ• 23:45:26\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0434 | Avg: 0.0782 | Tempo: 0.48s | Restante: ~1m10s | ğŸ• 23:45:27\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0688 | Avg: 0.0780 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:45:28\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.0800 | Avg: 0.0781 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:45:29\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0704 | Avg: 0.0779 | Tempo: 2.25s | Restante: ~1m11s | ğŸ• 23:45:31\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0492 | Avg: 0.0775 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:45:32\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0546 | Avg: 0.0771 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:45:34\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.0743 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:45:35\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0599 | Avg: 0.0768 | Tempo: 0.48s | Restante: ~1m9s | ğŸ• 23:45:36\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0619 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:45:37\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0413 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~1m7s | ğŸ• 23:45:38\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0961 | Avg: 0.0763 | Tempo: 0.66s | Restante: ~1m7s | ğŸ• 23:45:39\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.0396 | Avg: 0.0758 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:45:40\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0944 | Avg: 0.0760 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:45:41\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.2265 | Avg: 0.0782 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:45:43\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0325 | Avg: 0.0776 | Tempo: 0.49s | Restante: ~1m4s | ğŸ• 23:45:44\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0754 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:45:45\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0450 | Avg: 0.0771 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:45:46\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0846 | Avg: 0.0772 | Tempo: 0.47s | Restante: ~1m2s | ğŸ• 23:45:47\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0548 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:45:48\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.1316 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:45:49\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0494 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~1m0s | ğŸ• 23:45:50\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0898 | Avg: 0.0774 | Tempo: 0.66s | Restante: ~1m0s | ğŸ• 23:45:53\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.0450 | Avg: 0.0770 | Tempo: 0.47s | Restante: ~0m59s | ğŸ• 23:45:54\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0796 | Avg: 0.0770 | Tempo: 0.50s | Restante: ~0m59s | ğŸ• 23:45:55\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0711 | Avg: 0.0769 | Tempo: 0.61s | Restante: ~0m58s | ğŸ• 23:45:57\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.2573 | Avg: 0.0791 | Tempo: 0.48s | Restante: ~0m57s | ğŸ• 23:45:58\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.1312 | Avg: 0.0798 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:45:59\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0314 | Avg: 0.0792 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:46:00\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0769 | Avg: 0.0792 | Tempo: 0.48s | Restante: ~0m56s | ğŸ• 23:46:01\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0697 | Avg: 0.0791 | Tempo: 0.77s | Restante: ~0m55s | ğŸ• 23:46:03\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0539 | Avg: 0.0788 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:46:04\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0439 | Avg: 0.0784 | Tempo: 0.48s | Restante: ~0m54s | ğŸ• 23:46:05\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.0604 | Avg: 0.0782 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:46:06\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.0409 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:46:07\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0695 | Avg: 0.0777 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:46:08\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0681 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:46:09\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0519 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~0m51s | ğŸ• 23:46:10\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0584 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:46:12\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.1215 | Avg: 0.0776 | Tempo: 0.57s | Restante: ~0m50s | ğŸ• 23:46:13\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.1084 | Avg: 0.0779 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:46:14\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0606 | Avg: 0.0777 | Tempo: 0.50s | Restante: ~0m49s | ğŸ• 23:46:15\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0530 | Avg: 0.0774 | Tempo: 0.50s | Restante: ~0m48s | ğŸ• 23:46:16\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0818 | Avg: 0.0775 | Tempo: 0.49s | Restante: ~0m48s | ğŸ• 23:46:17\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0961 | Avg: 0.0777 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:46:18\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0399 | Avg: 0.0773 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:46:19\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0612 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:46:21\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0582 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:46:22\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.1142 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:46:23\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0670 | Avg: 0.0772 | Tempo: 0.54s | Restante: ~0m44s | ğŸ• 23:46:25\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.0808 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:46:26\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1013 | Avg: 0.0775 | Tempo: 0.61s | Restante: ~0m43s | ğŸ• 23:46:27\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0785 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~0m43s | ğŸ• 23:46:28\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0771 | Avg: 0.0775 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:46:29\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0513 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:46:30\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.1032 | Avg: 0.0775 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:46:31\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.0492 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:46:32\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.2084 | Avg: 0.0784 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:46:33\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.1168 | Avg: 0.0787 | Tempo: 0.50s | Restante: ~0m39s | ğŸ• 23:46:35\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.1056 | Avg: 0.0790 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:46:36\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.0312 | Avg: 0.0785 | Tempo: 0.89s | Restante: ~0m38s | ğŸ• 23:46:37\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0624 | Avg: 0.0784 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:46:38\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0339 | Avg: 0.0780 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:46:39\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0480 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:46:40\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.0971 | Avg: 0.0779 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:46:41\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.1048 | Avg: 0.0782 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:46:42\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.1104 | Avg: 0.0784 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:46:44\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.1078 | Avg: 0.0787 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:46:45\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.1006 | Avg: 0.0788 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:46:46\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.0586 | Avg: 0.0787 | Tempo: 0.50s | Restante: ~0m33s | ğŸ• 23:46:47\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0812 | Avg: 0.0787 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:46:48\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.0924 | Avg: 0.0788 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:46:49\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.0840 | Avg: 0.0789 | Tempo: 0.49s | Restante: ~0m31s | ğŸ• 23:46:50\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.0752 | Avg: 0.0788 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:46:51\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.0608 | Avg: 0.0787 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:46:52\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0439 | Avg: 0.0784 | Tempo: 0.50s | Restante: ~0m30s | ğŸ• 23:46:54\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0575 | Avg: 0.0783 | Tempo: 0.50s | Restante: ~0m29s | ğŸ• 23:46:55\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.0428 | Avg: 0.0780 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:46:56\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0929 | Avg: 0.0781 | Tempo: 0.93s | Restante: ~0m28s | ğŸ• 23:46:57\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0323 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:46:58\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.0967 | Avg: 0.0779 | Tempo: 0.49s | Restante: ~0m27s | ğŸ• 23:46:59\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0439 | Avg: 0.0777 | Tempo: 0.47s | Restante: ~0m27s | ğŸ• 23:47:00\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.0490 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:47:01\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0776 | Avg: 0.0774 | Tempo: 0.50s | Restante: ~0m26s | ğŸ• 23:47:03\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0456 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:47:04\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.1136 | Avg: 0.0775 | Tempo: 0.72s | Restante: ~0m25s | ğŸ• 23:47:07\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.0622 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:47:08\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.0722 | Avg: 0.0773 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:47:09\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.0590 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:47:10\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0687 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:47:11\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0597 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:47:12\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0714 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:47:13\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.0985 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:47:14\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0595 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:47:15\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.0662 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~0m20s | ğŸ• 23:47:17\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0854 | Avg: 0.0770 | Tempo: 0.47s | Restante: ~0m19s | ğŸ• 23:47:18\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.1122 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:47:19\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0500 | Avg: 0.0771 | Tempo: 0.50s | Restante: ~0m18s | ğŸ• 23:47:20\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0405 | Avg: 0.0768 | Tempo: 0.93s | Restante: ~0m18s | ğŸ• 23:47:21\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.0300 | Avg: 0.0765 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:47:22\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0606 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m17s | ğŸ• 23:47:23\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0564 | Avg: 0.0763 | Tempo: 0.50s | Restante: ~0m16s | ğŸ• 23:47:24\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.0798 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~0m15s | ğŸ• 23:47:26\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0366 | Avg: 0.0761 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:47:27\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0524 | Avg: 0.0759 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:47:28\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0838 | Avg: 0.0760 | Tempo: 0.47s | Restante: ~0m14s | ğŸ• 23:47:29\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.1150 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~0m13s | ğŸ• 23:47:30\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0581 | Avg: 0.0761 | Tempo: 0.59s | Restante: ~0m13s | ğŸ• 23:47:31\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0676 | Avg: 0.0760 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:47:32\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.0483 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:47:33\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0767 | Avg: 0.0759 | Tempo: 0.48s | Restante: ~0m11s | ğŸ• 23:47:35\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.1000 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m11s | ğŸ• 23:47:36\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0588 | Avg: 0.0759 | Tempo: 0.50s | Restante: ~0m10s | ğŸ• 23:47:37\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0701 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:47:38\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0774 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:47:39\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0468 | Avg: 0.0757 | Tempo: 0.47s | Restante: ~0m8s | ğŸ• 23:47:40\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0980 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:47:41\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.1540 | Avg: 0.0763 | Tempo: 0.65s | Restante: ~0m7s | ğŸ• 23:47:44\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0657 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:47:45\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0355 | Avg: 0.0760 | Tempo: 0.47s | Restante: ~0m6s | ğŸ• 23:47:46\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0300 | Avg: 0.0757 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:47:47\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0811 | Avg: 0.0758 | Tempo: 0.47s | Restante: ~0m5s | ğŸ• 23:47:48\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0709 | Avg: 0.0758 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:47:49\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0605 | Avg: 0.0757 | Tempo: 0.47s | Restante: ~0m4s | ğŸ• 23:47:50\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0484 | Avg: 0.0755 | Tempo: 0.50s | Restante: ~0m4s | ğŸ• 23:47:51\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0652 | Avg: 0.0755 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:47:52\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.1103 | Avg: 0.0757 | Tempo: 0.48s | Restante: ~0m3s | ğŸ• 23:47:54\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0491 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:47:55\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0602 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:47:56\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0888 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:47:57\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0744 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:47:58\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0665 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:47:59\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0606 | Avg: 0.0754 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:48:00\n",
      "\n",
      "âœ… Ã‰poca 7 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0754\n",
      "   ğŸ“Š Loss Classifier: 0.0257\n",
      "   ğŸ“Š Loss Box Reg: 0.0444\n",
      "   ğŸ“Š Loss Objectness: 0.0019\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0033\n",
      "   â±ï¸  Tempo total: 3m42s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.52s\n",
      "ğŸ”µ === FIM Ã‰POCA 7/10, Loss: 0.0754 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0754)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_7.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 8/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 8\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 8/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000050\n",
      "   ğŸ• InÃ­cio: 23:48:01\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0832\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0832 | Avg: 0.0832 | Tempo: 0.87s | Restante: ~2m42s | ğŸ• 23:48:02\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0639 | Avg: 0.0735 | Tempo: 0.85s | Restante: ~2m40s | ğŸ• 23:48:03\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0715 | Avg: 0.0728 | Tempo: 0.48s | Restante: ~2m16s | ğŸ• 23:48:05\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0624 | Avg: 0.0702 | Tempo: 0.48s | Restante: ~2m3s | ğŸ• 23:48:06\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0881 | Avg: 0.0738 | Tempo: 1.03s | Restante: ~2m16s | ğŸ• 23:48:07\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.1154 | Avg: 0.0807 | Tempo: 0.49s | Restante: ~2m7s | ğŸ• 23:48:08\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0547 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~2m1s | ğŸ• 23:48:09\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0438 | Avg: 0.0729 | Tempo: 0.49s | Restante: ~1m56s | ğŸ• 23:48:10\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.1091 | Avg: 0.0769 | Tempo: 0.50s | Restante: ~1m53s | ğŸ• 23:48:11\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0478 | Avg: 0.0740 | Tempo: 0.78s | Restante: ~1m55s | ğŸ• 23:48:12\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.0955 | Avg: 0.0759 | Tempo: 0.78s | Restante: ~1m56s | ğŸ• 23:48:13\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.0890 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~1m53s | ğŸ• 23:48:15\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0480 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~1m50s | ğŸ• 23:48:16\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0707 | Avg: 0.0745 | Tempo: 0.48s | Restante: ~1m48s | ğŸ• 23:48:17\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0693 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~1m46s | ğŸ• 23:48:18\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.0793 | Avg: 0.0745 | Tempo: 0.49s | Restante: ~1m44s | ğŸ• 23:48:19\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0530 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~1m42s | ğŸ• 23:48:20\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0605 | Avg: 0.0725 | Tempo: 0.47s | Restante: ~1m40s | ğŸ• 23:48:21\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.1163 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~1m39s | ğŸ• 23:48:22\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0616 | Avg: 0.0742 | Tempo: 0.65s | Restante: ~1m39s | ğŸ• 23:48:25\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0959 | Avg: 0.0752 | Tempo: 0.65s | Restante: ~1m38s | ğŸ• 23:48:27\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0638 | Avg: 0.0747 | Tempo: 0.48s | Restante: ~1m37s | ğŸ• 23:48:28\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0584 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~1m36s | ğŸ• 23:48:29\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0597 | Avg: 0.0734 | Tempo: 0.48s | Restante: ~1m34s | ğŸ• 23:48:30\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.0904 | Avg: 0.0740 | Tempo: 0.50s | Restante: ~1m33s | ğŸ• 23:48:31\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0644 | Avg: 0.0737 | Tempo: 0.48s | Restante: ~1m32s | ğŸ• 23:48:33\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0580 | Avg: 0.0731 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:48:34\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0787 | Avg: 0.0733 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:48:35\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0448 | Avg: 0.0723 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:48:36\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.0624 | Avg: 0.0720 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:48:37\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0548 | Avg: 0.0714 | Tempo: 0.49s | Restante: ~1m27s | ğŸ• 23:48:38\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0988 | Avg: 0.0723 | Tempo: 0.47s | Restante: ~1m26s | ğŸ• 23:48:39\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0800 | Avg: 0.0725 | Tempo: 0.48s | Restante: ~1m25s | ğŸ• 23:48:40\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0459 | Avg: 0.0717 | Tempo: 0.76s | Restante: ~1m26s | ğŸ• 23:48:42\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0497 | Avg: 0.0711 | Tempo: 0.53s | Restante: ~1m25s | ğŸ• 23:48:43\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1225 | Avg: 0.0725 | Tempo: 0.72s | Restante: ~1m25s | ğŸ• 23:48:46\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0327 | Avg: 0.0715 | Tempo: 0.47s | Restante: ~1m24s | ğŸ• 23:48:47\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0490 | Avg: 0.0709 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:48:48\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0991 | Avg: 0.0716 | Tempo: 0.48s | Restante: ~1m22s | ğŸ• 23:48:49\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0532 | Avg: 0.0711 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:48:51\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.0848 | Avg: 0.0715 | Tempo: 0.49s | Restante: ~1m21s | ğŸ• 23:48:52\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.0776 | Avg: 0.0716 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:48:53\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0508 | Avg: 0.0711 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:48:54\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0805 | Avg: 0.0713 | Tempo: 0.50s | Restante: ~1m19s | ğŸ• 23:48:55\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0818 | Avg: 0.0716 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:48:56\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.1161 | Avg: 0.0725 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:48:57\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0605 | Avg: 0.0723 | Tempo: 0.49s | Restante: ~1m16s | ğŸ• 23:48:58\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0563 | Avg: 0.0720 | Tempo: 0.48s | Restante: ~1m16s | ğŸ• 23:49:00\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0647 | Avg: 0.0718 | Tempo: 0.49s | Restante: ~1m15s | ğŸ• 23:49:01\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0767 | Avg: 0.0719 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:49:02\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0440 | Avg: 0.0714 | Tempo: 0.48s | Restante: ~1m13s | ğŸ• 23:49:03\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0582 | Avg: 0.0711 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:49:04\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0860 | Avg: 0.0714 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:49:05\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.1260 | Avg: 0.0724 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:49:06\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0920 | Avg: 0.0728 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:49:07\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.0641 | Avg: 0.0726 | Tempo: 0.50s | Restante: ~1m10s | ğŸ• 23:49:08\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0888 | Avg: 0.0729 | Tempo: 0.47s | Restante: ~1m9s | ğŸ• 23:49:10\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0863 | Avg: 0.0731 | Tempo: 0.94s | Restante: ~1m10s | ğŸ• 23:49:11\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.0475 | Avg: 0.0727 | Tempo: 0.50s | Restante: ~1m9s | ğŸ• 23:49:12\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0659 | Avg: 0.0726 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:49:13\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0807 | Avg: 0.0727 | Tempo: 0.58s | Restante: ~1m8s | ğŸ• 23:49:14\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0513 | Avg: 0.0724 | Tempo: 0.50s | Restante: ~1m7s | ğŸ• 23:49:15\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1135 | Avg: 0.0730 | Tempo: 0.91s | Restante: ~1m8s | ğŸ• 23:49:17\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0536 | Avg: 0.0727 | Tempo: 0.52s | Restante: ~1m7s | ğŸ• 23:49:18\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0888 | Avg: 0.0730 | Tempo: 0.51s | Restante: ~1m6s | ğŸ• 23:49:19\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0841 | Avg: 0.0731 | Tempo: 0.94s | Restante: ~1m7s | ğŸ• 23:49:20\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0529 | Avg: 0.0728 | Tempo: 0.49s | Restante: ~1m6s | ğŸ• 23:49:21\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.1130 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:49:22\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0482 | Avg: 0.0730 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:49:23\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.1347 | Avg: 0.0739 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:49:25\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0366 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:49:26\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0514 | Avg: 0.0731 | Tempo: 0.71s | Restante: ~1m3s | ğŸ• 23:49:29\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0664 | Avg: 0.0730 | Tempo: 0.61s | Restante: ~1m3s | ğŸ• 23:49:30\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0684 | Avg: 0.0729 | Tempo: 0.61s | Restante: ~1m2s | ğŸ• 23:49:31\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0502 | Avg: 0.0726 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:49:32\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0758 | Avg: 0.0727 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:49:33\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0530 | Avg: 0.0724 | Tempo: 0.48s | Restante: ~1m0s | ğŸ• 23:49:34\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0820 | Avg: 0.0725 | Tempo: 0.50s | Restante: ~1m0s | ğŸ• 23:49:35\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1008 | Avg: 0.0729 | Tempo: 0.48s | Restante: ~0m59s | ğŸ• 23:49:37\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0714 | Avg: 0.0729 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:49:38\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0966 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:49:39\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.2522 | Avg: 0.0754 | Tempo: 0.87s | Restante: ~0m58s | ğŸ• 23:49:40\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0613 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:49:41\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0854 | Avg: 0.0753 | Tempo: 0.50s | Restante: ~0m56s | ğŸ• 23:49:42\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0774 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:49:43\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0655 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m55s | ğŸ• 23:49:44\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.1180 | Avg: 0.0757 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:49:45\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.1897 | Avg: 0.0770 | Tempo: 0.64s | Restante: ~0m54s | ğŸ• 23:49:48\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.0576 | Avg: 0.0768 | Tempo: 0.50s | Restante: ~0m54s | ğŸ• 23:49:49\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.0352 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:49:50\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0840 | Avg: 0.0764 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:49:51\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.1846 | Avg: 0.0776 | Tempo: 0.55s | Restante: ~0m52s | ğŸ• 23:49:52\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0606 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:49:53\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0741 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:49:54\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0485 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:49:56\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.0955 | Avg: 0.0773 | Tempo: 0.48s | Restante: ~0m49s | ğŸ• 23:49:57\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.1315 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m49s | ğŸ• 23:49:58\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0881 | Avg: 0.0779 | Tempo: 0.50s | Restante: ~0m48s | ğŸ• 23:49:59\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0643 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~0m48s | ğŸ• 23:50:00\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0510 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~0m47s | ğŸ• 23:50:01\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0877 | Avg: 0.0776 | Tempo: 0.58s | Restante: ~0m47s | ğŸ• 23:50:02\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0615 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~0m46s | ğŸ• 23:50:03\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0650 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~0m45s | ğŸ• 23:50:05\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.0781 | Avg: 0.0773 | Tempo: 0.48s | Restante: ~0m45s | ğŸ• 23:50:06\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0657 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m44s | ğŸ• 23:50:07\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.0387 | Avg: 0.0769 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:50:08\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.1163 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m43s | ğŸ• 23:50:09\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0418 | Avg: 0.0769 | Tempo: 0.49s | Restante: ~0m42s | ğŸ• 23:50:10\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0755 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~0m42s | ğŸ• 23:50:11\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0566 | Avg: 0.0767 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:50:12\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.1013 | Avg: 0.0769 | Tempo: 0.50s | Restante: ~0m41s | ğŸ• 23:50:13\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.1123 | Avg: 0.0773 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:50:15\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.0504 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:50:16\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0805 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~0m39s | ğŸ• 23:50:17\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.0935 | Avg: 0.0772 | Tempo: 0.50s | Restante: ~0m38s | ğŸ• 23:50:18\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.1042 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:50:19\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0518 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:50:20\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0585 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:50:21\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0651 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:50:22\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.0875 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:50:24\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.0677 | Avg: 0.0770 | Tempo: 0.50s | Restante: ~0m35s | ğŸ• 23:50:25\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0446 | Avg: 0.0767 | Tempo: 0.48s | Restante: ~0m35s | ğŸ• 23:50:26\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0418 | Avg: 0.0764 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:50:27\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.0656 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:50:28\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.0881 | Avg: 0.0764 | Tempo: 0.48s | Restante: ~0m33s | ğŸ• 23:50:29\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0684 | Avg: 0.0763 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:50:30\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.0535 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~0m32s | ğŸ• 23:50:31\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.0705 | Avg: 0.0761 | Tempo: 0.60s | Restante: ~0m31s | ğŸ• 23:50:32\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.2069 | Avg: 0.0771 | Tempo: 0.48s | Restante: ~0m31s | ğŸ• 23:50:34\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.0369 | Avg: 0.0768 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:50:35\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.1953 | Avg: 0.0777 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:50:36\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0448 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~0m29s | ğŸ• 23:50:37\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.1600 | Avg: 0.0781 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:50:38\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0537 | Avg: 0.0779 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:50:39\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0608 | Avg: 0.0778 | Tempo: 0.53s | Restante: ~0m27s | ğŸ• 23:50:40\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.0482 | Avg: 0.0776 | Tempo: 0.51s | Restante: ~0m27s | ğŸ• 23:50:41\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0578 | Avg: 0.0774 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:50:43\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1842 | Avg: 0.0782 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:50:44\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0487 | Avg: 0.0780 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:50:45\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0430 | Avg: 0.0777 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:50:46\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.1292 | Avg: 0.0781 | Tempo: 0.66s | Restante: ~0m24s | ğŸ• 23:50:48\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.0653 | Avg: 0.0780 | Tempo: 0.48s | Restante: ~0m24s | ğŸ• 23:50:49\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.0416 | Avg: 0.0778 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:50:50\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.0590 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:50:52\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0719 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:50:53\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0270 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:50:54\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0460 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:50:55\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.0404 | Avg: 0.0768 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:50:56\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0324 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:50:57\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.2470 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:50:58\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0783 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:50:59\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0771 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~0m18s | ğŸ• 23:51:01\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0389 | Avg: 0.0774 | Tempo: 0.51s | Restante: ~0m18s | ğŸ• 23:51:02\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0646 | Avg: 0.0773 | Tempo: 1.02s | Restante: ~0m17s | ğŸ• 23:51:03\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.0444 | Avg: 0.0771 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:51:04\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0740 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m16s | ğŸ• 23:51:05\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.1043 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:51:06\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.1257 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:51:07\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0625 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:51:08\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0569 | Avg: 0.0773 | Tempo: 0.65s | Restante: ~0m14s | ğŸ• 23:51:11\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0493 | Avg: 0.0771 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:51:12\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0883 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:51:13\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0946 | Avg: 0.0773 | Tempo: 0.64s | Restante: ~0m13s | ğŸ• 23:51:15\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0836 | Avg: 0.0774 | Tempo: 0.48s | Restante: ~0m12s | ğŸ• 23:51:16\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.0705 | Avg: 0.0773 | Tempo: 0.50s | Restante: ~0m12s | ğŸ• 23:51:17\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0522 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:51:18\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.0494 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m11s | ğŸ• 23:51:20\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0693 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~0m10s | ğŸ• 23:51:21\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0495 | Avg: 0.0768 | Tempo: 0.49s | Restante: ~0m9s | ğŸ• 23:51:22\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0798 | Avg: 0.0768 | Tempo: 0.78s | Restante: ~0m9s | ğŸ• 23:51:23\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0555 | Avg: 0.0767 | Tempo: 0.48s | Restante: ~0m8s | ğŸ• 23:51:24\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0507 | Avg: 0.0765 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:51:25\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0572 | Avg: 0.0764 | Tempo: 0.48s | Restante: ~0m7s | ğŸ• 23:51:26\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.1088 | Avg: 0.0766 | Tempo: 0.48s | Restante: ~0m7s | ğŸ• 23:51:27\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0711 | Avg: 0.0766 | Tempo: 0.47s | Restante: ~0m6s | ğŸ• 23:51:28\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0623 | Avg: 0.0765 | Tempo: 0.47s | Restante: ~0m6s | ğŸ• 23:51:30\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0513 | Avg: 0.0764 | Tempo: 0.61s | Restante: ~0m5s | ğŸ• 23:51:32\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0484 | Avg: 0.0762 | Tempo: 0.51s | Restante: ~0m5s | ğŸ• 23:51:33\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0861 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:51:34\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0720 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:51:35\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0369 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m3s | ğŸ• 23:51:36\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.1317 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:51:37\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0321 | Avg: 0.0761 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:51:38\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0593 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:51:40\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0859 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:51:41\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0869 | Avg: 0.0761 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:51:42\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0366 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m0s | ğŸ• 23:51:43\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0451 | Avg: 0.0757 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:51:43\n",
      "\n",
      "âœ… Ã‰poca 8 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0757\n",
      "   ğŸ“Š Loss Classifier: 0.0262\n",
      "   ğŸ“Š Loss Box Reg: 0.0441\n",
      "   ğŸ“Š Loss Objectness: 0.0022\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0033\n",
      "   â±ï¸  Tempo total: 3m42s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.52s\n",
      "ğŸ”µ === FIM Ã‰POCA 8/10, Loss: 0.0757 ===\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_8.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 9/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 9\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 9/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000050\n",
      "   ğŸ• InÃ­cio: 23:51:46\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0686\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0686 | Avg: 0.0686 | Tempo: 0.50s | Restante: ~1m33s | ğŸ• 23:51:47\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0765 | Avg: 0.0726 | Tempo: 0.60s | Restante: ~1m42s | ğŸ• 23:51:48\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.0517 | Avg: 0.0656 | Tempo: 0.49s | Restante: ~1m37s | ğŸ• 23:51:49\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0898 | Avg: 0.0717 | Tempo: 0.48s | Restante: ~1m34s | ğŸ• 23:51:50\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0475 | Avg: 0.0668 | Tempo: 0.48s | Restante: ~1m33s | ğŸ• 23:51:51\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.1164 | Avg: 0.0751 | Tempo: 0.49s | Restante: ~1m32s | ğŸ• 23:51:52\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.1220 | Avg: 0.0818 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:51:53\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0755 | Avg: 0.0810 | Tempo: 0.48s | Restante: ~1m29s | ğŸ• 23:51:55\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0383 | Avg: 0.0763 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:51:56\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.2104 | Avg: 0.0897 | Tempo: 0.49s | Restante: ~1m28s | ğŸ• 23:51:57\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.0611 | Avg: 0.0871 | Tempo: 0.92s | Restante: ~1m34s | ğŸ• 23:51:58\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.0621 | Avg: 0.0850 | Tempo: 0.49s | Restante: ~1m33s | ğŸ• 23:51:59\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0740 | Avg: 0.0842 | Tempo: 0.49s | Restante: ~1m32s | ğŸ• 23:52:00\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.0369 | Avg: 0.0808 | Tempo: 0.48s | Restante: ~1m31s | ğŸ• 23:52:01\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0512 | Avg: 0.0788 | Tempo: 0.49s | Restante: ~1m30s | ğŸ• 23:52:02\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.2163 | Avg: 0.0874 | Tempo: 0.48s | Restante: ~1m29s | ğŸ• 23:52:03\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0710 | Avg: 0.0864 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:52:05\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0729 | Avg: 0.0857 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:52:06\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0984 | Avg: 0.0864 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:52:07\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0740 | Avg: 0.0857 | Tempo: 0.49s | Restante: ~1m26s | ğŸ• 23:52:08\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0665 | Avg: 0.0848 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:52:09\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0661 | Avg: 0.0840 | Tempo: 0.50s | Restante: ~1m24s | ğŸ• 23:52:10\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.1574 | Avg: 0.0872 | Tempo: 0.50s | Restante: ~1m24s | ğŸ• 23:52:11\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0476 | Avg: 0.0855 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:52:12\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.0538 | Avg: 0.0842 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:52:14\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0814 | Avg: 0.0841 | Tempo: 0.48s | Restante: ~1m22s | ğŸ• 23:52:15\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.0477 | Avg: 0.0828 | Tempo: 0.55s | Restante: ~1m21s | ğŸ• 23:52:16\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0451 | Avg: 0.0814 | Tempo: 0.48s | Restante: ~1m21s | ğŸ• 23:52:18\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.1422 | Avg: 0.0835 | Tempo: 0.65s | Restante: ~1m21s | ğŸ• 23:52:20\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.0533 | Avg: 0.0825 | Tempo: 0.47s | Restante: ~1m20s | ğŸ• 23:52:21\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0959 | Avg: 0.0830 | Tempo: 0.49s | Restante: ~1m20s | ğŸ• 23:52:22\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0370 | Avg: 0.0815 | Tempo: 0.48s | Restante: ~1m19s | ğŸ• 23:52:23\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0814 | Avg: 0.0815 | Tempo: 0.49s | Restante: ~1m18s | ğŸ• 23:52:24\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0720 | Avg: 0.0812 | Tempo: 0.50s | Restante: ~1m18s | ğŸ• 23:52:25\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0636 | Avg: 0.0807 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:52:27\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.0615 | Avg: 0.0802 | Tempo: 0.48s | Restante: ~1m17s | ğŸ• 23:52:28\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0371 | Avg: 0.0790 | Tempo: 0.48s | Restante: ~1m16s | ğŸ• 23:52:29\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0538 | Avg: 0.0784 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:52:30\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0815 | Avg: 0.0785 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:52:31\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0542 | Avg: 0.0778 | Tempo: 0.49s | Restante: ~1m14s | ğŸ• 23:52:32\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.0635 | Avg: 0.0775 | Tempo: 0.50s | Restante: ~1m14s | ğŸ• 23:52:33\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.1558 | Avg: 0.0794 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:52:34\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0427 | Avg: 0.0785 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:52:35\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0394 | Avg: 0.0776 | Tempo: 0.48s | Restante: ~1m12s | ğŸ• 23:52:37\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0968 | Avg: 0.0780 | Tempo: 0.47s | Restante: ~1m11s | ğŸ• 23:52:38\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.0532 | Avg: 0.0775 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:52:39\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0472 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~1m10s | ğŸ• 23:52:40\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0631 | Avg: 0.0766 | Tempo: 0.48s | Restante: ~1m10s | ğŸ• 23:52:41\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0622 | Avg: 0.0763 | Tempo: 1.03s | Restante: ~1m11s | ğŸ• 23:52:42\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0693 | Avg: 0.0761 | Tempo: 0.49s | Restante: ~1m10s | ğŸ• 23:52:43\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.0430 | Avg: 0.0755 | Tempo: 0.51s | Restante: ~1m10s | ğŸ• 23:52:44\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0774 | Avg: 0.0755 | Tempo: 0.49s | Restante: ~1m9s | ğŸ• 23:52:46\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0693 | Avg: 0.0754 | Tempo: 0.50s | Restante: ~1m9s | ğŸ• 23:52:47\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.0822 | Avg: 0.0755 | Tempo: 0.48s | Restante: ~1m8s | ğŸ• 23:52:48\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0351 | Avg: 0.0748 | Tempo: 0.48s | Restante: ~1m7s | ğŸ• 23:52:49\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.2080 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~1m7s | ğŸ• 23:52:50\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0780 | Avg: 0.0772 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:52:51\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0932 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:52:52\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.0640 | Avg: 0.0772 | Tempo: 0.49s | Restante: ~1m5s | ğŸ• 23:52:53\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0641 | Avg: 0.0770 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:52:54\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.0605 | Avg: 0.0768 | Tempo: 0.51s | Restante: ~1m4s | ğŸ• 23:52:56\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.1231 | Avg: 0.0775 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:52:57\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.0425 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~1m3s | ğŸ• 23:52:58\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0716 | Avg: 0.0769 | Tempo: 0.48s | Restante: ~1m2s | ğŸ• 23:52:59\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0841 | Avg: 0.0770 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:53:00\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0683 | Avg: 0.0768 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:53:01\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0557 | Avg: 0.0765 | Tempo: 0.57s | Restante: ~1m1s | ğŸ• 23:53:02\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.0658 | Avg: 0.0764 | Tempo: 0.48s | Restante: ~1m0s | ğŸ• 23:53:03\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0671 | Avg: 0.0762 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:53:04\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.0490 | Avg: 0.0758 | Tempo: 0.50s | Restante: ~0m59s | ğŸ• 23:53:06\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0809 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:53:07\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0654 | Avg: 0.0758 | Tempo: 0.47s | Restante: ~0m58s | ğŸ• 23:53:08\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0520 | Avg: 0.0754 | Tempo: 0.48s | Restante: ~0m58s | ğŸ• 23:53:09\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.1054 | Avg: 0.0758 | Tempo: 0.51s | Restante: ~0m57s | ğŸ• 23:53:10\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0836 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~0m57s | ğŸ• 23:53:11\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0743 | Avg: 0.0759 | Tempo: 0.49s | Restante: ~0m56s | ğŸ• 23:53:12\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0496 | Avg: 0.0756 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:53:13\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0564 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~0m55s | ğŸ• 23:53:15\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.1684 | Avg: 0.0765 | Tempo: 0.48s | Restante: ~0m54s | ğŸ• 23:53:16\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0814 | Avg: 0.0766 | Tempo: 0.47s | Restante: ~0m54s | ğŸ• 23:53:17\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0478 | Avg: 0.0762 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:53:18\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0725 | Avg: 0.0762 | Tempo: 0.50s | Restante: ~0m53s | ğŸ• 23:53:19\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0678 | Avg: 0.0761 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:53:20\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0432 | Avg: 0.0757 | Tempo: 0.48s | Restante: ~0m52s | ğŸ• 23:53:21\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0485 | Avg: 0.0754 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:53:22\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0733 | Avg: 0.0753 | Tempo: 0.47s | Restante: ~0m51s | ğŸ• 23:53:24\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0722 | Avg: 0.0753 | Tempo: 0.47s | Restante: ~0m50s | ğŸ• 23:53:25\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0484 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:53:26\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.0836 | Avg: 0.0751 | Tempo: 0.70s | Restante: ~0m49s | ğŸ• 23:53:29\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.0777 | Avg: 0.0751 | Tempo: 0.47s | Restante: ~0m49s | ğŸ• 23:53:30\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0385 | Avg: 0.0747 | Tempo: 0.50s | Restante: ~0m48s | ğŸ• 23:53:31\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0419 | Avg: 0.0744 | Tempo: 0.49s | Restante: ~0m48s | ğŸ• 23:53:32\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0402 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m47s | ğŸ• 23:53:33\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.1242 | Avg: 0.0745 | Tempo: 0.66s | Restante: ~0m47s | ğŸ• 23:53:35\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0869 | Avg: 0.0747 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:53:37\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.0586 | Avg: 0.0745 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:53:38\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0563 | Avg: 0.0743 | Tempo: 0.50s | Restante: ~0m45s | ğŸ• 23:53:39\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0416 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m45s | ğŸ• 23:53:40\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0466 | Avg: 0.0737 | Tempo: 0.48s | Restante: ~0m44s | ğŸ• 23:53:41\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0691 | Avg: 0.0737 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:53:42\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0684 | Avg: 0.0736 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:53:43\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.0517 | Avg: 0.0734 | Tempo: 0.47s | Restante: ~0m43s | ğŸ• 23:53:44\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0350 | Avg: 0.0730 | Tempo: 0.50s | Restante: ~0m42s | ğŸ• 23:53:46\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.1539 | Avg: 0.0738 | Tempo: 0.47s | Restante: ~0m42s | ğŸ• 23:53:47\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0448 | Avg: 0.0735 | Tempo: 0.50s | Restante: ~0m41s | ğŸ• 23:53:48\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.1005 | Avg: 0.0738 | Tempo: 0.48s | Restante: ~0m41s | ğŸ• 23:53:49\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.0801 | Avg: 0.0738 | Tempo: 0.61s | Restante: ~0m40s | ğŸ• 23:53:51\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0824 | Avg: 0.0739 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:53:52\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.1505 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:53:53\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0397 | Avg: 0.0743 | Tempo: 0.48s | Restante: ~0m39s | ğŸ• 23:53:54\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.0453 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:53:55\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.0901 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m38s | ğŸ• 23:53:57\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.1034 | Avg: 0.0744 | Tempo: 0.48s | Restante: ~0m37s | ğŸ• 23:53:58\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0358 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:53:59\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.0406 | Avg: 0.0738 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:54:00\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.1582 | Avg: 0.0745 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:54:01\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.0370 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m35s | ğŸ• 23:54:02\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0411 | Avg: 0.0739 | Tempo: 0.47s | Restante: ~0m35s | ğŸ• 23:54:03\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0896 | Avg: 0.0741 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:54:04\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.1088 | Avg: 0.0744 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:54:05\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.0733 | Avg: 0.0743 | Tempo: 0.48s | Restante: ~0m33s | ğŸ• 23:54:07\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0315 | Avg: 0.0740 | Tempo: 0.65s | Restante: ~0m33s | ğŸ• 23:54:09\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0911 | Avg: 0.0741 | Tempo: 0.50s | Restante: ~0m32s | ğŸ• 23:54:10\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.0749 | Avg: 0.0741 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:54:11\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.0605 | Avg: 0.0740 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:54:12\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0962 | Avg: 0.0742 | Tempo: 0.49s | Restante: ~0m31s | ğŸ• 23:54:13\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.0735 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m30s | ğŸ• 23:54:15\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.1118 | Avg: 0.0745 | Tempo: 0.71s | Restante: ~0m30s | ğŸ• 23:54:17\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.0811 | Avg: 0.0745 | Tempo: 0.48s | Restante: ~0m29s | ğŸ• 23:54:19\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.1154 | Avg: 0.0749 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:54:20\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0574 | Avg: 0.0747 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:54:21\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0469 | Avg: 0.0745 | Tempo: 0.48s | Restante: ~0m28s | ğŸ• 23:54:22\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.0762 | Avg: 0.0745 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:54:23\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0821 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:54:24\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0302 | Avg: 0.0743 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:54:25\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.0614 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m26s | ğŸ• 23:54:26\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0518 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:54:28\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.0644 | Avg: 0.0739 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:54:29\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.1338 | Avg: 0.0744 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:54:30\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0439 | Avg: 0.0741 | Tempo: 0.47s | Restante: ~0m24s | ğŸ• 23:54:31\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0848 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m23s | ğŸ• 23:54:32\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.1117 | Avg: 0.0745 | Tempo: 0.65s | Restante: ~0m23s | ğŸ• 23:54:34\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.0905 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:54:35\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.0713 | Avg: 0.0746 | Tempo: 0.49s | Restante: ~0m22s | ğŸ• 23:54:36\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0328 | Avg: 0.0743 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:54:38\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0636 | Avg: 0.0742 | Tempo: 0.50s | Restante: ~0m21s | ğŸ• 23:54:39\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0684 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m20s | ğŸ• 23:54:40\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.0628 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:54:41\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0860 | Avg: 0.0742 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:54:42\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.1173 | Avg: 0.0745 | Tempo: 0.49s | Restante: ~0m19s | ğŸ• 23:54:43\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.1945 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~0m18s | ğŸ• 23:54:44\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0927 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m18s | ğŸ• 23:54:45\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0499 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:54:47\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0987 | Avg: 0.0754 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:54:48\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.1365 | Avg: 0.0757 | Tempo: 0.64s | Restante: ~0m16s | ğŸ• 23:54:50\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0560 | Avg: 0.0756 | Tempo: 0.65s | Restante: ~0m16s | ğŸ• 23:54:52\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0327 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:54:53\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.0743 | Avg: 0.0753 | Tempo: 1.02s | Restante: ~0m15s | ğŸ• 23:54:54\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0471 | Avg: 0.0752 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:54:56\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0911 | Avg: 0.0753 | Tempo: 0.48s | Restante: ~0m14s | ğŸ• 23:54:57\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0918 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~0m13s | ğŸ• 23:54:58\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0343 | Avg: 0.0751 | Tempo: 0.48s | Restante: ~0m13s | ğŸ• 23:54:59\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.0652 | Avg: 0.0751 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:55:00\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0627 | Avg: 0.0750 | Tempo: 0.48s | Restante: ~0m12s | ğŸ• 23:55:01\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.0789 | Avg: 0.0750 | Tempo: 0.48s | Restante: ~0m11s | ğŸ• 23:55:02\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0527 | Avg: 0.0749 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:55:03\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.0862 | Avg: 0.0749 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:55:05\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0413 | Avg: 0.0747 | Tempo: 0.48s | Restante: ~0m10s | ğŸ• 23:55:06\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0518 | Avg: 0.0746 | Tempo: 0.50s | Restante: ~0m9s | ğŸ• 23:55:07\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0789 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m9s | ğŸ• 23:55:08\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.1086 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:55:09\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0544 | Avg: 0.0747 | Tempo: 0.48s | Restante: ~0m8s | ğŸ• 23:55:10\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.0701 | Avg: 0.0747 | Tempo: 0.88s | Restante: ~0m7s | ğŸ• 23:55:11\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0904 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~0m7s | ğŸ• 23:55:12\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.1441 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~0m6s | ğŸ• 23:55:13\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.0398 | Avg: 0.0750 | Tempo: 0.89s | Restante: ~0m6s | ğŸ• 23:55:15\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.0478 | Avg: 0.0748 | Tempo: 0.54s | Restante: ~0m5s | ğŸ• 23:55:16\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0779 | Avg: 0.0748 | Tempo: 0.53s | Restante: ~0m5s | ğŸ• 23:55:17\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0723 | Avg: 0.0748 | Tempo: 0.47s | Restante: ~0m4s | ğŸ• 23:55:18\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0344 | Avg: 0.0746 | Tempo: 0.51s | Restante: ~0m4s | ğŸ• 23:55:19\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.1074 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~0m3s | ğŸ• 23:55:20\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.0899 | Avg: 0.0749 | Tempo: 0.50s | Restante: ~0m3s | ğŸ• 23:55:21\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0853 | Avg: 0.0749 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:55:23\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0951 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~0m2s | ğŸ• 23:55:24\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0548 | Avg: 0.0749 | Tempo: 0.50s | Restante: ~0m1s | ğŸ• 23:55:25\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0531 | Avg: 0.0748 | Tempo: 0.48s | Restante: ~0m1s | ğŸ• 23:55:26\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0311 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m0s | ğŸ• 23:55:27\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.1135 | Avg: 0.0748 | Tempo: 0.25s | Restante: ~0m0s | ğŸ• 23:55:28\n",
      "\n",
      "âœ… Ã‰poca 9 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0748\n",
      "   ğŸ“Š Loss Classifier: 0.0258\n",
      "   ğŸ“Š Loss Box Reg: 0.0436\n",
      "   ğŸ“Š Loss Objectness: 0.0021\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0032\n",
      "   â±ï¸  Tempo total: 3m42s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.51s\n",
      "ğŸ”µ === FIM Ã‰POCA 9/10, Loss: 0.0748 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0748)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_9.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "ğŸ”µ === INÃCIO Ã‰POCA 10/10 ===\n",
      "\n",
      "ğŸ”µ FUNÃ‡ÃƒO train_one_epoch CHAMADA - Ã‰poca 10\n",
      "ğŸ”µ Modelo em modo train()\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Ã‰POCA 10/10\n",
      "============================================================\n",
      "   ğŸ“Š Total de batches: 188\n",
      "   ğŸ“¦ Batch size: 4\n",
      "   ğŸ“ˆ Learning rate: 0.000005\n",
      "   ğŸ• InÃ­cio: 23:55:31\n",
      "\n",
      "ğŸ”„ Iniciando treinamento...\n",
      "\n",
      "   ğŸ” Testando DataLoader...\n",
      "   âœ… DataLoader funcionando! Primeiro batch obtido.\n",
      "   âœ… Primeiro batch tem 4 imagens\n",
      "   ğŸ” Processando primeiro batch: 4 imagens\n",
      "   ğŸ” Imagens movidas para cuda, shape: [torch.Size([3, 416, 416]), torch.Size([3, 416, 416])]\n",
      "   ğŸ” Iniciando forward pass...\n",
      "   âœ… Forward pass concluÃ­do, loss: 0.0682\n",
      "   ğŸ“¦ [  1/188] |   0.5% | Loss: 0.0682 | Avg: 0.0682 | Tempo: 0.56s | Restante: ~1m44s | ğŸ• 23:55:32\n",
      "   ğŸ“¦ [  2/188] |   1.1% | Loss: 0.0562 | Avg: 0.0622 | Tempo: 0.65s | Restante: ~1m52s | ğŸ• 23:55:34\n",
      "   ğŸ“¦ [  3/188] |   1.6% | Loss: 0.1003 | Avg: 0.0749 | Tempo: 0.48s | Restante: ~1m44s | ğŸ• 23:55:35\n",
      "   ğŸ“¦ [  4/188] |   2.1% | Loss: 0.0548 | Avg: 0.0699 | Tempo: 0.48s | Restante: ~1m39s | ğŸ• 23:55:37\n",
      "   ğŸ“¦ [  5/188] |   2.7% | Loss: 0.0590 | Avg: 0.0677 | Tempo: 0.48s | Restante: ~1m37s | ğŸ• 23:55:38\n",
      "   ğŸ“¦ [  6/188] |   3.2% | Loss: 0.0819 | Avg: 0.0701 | Tempo: 0.49s | Restante: ~1m35s | ğŸ• 23:55:39\n",
      "   ğŸ“¦ [  7/188] |   3.7% | Loss: 0.0692 | Avg: 0.0699 | Tempo: 0.48s | Restante: ~1m33s | ğŸ• 23:55:40\n",
      "   ğŸ“¦ [  8/188] |   4.3% | Loss: 0.0499 | Avg: 0.0674 | Tempo: 0.47s | Restante: ~1m32s | ğŸ• 23:55:41\n",
      "   ğŸ“¦ [  9/188] |   4.8% | Loss: 0.0566 | Avg: 0.0662 | Tempo: 1.00s | Restante: ~1m41s | ğŸ• 23:55:42\n",
      "   ğŸ“¦ [ 10/188] |   5.3% | Loss: 0.0404 | Avg: 0.0636 | Tempo: 0.48s | Restante: ~1m39s | ğŸ• 23:55:43\n",
      "   ğŸ“¦ [ 11/188] |   5.9% | Loss: 0.1791 | Avg: 0.0741 | Tempo: 0.50s | Restante: ~1m38s | ğŸ• 23:55:44\n",
      "   ğŸ“¦ [ 12/188] |   6.4% | Loss: 0.0813 | Avg: 0.0747 | Tempo: 0.49s | Restante: ~1m36s | ğŸ• 23:55:45\n",
      "   ğŸ“¦ [ 13/188] |   6.9% | Loss: 0.0574 | Avg: 0.0734 | Tempo: 0.50s | Restante: ~1m35s | ğŸ• 23:55:47\n",
      "   ğŸ“¦ [ 14/188] |   7.4% | Loss: 0.1178 | Avg: 0.0766 | Tempo: 0.49s | Restante: ~1m34s | ğŸ• 23:55:48\n",
      "   ğŸ“¦ [ 15/188] |   8.0% | Loss: 0.0554 | Avg: 0.0752 | Tempo: 0.61s | Restante: ~1m34s | ğŸ• 23:55:50\n",
      "   ğŸ“¦ [ 16/188] |   8.5% | Loss: 0.0530 | Avg: 0.0738 | Tempo: 0.49s | Restante: ~1m33s | ğŸ• 23:55:51\n",
      "   ğŸ“¦ [ 17/188] |   9.0% | Loss: 0.0556 | Avg: 0.0727 | Tempo: 0.48s | Restante: ~1m32s | ğŸ• 23:55:52\n",
      "   ğŸ“¦ [ 18/188] |   9.6% | Loss: 0.0330 | Avg: 0.0705 | Tempo: 0.49s | Restante: ~1m31s | ğŸ• 23:55:53\n",
      "   ğŸ“¦ [ 19/188] |  10.1% | Loss: 0.0814 | Avg: 0.0711 | Tempo: 0.48s | Restante: ~1m30s | ğŸ• 23:55:54\n",
      "   ğŸ“¦ [ 20/188] |  10.6% | Loss: 0.0371 | Avg: 0.0694 | Tempo: 0.49s | Restante: ~1m29s | ğŸ• 23:55:55\n",
      "   ğŸ“¦ [ 21/188] |  11.2% | Loss: 0.0622 | Avg: 0.0690 | Tempo: 0.48s | Restante: ~1m28s | ğŸ• 23:55:57\n",
      "   ğŸ“¦ [ 22/188] |  11.7% | Loss: 0.0776 | Avg: 0.0694 | Tempo: 0.48s | Restante: ~1m27s | ğŸ• 23:55:58\n",
      "   ğŸ“¦ [ 23/188] |  12.2% | Loss: 0.0498 | Avg: 0.0686 | Tempo: 0.48s | Restante: ~1m26s | ğŸ• 23:55:59\n",
      "   ğŸ“¦ [ 24/188] |  12.8% | Loss: 0.0513 | Avg: 0.0679 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:56:00\n",
      "   ğŸ“¦ [ 25/188] |  13.3% | Loss: 0.1949 | Avg: 0.0729 | Tempo: 0.49s | Restante: ~1m25s | ğŸ• 23:56:01\n",
      "   ğŸ“¦ [ 26/188] |  13.8% | Loss: 0.0473 | Avg: 0.0719 | Tempo: 0.48s | Restante: ~1m24s | ğŸ• 23:56:02\n",
      "   ğŸ“¦ [ 27/188] |  14.4% | Loss: 0.1494 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~1m23s | ğŸ• 23:56:03\n",
      "   ğŸ“¦ [ 28/188] |  14.9% | Loss: 0.0670 | Avg: 0.0745 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:56:04\n",
      "   ğŸ“¦ [ 29/188] |  15.4% | Loss: 0.0431 | Avg: 0.0735 | Tempo: 0.49s | Restante: ~1m22s | ğŸ• 23:56:06\n",
      "   ğŸ“¦ [ 30/188] |  16.0% | Loss: 0.0632 | Avg: 0.0731 | Tempo: 0.48s | Restante: ~1m21s | ğŸ• 23:56:07\n",
      "   ğŸ“¦ [ 31/188] |  16.5% | Loss: 0.0762 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~1m20s | ğŸ• 23:56:08\n",
      "   ğŸ“¦ [ 32/188] |  17.0% | Loss: 0.0815 | Avg: 0.0735 | Tempo: 0.50s | Restante: ~1m20s | ğŸ• 23:56:09\n",
      "   ğŸ“¦ [ 33/188] |  17.6% | Loss: 0.0466 | Avg: 0.0727 | Tempo: 0.49s | Restante: ~1m19s | ğŸ• 23:56:10\n",
      "   ğŸ“¦ [ 34/188] |  18.1% | Loss: 0.0713 | Avg: 0.0726 | Tempo: 0.48s | Restante: ~1m18s | ğŸ• 23:56:11\n",
      "   ğŸ“¦ [ 35/188] |  18.6% | Loss: 0.0545 | Avg: 0.0721 | Tempo: 0.47s | Restante: ~1m18s | ğŸ• 23:56:12\n",
      "   ğŸ“¦ [ 36/188] |  19.1% | Loss: 0.1088 | Avg: 0.0731 | Tempo: 0.65s | Restante: ~1m18s | ğŸ• 23:56:15\n",
      "   ğŸ“¦ [ 37/188] |  19.7% | Loss: 0.0401 | Avg: 0.0722 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:56:16\n",
      "   ğŸ“¦ [ 38/188] |  20.2% | Loss: 0.0865 | Avg: 0.0726 | Tempo: 0.49s | Restante: ~1m17s | ğŸ• 23:56:17\n",
      "   ğŸ“¦ [ 39/188] |  20.7% | Loss: 0.0692 | Avg: 0.0725 | Tempo: 0.47s | Restante: ~1m16s | ğŸ• 23:56:18\n",
      "   ğŸ“¦ [ 40/188] |  21.3% | Loss: 0.0647 | Avg: 0.0723 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:56:19\n",
      "   ğŸ“¦ [ 41/188] |  21.8% | Loss: 0.0680 | Avg: 0.0722 | Tempo: 0.48s | Restante: ~1m15s | ğŸ• 23:56:20\n",
      "   ğŸ“¦ [ 42/188] |  22.3% | Loss: 0.0503 | Avg: 0.0717 | Tempo: 0.48s | Restante: ~1m14s | ğŸ• 23:56:21\n",
      "   ğŸ“¦ [ 43/188] |  22.9% | Loss: 0.0433 | Avg: 0.0710 | Tempo: 0.49s | Restante: ~1m13s | ğŸ• 23:56:22\n",
      "   ğŸ“¦ [ 44/188] |  23.4% | Loss: 0.0632 | Avg: 0.0709 | Tempo: 0.50s | Restante: ~1m13s | ğŸ• 23:56:24\n",
      "   ğŸ“¦ [ 45/188] |  23.9% | Loss: 0.0366 | Avg: 0.0701 | Tempo: 0.51s | Restante: ~1m12s | ğŸ• 23:56:25\n",
      "   ğŸ“¦ [ 46/188] |  24.5% | Loss: 0.0441 | Avg: 0.0695 | Tempo: 0.52s | Restante: ~1m12s | ğŸ• 23:56:26\n",
      "   ğŸ“¦ [ 47/188] |  25.0% | Loss: 0.0827 | Avg: 0.0698 | Tempo: 0.49s | Restante: ~1m11s | ğŸ• 23:56:27\n",
      "   ğŸ“¦ [ 48/188] |  25.5% | Loss: 0.0836 | Avg: 0.0701 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:56:28\n",
      "   ğŸ“¦ [ 49/188] |  26.1% | Loss: 0.0900 | Avg: 0.0705 | Tempo: 0.73s | Restante: ~1m11s | ğŸ• 23:56:29\n",
      "   ğŸ“¦ [ 50/188] |  26.6% | Loss: 0.0743 | Avg: 0.0706 | Tempo: 0.86s | Restante: ~1m11s | ğŸ• 23:56:30\n",
      "   ğŸ“¦ [ 51/188] |  27.1% | Loss: 0.1229 | Avg: 0.0716 | Tempo: 0.70s | Restante: ~1m11s | ğŸ• 23:56:33\n",
      "   ğŸ“¦ [ 52/188] |  27.7% | Loss: 0.0598 | Avg: 0.0714 | Tempo: 0.48s | Restante: ~1m11s | ğŸ• 23:56:34\n",
      "   ğŸ“¦ [ 53/188] |  28.2% | Loss: 0.0652 | Avg: 0.0713 | Tempo: 0.47s | Restante: ~1m10s | ğŸ• 23:56:36\n",
      "   ğŸ“¦ [ 54/188] |  28.7% | Loss: 0.0260 | Avg: 0.0704 | Tempo: 0.48s | Restante: ~1m9s | ğŸ• 23:56:37\n",
      "   ğŸ“¦ [ 55/188] |  29.3% | Loss: 0.0986 | Avg: 0.0709 | Tempo: 0.50s | Restante: ~1m9s | ğŸ• 23:56:38\n",
      "   ğŸ“¦ [ 56/188] |  29.8% | Loss: 0.0621 | Avg: 0.0708 | Tempo: 0.50s | Restante: ~1m8s | ğŸ• 23:56:39\n",
      "   ğŸ“¦ [ 57/188] |  30.3% | Loss: 0.0491 | Avg: 0.0704 | Tempo: 0.49s | Restante: ~1m8s | ğŸ• 23:56:40\n",
      "   ğŸ“¦ [ 58/188] |  30.9% | Loss: 0.0658 | Avg: 0.0703 | Tempo: 0.47s | Restante: ~1m7s | ğŸ• 23:56:41\n",
      "   ğŸ“¦ [ 59/188] |  31.4% | Loss: 0.1538 | Avg: 0.0717 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:56:42\n",
      "   ğŸ“¦ [ 60/188] |  31.9% | Loss: 0.0667 | Avg: 0.0716 | Tempo: 0.48s | Restante: ~1m6s | ğŸ• 23:56:43\n",
      "   ğŸ“¦ [ 61/188] |  32.4% | Loss: 0.2264 | Avg: 0.0742 | Tempo: 0.47s | Restante: ~1m5s | ğŸ• 23:56:45\n",
      "   ğŸ“¦ [ 62/188] |  33.0% | Loss: 0.0633 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~1m5s | ğŸ• 23:56:46\n",
      "   ğŸ“¦ [ 63/188] |  33.5% | Loss: 0.1979 | Avg: 0.0760 | Tempo: 0.48s | Restante: ~1m4s | ğŸ• 23:56:47\n",
      "   ğŸ“¦ [ 64/188] |  34.0% | Loss: 0.0605 | Avg: 0.0757 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:56:48\n",
      "   ğŸ“¦ [ 65/188] |  34.6% | Loss: 0.0485 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~1m3s | ğŸ• 23:56:49\n",
      "   ğŸ“¦ [ 66/188] |  35.1% | Loss: 0.0669 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~1m2s | ğŸ• 23:56:50\n",
      "   ğŸ“¦ [ 67/188] |  35.6% | Loss: 0.0788 | Avg: 0.0752 | Tempo: 0.56s | Restante: ~1m2s | ğŸ• 23:56:51\n",
      "   ğŸ“¦ [ 68/188] |  36.2% | Loss: 0.0609 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~1m1s | ğŸ• 23:56:52\n",
      "   ğŸ“¦ [ 69/188] |  36.7% | Loss: 0.0765 | Avg: 0.0750 | Tempo: 0.48s | Restante: ~1m1s | ğŸ• 23:56:54\n",
      "   ğŸ“¦ [ 70/188] |  37.2% | Loss: 0.0979 | Avg: 0.0754 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:56:55\n",
      "   ğŸ“¦ [ 71/188] |  37.8% | Loss: 0.0656 | Avg: 0.0752 | Tempo: 0.49s | Restante: ~1m0s | ğŸ• 23:56:56\n",
      "   ğŸ“¦ [ 72/188] |  38.3% | Loss: 0.0777 | Avg: 0.0753 | Tempo: 0.49s | Restante: ~0m59s | ğŸ• 23:56:57\n",
      "   ğŸ“¦ [ 73/188] |  38.8% | Loss: 0.0669 | Avg: 0.0752 | Tempo: 1.06s | Restante: ~0m59s | ğŸ• 23:56:58\n",
      "   ğŸ“¦ [ 74/188] |  39.4% | Loss: 0.0575 | Avg: 0.0749 | Tempo: 0.48s | Restante: ~0m59s | ğŸ• 23:56:59\n",
      "   ğŸ“¦ [ 75/188] |  39.9% | Loss: 0.0708 | Avg: 0.0749 | Tempo: 0.49s | Restante: ~0m58s | ğŸ• 23:57:00\n",
      "   ğŸ“¦ [ 76/188] |  40.4% | Loss: 0.0483 | Avg: 0.0745 | Tempo: 0.51s | Restante: ~0m58s | ğŸ• 23:57:01\n",
      "   ğŸ“¦ [ 77/188] |  41.0% | Loss: 0.0568 | Avg: 0.0743 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:57:03\n",
      "   ğŸ“¦ [ 78/188] |  41.5% | Loss: 0.0337 | Avg: 0.0738 | Tempo: 0.49s | Restante: ~0m57s | ğŸ• 23:57:04\n",
      "   ğŸ“¦ [ 79/188] |  42.0% | Loss: 0.0569 | Avg: 0.0735 | Tempo: 0.47s | Restante: ~0m56s | ğŸ• 23:57:05\n",
      "   ğŸ“¦ [ 80/188] |  42.6% | Loss: 0.0742 | Avg: 0.0736 | Tempo: 0.48s | Restante: ~0m55s | ğŸ• 23:57:06\n",
      "   ğŸ“¦ [ 81/188] |  43.1% | Loss: 0.0991 | Avg: 0.0739 | Tempo: 0.49s | Restante: ~0m55s | ğŸ• 23:57:07\n",
      "   ğŸ“¦ [ 82/188] |  43.6% | Loss: 0.0590 | Avg: 0.0737 | Tempo: 0.91s | Restante: ~0m55s | ğŸ• 23:57:08\n",
      "   ğŸ“¦ [ 83/188] |  44.1% | Loss: 0.0979 | Avg: 0.0740 | Tempo: 0.50s | Restante: ~0m54s | ğŸ• 23:57:09\n",
      "   ğŸ“¦ [ 84/188] |  44.7% | Loss: 0.0674 | Avg: 0.0739 | Tempo: 0.48s | Restante: ~0m54s | ğŸ• 23:57:10\n",
      "   ğŸ“¦ [ 85/188] |  45.2% | Loss: 0.0902 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m53s | ğŸ• 23:57:11\n",
      "   ğŸ“¦ [ 86/188] |  45.7% | Loss: 0.0388 | Avg: 0.0737 | Tempo: 0.48s | Restante: ~0m53s | ğŸ• 23:57:13\n",
      "   ğŸ“¦ [ 87/188] |  46.3% | Loss: 0.0601 | Avg: 0.0735 | Tempo: 0.49s | Restante: ~0m52s | ğŸ• 23:57:14\n",
      "   ğŸ“¦ [ 88/188] |  46.8% | Loss: 0.0816 | Avg: 0.0736 | Tempo: 0.61s | Restante: ~0m52s | ğŸ• 23:57:15\n",
      "   ğŸ“¦ [ 89/188] |  47.3% | Loss: 0.0373 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~0m51s | ğŸ• 23:57:16\n",
      "   ğŸ“¦ [ 90/188] |  47.9% | Loss: 0.0383 | Avg: 0.0728 | Tempo: 0.49s | Restante: ~0m51s | ğŸ• 23:57:17\n",
      "   ğŸ“¦ [ 91/188] |  48.4% | Loss: 0.0403 | Avg: 0.0725 | Tempo: 0.49s | Restante: ~0m50s | ğŸ• 23:57:18\n",
      "   ğŸ“¦ [ 92/188] |  48.9% | Loss: 0.0455 | Avg: 0.0722 | Tempo: 1.04s | Restante: ~0m50s | ğŸ• 23:57:19\n",
      "   ğŸ“¦ [ 93/188] |  49.5% | Loss: 0.0772 | Avg: 0.0722 | Tempo: 0.99s | Restante: ~0m50s | ğŸ• 23:57:21\n",
      "   ğŸ“¦ [ 94/188] |  50.0% | Loss: 0.0581 | Avg: 0.0721 | Tempo: 0.50s | Restante: ~0m49s | ğŸ• 23:57:22\n",
      "   ğŸ“¦ [ 95/188] |  50.5% | Loss: 0.0499 | Avg: 0.0718 | Tempo: 0.50s | Restante: ~0m49s | ğŸ• 23:57:23\n",
      "   ğŸ“¦ [ 96/188] |  51.1% | Loss: 0.0531 | Avg: 0.0716 | Tempo: 0.50s | Restante: ~0m48s | ğŸ• 23:57:24\n",
      "   ğŸ“¦ [ 97/188] |  51.6% | Loss: 0.0737 | Avg: 0.0717 | Tempo: 0.63s | Restante: ~0m48s | ğŸ• 23:57:25\n",
      "   ğŸ“¦ [ 98/188] |  52.1% | Loss: 0.0390 | Avg: 0.0713 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:57:26\n",
      "   ğŸ“¦ [ 99/188] |  52.7% | Loss: 0.0495 | Avg: 0.0711 | Tempo: 0.49s | Restante: ~0m47s | ğŸ• 23:57:27\n",
      "   ğŸ“¦ [100/188] |  53.2% | Loss: 0.0677 | Avg: 0.0711 | Tempo: 0.49s | Restante: ~0m46s | ğŸ• 23:57:29\n",
      "   ğŸ“¦ [101/188] |  53.7% | Loss: 0.0708 | Avg: 0.0711 | Tempo: 0.74s | Restante: ~0m46s | ğŸ• 23:57:30\n",
      "   ğŸ“¦ [102/188] |  54.3% | Loss: 0.1515 | Avg: 0.0719 | Tempo: 0.47s | Restante: ~0m45s | ğŸ• 23:57:31\n",
      "   ğŸ“¦ [103/188] |  54.8% | Loss: 0.0796 | Avg: 0.0719 | Tempo: 0.51s | Restante: ~0m45s | ğŸ• 23:57:32\n",
      "   ğŸ“¦ [104/188] |  55.3% | Loss: 0.0606 | Avg: 0.0718 | Tempo: 0.49s | Restante: ~0m44s | ğŸ• 23:57:33\n",
      "   ğŸ“¦ [105/188] |  55.9% | Loss: 0.0746 | Avg: 0.0719 | Tempo: 0.47s | Restante: ~0m44s | ğŸ• 23:57:34\n",
      "   ğŸ“¦ [106/188] |  56.4% | Loss: 0.1297 | Avg: 0.0724 | Tempo: 0.49s | Restante: ~0m43s | ğŸ• 23:57:35\n",
      "   ğŸ“¦ [107/188] |  56.9% | Loss: 0.0392 | Avg: 0.0721 | Tempo: 0.48s | Restante: ~0m42s | ğŸ• 23:57:36\n",
      "   ğŸ“¦ [108/188] |  57.4% | Loss: 0.0425 | Avg: 0.0718 | Tempo: 0.50s | Restante: ~0m42s | ğŸ• 23:57:38\n",
      "   ğŸ“¦ [109/188] |  58.0% | Loss: 0.0348 | Avg: 0.0715 | Tempo: 0.47s | Restante: ~0m41s | ğŸ• 23:57:39\n",
      "   ğŸ“¦ [110/188] |  58.5% | Loss: 0.0505 | Avg: 0.0713 | Tempo: 0.49s | Restante: ~0m41s | ğŸ• 23:57:40\n",
      "   ğŸ“¦ [111/188] |  59.0% | Loss: 0.0442 | Avg: 0.0710 | Tempo: 0.49s | Restante: ~0m40s | ğŸ• 23:57:41\n",
      "   ğŸ“¦ [112/188] |  59.6% | Loss: 0.1017 | Avg: 0.0713 | Tempo: 0.48s | Restante: ~0m40s | ğŸ• 23:57:42\n",
      "   ğŸ“¦ [113/188] |  60.1% | Loss: 0.0838 | Avg: 0.0714 | Tempo: 0.67s | Restante: ~0m39s | ğŸ• 23:57:44\n",
      "   ğŸ“¦ [114/188] |  60.6% | Loss: 0.0274 | Avg: 0.0710 | Tempo: 0.50s | Restante: ~0m39s | ğŸ• 23:57:45\n",
      "   ğŸ“¦ [115/188] |  61.2% | Loss: 0.1263 | Avg: 0.0715 | Tempo: 0.65s | Restante: ~0m38s | ğŸ• 23:57:48\n",
      "   ğŸ“¦ [116/188] |  61.7% | Loss: 0.1395 | Avg: 0.0721 | Tempo: 0.47s | Restante: ~0m38s | ğŸ• 23:57:49\n",
      "   ğŸ“¦ [117/188] |  62.2% | Loss: 0.2926 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m37s | ğŸ• 23:57:50\n",
      "   ğŸ“¦ [118/188] |  62.8% | Loss: 0.0865 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m36s | ğŸ• 23:57:51\n",
      "   ğŸ“¦ [119/188] |  63.3% | Loss: 0.0527 | Avg: 0.0739 | Tempo: 0.48s | Restante: ~0m36s | ğŸ• 23:57:52\n",
      "   ğŸ“¦ [120/188] |  63.8% | Loss: 0.0826 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m35s | ğŸ• 23:57:53\n",
      "   ğŸ“¦ [121/188] |  64.4% | Loss: 0.0933 | Avg: 0.0742 | Tempo: 0.64s | Restante: ~0m35s | ğŸ• 23:57:56\n",
      "   ğŸ“¦ [122/188] |  64.9% | Loss: 0.0765 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m34s | ğŸ• 23:57:57\n",
      "   ğŸ“¦ [123/188] |  65.4% | Loss: 0.0680 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m34s | ğŸ• 23:57:58\n",
      "   ğŸ“¦ [124/188] |  66.0% | Loss: 0.0386 | Avg: 0.0738 | Tempo: 0.49s | Restante: ~0m33s | ğŸ• 23:57:59\n",
      "   ğŸ“¦ [125/188] |  66.5% | Loss: 0.1383 | Avg: 0.0743 | Tempo: 0.66s | Restante: ~0m33s | ğŸ• 23:58:01\n",
      "   ğŸ“¦ [126/188] |  67.0% | Loss: 0.0597 | Avg: 0.0742 | Tempo: 0.48s | Restante: ~0m32s | ğŸ• 23:58:02\n",
      "   ğŸ“¦ [127/188] |  67.6% | Loss: 0.1967 | Avg: 0.0752 | Tempo: 0.75s | Restante: ~0m32s | ğŸ• 23:58:03\n",
      "   ğŸ“¦ [128/188] |  68.1% | Loss: 0.0392 | Avg: 0.0749 | Tempo: 0.49s | Restante: ~0m31s | ğŸ• 23:58:05\n",
      "   ğŸ“¦ [129/188] |  68.6% | Loss: 0.1014 | Avg: 0.0751 | Tempo: 0.50s | Restante: ~0m31s | ğŸ• 23:58:06\n",
      "   ğŸ“¦ [130/188] |  69.1% | Loss: 0.0379 | Avg: 0.0748 | Tempo: 0.47s | Restante: ~0m30s | ğŸ• 23:58:07\n",
      "   ğŸ“¦ [131/188] |  69.7% | Loss: 0.0945 | Avg: 0.0750 | Tempo: 0.49s | Restante: ~0m30s | ğŸ• 23:58:08\n",
      "   ğŸ“¦ [132/188] |  70.2% | Loss: 0.0574 | Avg: 0.0749 | Tempo: 0.48s | Restante: ~0m29s | ğŸ• 23:58:09\n",
      "   ğŸ“¦ [133/188] |  70.7% | Loss: 0.0538 | Avg: 0.0747 | Tempo: 0.49s | Restante: ~0m29s | ğŸ• 23:58:10\n",
      "   ğŸ“¦ [134/188] |  71.3% | Loss: 0.0475 | Avg: 0.0745 | Tempo: 0.49s | Restante: ~0m28s | ğŸ• 23:58:11\n",
      "   ğŸ“¦ [135/188] |  71.8% | Loss: 0.0593 | Avg: 0.0744 | Tempo: 0.48s | Restante: ~0m27s | ğŸ• 23:58:12\n",
      "   ğŸ“¦ [136/188] |  72.3% | Loss: 0.1042 | Avg: 0.0746 | Tempo: 0.50s | Restante: ~0m27s | ğŸ• 23:58:14\n",
      "   ğŸ“¦ [137/188] |  72.9% | Loss: 0.0792 | Avg: 0.0746 | Tempo: 0.47s | Restante: ~0m26s | ğŸ• 23:58:15\n",
      "   ğŸ“¦ [138/188] |  73.4% | Loss: 0.1029 | Avg: 0.0748 | Tempo: 0.49s | Restante: ~0m26s | ğŸ• 23:58:16\n",
      "   ğŸ“¦ [139/188] |  73.9% | Loss: 0.0395 | Avg: 0.0746 | Tempo: 0.48s | Restante: ~0m25s | ğŸ• 23:58:17\n",
      "   ğŸ“¦ [140/188] |  74.5% | Loss: 0.0553 | Avg: 0.0744 | Tempo: 0.49s | Restante: ~0m25s | ğŸ• 23:58:18\n",
      "   ğŸ“¦ [141/188] |  75.0% | Loss: 0.0486 | Avg: 0.0743 | Tempo: 0.49s | Restante: ~0m24s | ğŸ• 23:58:19\n",
      "   ğŸ“¦ [142/188] |  75.5% | Loss: 0.0737 | Avg: 0.0743 | Tempo: 2.59s | Restante: ~0m24s | ğŸ• 23:58:22\n",
      "   ğŸ“¦ [143/188] |  76.1% | Loss: 0.0588 | Avg: 0.0741 | Tempo: 0.89s | Restante: ~0m24s | ğŸ• 23:58:23\n",
      "   ğŸ“¦ [144/188] |  76.6% | Loss: 0.0476 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m23s | ğŸ• 23:58:24\n",
      "   ğŸ“¦ [145/188] |  77.1% | Loss: 0.0773 | Avg: 0.0740 | Tempo: 0.58s | Restante: ~0m23s | ğŸ• 23:58:25\n",
      "   ğŸ“¦ [146/188] |  77.7% | Loss: 0.0290 | Avg: 0.0737 | Tempo: 0.48s | Restante: ~0m22s | ğŸ• 23:58:27\n",
      "   ğŸ“¦ [147/188] |  78.2% | Loss: 0.0723 | Avg: 0.0737 | Tempo: 0.49s | Restante: ~0m22s | ğŸ• 23:58:28\n",
      "   ğŸ“¦ [148/188] |  78.7% | Loss: 0.1080 | Avg: 0.0739 | Tempo: 0.49s | Restante: ~0m21s | ğŸ• 23:58:29\n",
      "   ğŸ“¦ [149/188] |  79.3% | Loss: 0.0833 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m21s | ğŸ• 23:58:30\n",
      "   ğŸ“¦ [150/188] |  79.8% | Loss: 0.1147 | Avg: 0.0742 | Tempo: 0.49s | Restante: ~0m20s | ğŸ• 23:58:31\n",
      "   ğŸ“¦ [151/188] |  80.3% | Loss: 0.0823 | Avg: 0.0743 | Tempo: 0.50s | Restante: ~0m19s | ğŸ• 23:58:32\n",
      "   ğŸ“¦ [152/188] |  80.9% | Loss: 0.0697 | Avg: 0.0743 | Tempo: 0.48s | Restante: ~0m19s | ğŸ• 23:58:33\n",
      "   ğŸ“¦ [153/188] |  81.4% | Loss: 0.0456 | Avg: 0.0741 | Tempo: 0.49s | Restante: ~0m18s | ğŸ• 23:58:34\n",
      "   ğŸ“¦ [154/188] |  81.9% | Loss: 0.0558 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m18s | ğŸ• 23:58:36\n",
      "   ğŸ“¦ [155/188] |  82.4% | Loss: 0.0634 | Avg: 0.0739 | Tempo: 0.55s | Restante: ~0m17s | ğŸ• 23:58:37\n",
      "   ğŸ“¦ [156/188] |  83.0% | Loss: 0.0663 | Avg: 0.0738 | Tempo: 0.48s | Restante: ~0m17s | ğŸ• 23:58:39\n",
      "   ğŸ“¦ [157/188] |  83.5% | Loss: 0.0462 | Avg: 0.0737 | Tempo: 0.73s | Restante: ~0m16s | ğŸ• 23:58:40\n",
      "   ğŸ“¦ [158/188] |  84.0% | Loss: 0.0522 | Avg: 0.0735 | Tempo: 0.48s | Restante: ~0m16s | ğŸ• 23:58:41\n",
      "   ğŸ“¦ [159/188] |  84.6% | Loss: 0.0683 | Avg: 0.0735 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:58:42\n",
      "   ğŸ“¦ [160/188] |  85.1% | Loss: 0.0538 | Avg: 0.0734 | Tempo: 0.48s | Restante: ~0m15s | ğŸ• 23:58:43\n",
      "   ğŸ“¦ [161/188] |  85.6% | Loss: 0.0820 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:58:44\n",
      "   ğŸ“¦ [162/188] |  86.2% | Loss: 0.0802 | Avg: 0.0735 | Tempo: 0.49s | Restante: ~0m14s | ğŸ• 23:58:45\n",
      "   ğŸ“¦ [163/188] |  86.7% | Loss: 0.1021 | Avg: 0.0736 | Tempo: 0.49s | Restante: ~0m13s | ğŸ• 23:58:46\n",
      "   ğŸ“¦ [164/188] |  87.2% | Loss: 0.0393 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:58:47\n",
      "   ğŸ“¦ [165/188] |  87.8% | Loss: 0.1017 | Avg: 0.0736 | Tempo: 0.49s | Restante: ~0m12s | ğŸ• 23:58:49\n",
      "   ğŸ“¦ [166/188] |  88.3% | Loss: 0.0596 | Avg: 0.0735 | Tempo: 0.55s | Restante: ~0m11s | ğŸ• 23:58:50\n",
      "   ğŸ“¦ [167/188] |  88.8% | Loss: 0.0571 | Avg: 0.0734 | Tempo: 0.49s | Restante: ~0m11s | ğŸ• 23:58:51\n",
      "   ğŸ“¦ [168/188] |  89.4% | Loss: 0.0569 | Avg: 0.0733 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:58:52\n",
      "   ğŸ“¦ [169/188] |  89.9% | Loss: 0.0523 | Avg: 0.0732 | Tempo: 0.49s | Restante: ~0m10s | ğŸ• 23:58:53\n",
      "   ğŸ“¦ [170/188] |  90.4% | Loss: 0.0754 | Avg: 0.0732 | Tempo: 0.48s | Restante: ~0m9s | ğŸ• 23:58:54\n",
      "   ğŸ“¦ [171/188] |  91.0% | Loss: 0.0776 | Avg: 0.0732 | Tempo: 0.47s | Restante: ~0m9s | ğŸ• 23:58:55\n",
      "   ğŸ“¦ [172/188] |  91.5% | Loss: 0.0616 | Avg: 0.0732 | Tempo: 0.49s | Restante: ~0m8s | ğŸ• 23:58:56\n",
      "   ğŸ“¦ [173/188] |  92.0% | Loss: 0.1533 | Avg: 0.0736 | Tempo: 0.47s | Restante: ~0m8s | ğŸ• 23:58:58\n",
      "   ğŸ“¦ [174/188] |  92.6% | Loss: 0.0905 | Avg: 0.0737 | Tempo: 0.50s | Restante: ~0m7s | ğŸ• 23:58:59\n",
      "   ğŸ“¦ [175/188] |  93.1% | Loss: 0.0911 | Avg: 0.0738 | Tempo: 0.48s | Restante: ~0m6s | ğŸ• 23:59:00\n",
      "   ğŸ“¦ [176/188] |  93.6% | Loss: 0.1112 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m6s | ğŸ• 23:59:01\n",
      "   ğŸ“¦ [177/188] |  94.1% | Loss: 0.1213 | Avg: 0.0743 | Tempo: 0.49s | Restante: ~0m5s | ğŸ• 23:59:02\n",
      "   ğŸ“¦ [178/188] |  94.7% | Loss: 0.0464 | Avg: 0.0741 | Tempo: 0.48s | Restante: ~0m5s | ğŸ• 23:59:03\n",
      "   ğŸ“¦ [179/188] |  95.2% | Loss: 0.0472 | Avg: 0.0740 | Tempo: 0.48s | Restante: ~0m4s | ğŸ• 23:59:04\n",
      "   ğŸ“¦ [180/188] |  95.7% | Loss: 0.0743 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m4s | ğŸ• 23:59:05\n",
      "   ğŸ“¦ [181/188] |  96.3% | Loss: 0.0852 | Avg: 0.0741 | Tempo: 0.48s | Restante: ~0m3s | ğŸ• 23:59:07\n",
      "   ğŸ“¦ [182/188] |  96.8% | Loss: 0.1301 | Avg: 0.0744 | Tempo: 0.47s | Restante: ~0m3s | ğŸ• 23:59:08\n",
      "   ğŸ“¦ [183/188] |  97.3% | Loss: 0.0560 | Avg: 0.0743 | Tempo: 0.48s | Restante: ~0m2s | ğŸ• 23:59:09\n",
      "   ğŸ“¦ [184/188] |  97.9% | Loss: 0.0591 | Avg: 0.0742 | Tempo: 0.53s | Restante: ~0m2s | ğŸ• 23:59:10\n",
      "   ğŸ“¦ [185/188] |  98.4% | Loss: 0.0493 | Avg: 0.0741 | Tempo: 0.50s | Restante: ~0m1s | ğŸ• 23:59:11\n",
      "   ğŸ“¦ [186/188] |  98.9% | Loss: 0.0596 | Avg: 0.0740 | Tempo: 0.49s | Restante: ~0m1s | ğŸ• 23:59:12\n",
      "   ğŸ“¦ [187/188] |  99.5% | Loss: 0.0743 | Avg: 0.0740 | Tempo: 0.47s | Restante: ~0m0s | ğŸ• 23:59:13\n",
      "   ğŸ“¦ [188/188] | 100.0% | Loss: 0.0509 | Avg: 0.0739 | Tempo: 0.26s | Restante: ~0m0s | ğŸ• 23:59:14\n",
      "\n",
      "âœ… Ã‰poca 10 concluÃ­da!\n",
      "   ğŸ“Š Loss Total: 0.0739\n",
      "   ğŸ“Š Loss Classifier: 0.0255\n",
      "   ğŸ“Š Loss Box Reg: 0.0432\n",
      "   ğŸ“Š Loss Objectness: 0.0020\n",
      "   ğŸ“Š Loss RPN Box Reg: 0.0032\n",
      "   â±ï¸  Tempo total: 3m43s\n",
      "   â±ï¸  Tempo mÃ©dio por batch: 0.53s\n",
      "ğŸ”µ === FIM Ã‰POCA 10/10, Loss: 0.0739 ===\n",
      "\n",
      "   ğŸ’¾ Melhor modelo salvo! (Loss: 0.0739)\n",
      "      ğŸ“‚ Caminho: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ’¾ Checkpoint salvo: runs/aula9_coco_gun\\checkpoint_epoch_10.pth\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… TREINAMENTO CONCLUÃDO!\n",
      "============================================================\n",
      "   ğŸ“Š Total de Ã©pocas treinadas: 10\n",
      "   ğŸ“Š Melhor loss: 0.0739\n",
      "   ğŸ“‚ Modelo salvo em: runs/aula9_coco_gun\\best_model.pth\n",
      "   ğŸ“‚ HistÃ³rico salvo em: runs/aula9_coco_gun\\history.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# LOOP DE TREINAMENTO\n",
    "# ================================================================\n",
    "\n",
    "# Criar diretÃ³rio para salvar modelo\n",
    "save_dir = \"runs/aula9_coco_gun\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# HistÃ³rico de treinamento\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"ar_all_100\": []\n",
    "}\n",
    "\n",
    "# Melhor mAP (usaremos mAP@0.5:0.95 como critÃ©rio)\n",
    "best_map = 0.0\n",
    "\n",
    "print(f\"\\n{'='*60}\", flush=True)\n",
    "print(f\"ğŸ¯ INICIANDO TREINAMENTO\", flush=True)\n",
    "print(f\"{'='*60}\", flush=True)\n",
    "print(f\"   ğŸ“‚ DiretÃ³rio de salvamento: {save_dir}\", flush=True)\n",
    "print(f\"   ğŸ“Š Total de Ã©pocas: {NUM_EPOCHS}\", flush=True)\n",
    "print(f\"   ğŸ“¸ Imagens por Ã©poca: {len(train_dataset)}\", flush=True)\n",
    "print(f\"   ğŸ“¦ Batches por Ã©poca: {len(train_loader)}\", flush=True)\n",
    "print(f\"{'='*60}\\n\", flush=True)\n",
    "\n",
    "# Loop de treinamento\n",
    "print(f\"ğŸ”µ Iniciando loop de treinamento...\", flush=True)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nğŸ”µ === INÃCIO Ã‰POCA {epoch+1}/{NUM_EPOCHS} ===\", flush=True)\n",
    "    # Treinar uma Ã©poca\n",
    "    avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, NUM_EPOCHS)\n",
    "    print(f\"ğŸ”µ === FIM Ã‰POCA {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f} ===\", flush=True)\n",
    "\n",
    "    # Atualizar learning rate\n",
    "    lr_scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # ================================================================\n",
    "    # AVALIAÃ‡ÃƒO COCO NO DATASET VAL APÃ“S CADA Ã‰POCA\n",
    "    # ================================================================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“Š AVALIANDO MODELO NO DATASET VAL - Ã‰POCA {epoch+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Avaliar no dataset de VALIDAÃ‡ÃƒO\n",
    "    ann_path = os.path.join(root_dir, \"coco_annotations_val.json\")\n",
    "    metrics = evaluate_coco(\n",
    "        model,\n",
    "        val_dataset,\n",
    "        ann_path,\n",
    "        device=device,\n",
    "        score_threshold=0.3,\n",
    "        valid_indices=valid_indices\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š MÃ‰TRICAS DA Ã‰POCA {epoch+1} (VAL):\")\n",
    "    print(f\"   âœ… mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "    print(f\"   âœ… mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "    print(f\"   âœ… mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "    print(f\"   âœ… AR@100:       {metrics['ar_all_100']:.4f} ({metrics['ar_all_100']*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Salvar histÃ³rico\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    history[\"map50_95\"].append(metrics['map50_95'])\n",
    "    history[\"map50\"].append(metrics['map50'])\n",
    "    history[\"map75\"].append(metrics['map75'])\n",
    "    history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "    \n",
    "    # Salvar melhor modelo (baseado em mAP@0.5:0.95)\n",
    "    current_map = metrics['map50_95']\n",
    "    if current_map > best_map:\n",
    "        best_map = current_map\n",
    "        model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"\\n   ğŸ’¾ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "        print(f\"      ğŸ“‚ Caminho: {model_path}\")\n",
    "    \n",
    "    # Salvar checkpoint\n",
    "    checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "        'history': history\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"   ğŸ’¾ Checkpoint salvo: {checkpoint_path}\")\n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Salvar histÃ³rico final\n",
    "history_path = os.path.join(save_dir, \"history.json\")\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… TREINAMENTO CONCLUÃDO!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   ğŸ“Š Total de Ã©pocas treinadas: {NUM_EPOCHS}\")\n",
    "print(f\"   ğŸ“Š Melhor mAP@0.5:0.95: {best_map:.4f}\")\n",
    "print(f\"   ğŸ“‚ Modelo salvo em: {os.path.join(save_dir, 'best_model.pth')}\")\n",
    "print(f\"   ğŸ“‚ HistÃ³rico salvo em: {history_path}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GrÃ¡ficos salvos em: runs/aula9_coco_gun\\training_history.png\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# VISUALIZAR HISTÃ“RICO DE TREINAMENTO\n",
    "# ================================================================\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, \"history.json\")):\n",
    "    with open(os.path.join(save_dir, \"history.json\"), 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "        # Criar figura com mÃºltiplos subplots\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train Loss', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Learning Rate\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history[\"epoch\"], history[\"lr\"], 'r-', linewidth=2, label='Learning Rate', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: mAP principais\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=2, label='mAP@0.5:0.95', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=2, label='mAP@0.5', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"map75\"], 'r-', linewidth=2, label='mAP@0.75', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.title('Average Precision (AP) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: mAP@0.5:0.95 (principal)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=3, label='mAP@0.5:0.95', marker='o', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP@0.5:0.95')\n",
    "    plt.title('mAP@0.5:0.95 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 5: mAP@0.5\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=3, label='mAP@0.5', marker='s', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP@0.5')\n",
    "    plt.title('mAP@0.5 ao Longo do Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 6: AR@100\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_100\"], 'm-', linewidth=3, label='AR@100', marker='d', markersize=8)\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('AR@100')\n",
    "    plt.title('Average Recall (AR@100) - VAL')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_history.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"âœ… GrÃ¡ficos salvos em: {os.path.join(save_dir, 'training_history.png')}\")\n",
    "else:\n",
    "    print(\"âš ï¸  HistÃ³rico nÃ£o encontrado. Execute o treinamento primeiro.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ MODIFICAÃ‡Ã•ES NECESSÃRIAS\n",
    "\n",
    "**Para adicionar mÃ©tricas durante o treinamento:**\n",
    "\n",
    "1. **Modificar funÃ§Ã£o `evaluate_coco` (cÃ©lula 14)**: Alterar o retorno para retornar um dicionÃ¡rio com todas as mÃ©tricas\n",
    "2. **Modificar loop de treinamento (cÃ©lula 9)**: Adicionar avaliaÃ§Ã£o apÃ³s cada Ã©poca\n",
    "3. **Modificar histÃ³rico (cÃ©lula 9)**: Adicionar todas as mÃ©tricas ao histÃ³rico\n",
    "4. **Modificar grÃ¡ficos (cÃ©lula 12)**: Adicionar grÃ¡ficos para todas as mÃ©tricas\n",
    "\n",
    "Veja as prÃ³ximas cÃ©lulas para o cÃ³digo completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FUNÃ‡ÃƒO evaluate_coco MODIFICADA - RETORNA TODAS AS MÃ‰TRICAS\n",
    "# ================================================================\n",
    "# SUBSTITUA A FUNÃ‡ÃƒO evaluate_coco NA CÃ‰LULA 14 POR ESTA VERSÃƒO\n",
    "\n",
    "def evaluate_coco_completo(model, dataset, ann_path, device=\"cuda\", score_threshold=0.3, valid_indices=None):\n",
    "    \"\"\"\n",
    "    Avalia modelo usando mÃ©tricas COCO - RETORNA TODAS AS MÃ‰TRICAS.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de detecÃ§Ã£o\n",
    "        dataset: Dataset de validaÃ§Ã£o\n",
    "        ann_path: Caminho para anotaÃ§Ãµes COCO\n",
    "        device: Device (cuda/cpu)\n",
    "        score_threshold: Score mÃ­nimo para incluir prediÃ§Ã£o\n",
    "        valid_indices: Lista de Ã­ndices vÃ¡lidos do dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” INÃCIO DA AVALIAÃ‡ÃƒO COCO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ğŸ“‚ Arquivo de anotaÃ§Ãµes: {ann_path}\")\n",
    "    print(f\"   ğŸ¯ Score threshold: {score_threshold}\")\n",
    "    print(f\"   ğŸ’» Device: {device}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nğŸ“‹ Carregando anotaÃ§Ãµes COCO...\")\n",
    "    coco_gt = COCO(ann_path)\n",
    "    \n",
    "    # Garantir que o dataset tenha a chave 'info' necessÃ¡ria para loadRes\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        print(f\"   âš ï¸  Adicionando chave 'info' ao dataset\")\n",
    "        coco_gt.dataset['info'] = {\n",
    "            \"description\": \"COCO Dataset - Gun Detection\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   âœ… Chave 'info' jÃ¡ existe no dataset\")\n",
    "    \n",
    "    # Obter category_id correto (deve ser 1 para gun)\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    print(f\"   ğŸ“Š Categorias encontradas: {cat_ids}\")\n",
    "    expected_category_id = cat_ids[0] if len(cat_ids) > 0 else 1\n",
    "    print(f\"   ğŸ“‹ Category ID esperado: {expected_category_id} (gun)\")\n",
    "    \n",
    "    # Coletar image_ids do subset que serÃ¡ avaliado\n",
    "    subset_image_ids = []\n",
    "    results = []\n",
    "    processed_images = 0\n",
    "    \n",
    "    # Usar apenas Ã­ndices vÃ¡lidos se especificado\n",
    "    indices_to_use = valid_indices if valid_indices is not None else list(range(len(dataset)))\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Processando {len(indices_to_use)} imagens...\")\n",
    "    \n",
    "    for idx in indices_to_use:\n",
    "        sample = dataset[idx]\n",
    "        if sample is None:\n",
    "            continue\n",
    "            \n",
    "        img, target = sample\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "        image_id = int(target[\"image_id\"].item())\n",
    "        subset_image_ids.append(image_id)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        boxes = output[\"boxes\"].cpu().numpy()\n",
    "        scores = output[\"scores\"].cpu().numpy()\n",
    "        labels = output[\"labels\"].cpu().numpy()\n",
    "\n",
    "        # Filtrar por score threshold\n",
    "        mask = scores >= score_threshold\n",
    "        boxes = boxes[mask]\n",
    "        scores = scores[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            # Converter para formato COCO [x, y, width, height]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            # Garantir valores vÃ¡lidos\n",
    "            if w > 0 and h > 0:\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": expected_category_id,\n",
    "                    \"bbox\": [x1, y1, w, h],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "        \n",
    "        processed_images += 1\n",
    "        if processed_images % 10 == 0:\n",
    "            print(f\"   â³ Processadas {processed_images}/{len(indices_to_use)} imagens...\")\n",
    "\n",
    "    print(f\"\\nâœ… Processamento concluÃ­do!\")\n",
    "    print(f\"   ğŸ“Š Total de prediÃ§Ãµes: {len(results)}\")\n",
    "    print(f\"   ğŸ“¸ Imagens processadas: {processed_images}\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"âš ï¸  Nenhuma prediÃ§Ã£o gerada!\")\n",
    "        return {\n",
    "            'map50_95': 0.0, 'map50': 0.0, 'map75': 0.0,\n",
    "            'map50_95_small': 0.0, 'map50_95_medium': 0.0, 'map50_95_large': 0.0,\n",
    "            'ar_all_1': 0.0, 'ar_all_10': 0.0, 'ar_all_100': 0.0,\n",
    "            'ar_small': 0.0, 'ar_medium': 0.0, 'ar_large': 0.0\n",
    "        }\n",
    "\n",
    "    # Filtrar COCO GT para usar apenas as imagens do subset\n",
    "    print(f\"\\nğŸ“‹ Filtrando anotaÃ§Ãµes COCO para o subset...\")\n",
    "    print(f\"   ğŸ“¸ Image IDs no subset: {len(subset_image_ids)}\")\n",
    "    \n",
    "    # Avaliar com COCOeval\n",
    "    print(f\"\\nğŸ“Š Calculando mÃ©tricas COCO...\")\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    \n",
    "    # Filtrar para usar apenas as imagens do subset\n",
    "    coco_eval.params.imgIds = subset_image_ids\n",
    "    \n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # Extrair todas as mÃ©tricas COCO\n",
    "    metrics = {\n",
    "        'map50_95': coco_eval.stats[0],  # mAP@0.5:0.95\n",
    "        'map50': coco_eval.stats[1],     # mAP@0.5\n",
    "        'map75': coco_eval.stats[2],     # mAP@0.75\n",
    "        'map50_95_small': coco_eval.stats[3],\n",
    "        'map50_95_medium': coco_eval.stats[4],\n",
    "        'map50_95_large': coco_eval.stats[5],\n",
    "        'ar_all_1': coco_eval.stats[6],\n",
    "        'ar_all_10': coco_eval.stats[7],\n",
    "        'ar_all_100': coco_eval.stats[8],\n",
    "        'ar_small': coco_eval.stats[9],\n",
    "        'ar_medium': coco_eval.stats[10],\n",
    "        'ar_large': coco_eval.stats[11]\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"âœ… FunÃ§Ã£o evaluate_coco_completo criada!\")\n",
    "print(\"   ğŸ“ Copie esta funÃ§Ã£o para substituir a funÃ§Ã£o evaluate_coco na cÃ©lula 14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CÃ“DIGO MODIFICADO PARA O LOOP DE TREINAMENTO\n",
    "# ================================================================\n",
    "# SUBSTITUA O HISTÃ“RICO E O LOOP DE TREINAMENTO NA CÃ‰LULA 9 POR ESTE CÃ“DIGO\n",
    "\n",
    "# HistÃ³rico de treinamento (MODIFICADO - adicionar todas as mÃ©tricas)\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"lr\": [],\n",
    "    \"map50_95\": [],\n",
    "    \"map50\": [],\n",
    "    \"map75\": [],\n",
    "    \"map50_95_small\": [],\n",
    "    \"map50_95_medium\": [],\n",
    "    \"map50_95_large\": [],\n",
    "    \"ar_all_1\": [],\n",
    "    \"ar_all_10\": [],\n",
    "    \"ar_all_100\": [],\n",
    "    \"ar_small\": [],\n",
    "    \"ar_medium\": [],\n",
    "    \"ar_large\": []\n",
    "}\n",
    "\n",
    "# Melhor mAP (usaremos mAP@0.5:0.95 como critÃ©rio)\n",
    "best_map = 0.0\n",
    "\n",
    "print(\"âœ… HistÃ³rico modificado para incluir todas as mÃ©tricas COCO\")\n",
    "print(\"   ğŸ“ Copie este cÃ³digo para substituir o histÃ³rico na cÃ©lula 9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CÃ“DIGO PARA ADICIONAR NO LOOP DE TREINAMENTO (CÃ‰LULA 9)\n",
    "# ================================================================\n",
    "# ADICIONE ESTE CÃ“DIGO APÃ“S A LINHA \"current_lr = optimizer.param_groups[0]['lr']\"\n",
    "\n",
    "# ================================================================\n",
    "# AVALIAÃ‡ÃƒO COCO APÃ“S CADA Ã‰POCA\n",
    "# ================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š AVALIANDO MODELO APÃ“S Ã‰POCA {epoch+1}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Avaliar no dataset de teste\n",
    "ann_path = os.path.join(root_dir, \"coco_annotations_test.json\")\n",
    "metrics = evaluate_coco_completo(  # Use evaluate_coco_completo se substituiu a funÃ§Ã£o\n",
    "    model,\n",
    "    test_dataset,\n",
    "    ann_path,\n",
    "    device=device,\n",
    "    score_threshold=0.3,\n",
    "    valid_indices=valid_indices\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š MÃ‰TRICAS DA Ã‰POCA {epoch+1}:\")\n",
    "print(f\"   âœ… mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "print(f\"   âœ… mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "print(f\"   âœ… mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Salvar histÃ³rico (ADICIONAR TODAS AS MÃ‰TRICAS)\n",
    "history[\"epoch\"].append(epoch + 1)\n",
    "history[\"train_loss\"].append(avg_loss)\n",
    "history[\"lr\"].append(current_lr)\n",
    "history[\"map50_95\"].append(metrics['map50_95'])\n",
    "history[\"map50\"].append(metrics['map50'])\n",
    "history[\"map75\"].append(metrics['map75'])\n",
    "history[\"map50_95_small\"].append(metrics['map50_95_small'])\n",
    "history[\"map50_95_medium\"].append(metrics['map50_95_medium'])\n",
    "history[\"map50_95_large\"].append(metrics['map50_95_large'])\n",
    "history[\"ar_all_1\"].append(metrics['ar_all_1'])\n",
    "history[\"ar_all_10\"].append(metrics['ar_all_10'])\n",
    "history[\"ar_all_100\"].append(metrics['ar_all_100'])\n",
    "history[\"ar_small\"].append(metrics['ar_small'])\n",
    "history[\"ar_medium\"].append(metrics['ar_medium'])\n",
    "history[\"ar_large\"].append(metrics['ar_large'])\n",
    "\n",
    "# Salvar melhor modelo (baseado em mAP@0.5:0.95)\n",
    "current_map = metrics['map50_95']\n",
    "if current_map > best_map:\n",
    "    best_map = current_map\n",
    "    model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\n   ğŸ’¾ Melhor modelo salvo! (mAP@0.5:0.95: {best_map:.4f})\")\n",
    "    print(f\"      ğŸ“‚ Caminho: {model_path}\")\n",
    "\n",
    "print(\"âœ… CÃ³digo de avaliaÃ§Ã£o durante treinamento criado!\")\n",
    "print(\"   ğŸ“ Adicione este cÃ³digo no loop de treinamento (cÃ©lula 9) apÃ³s atualizar o learning rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# GRÃFICOS COMPLETOS COM TODAS AS MÃ‰TRICAS\n",
    "# ================================================================\n",
    "# SUBSTITUA O CÃ“DIGO DE GRÃFICOS NA CÃ‰LULA 12 POR ESTE\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, \"history.json\")):\n",
    "    with open(os.path.join(save_dir, \"history.json\"), 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Criar figura com mÃºltiplos subplots\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train Loss', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Learning Rate\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history[\"epoch\"], history[\"lr\"], 'r-', linewidth=2, label='Learning Rate', marker='o')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: mAP principais\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95\"], 'g-', linewidth=2, label='mAP@0.5:0.95', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50\"], 'b-', linewidth=2, label='mAP@0.5', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"map75\"], 'r-', linewidth=2, label='mAP@0.75', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.title('Average Precision (AP)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: mAP por tamanho\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95_small\"], 'b-', linewidth=2, label='Small', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95_medium\"], 'g-', linewidth=2, label='Medium', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"map50_95_large\"], 'r-', linewidth=2, label='Large', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('mAP@0.5:0.95')\n",
    "    plt.title('mAP por Tamanho de Objeto')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 5: AR principais\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_1\"], 'b-', linewidth=2, label='AR@maxDets=1', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_10\"], 'g-', linewidth=2, label='AR@maxDets=10', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_all_100\"], 'r-', linewidth=2, label='AR@maxDets=100', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('AR')\n",
    "    plt.title('Average Recall (AR)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 6: AR por tamanho\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_small\"], 'b-', linewidth=2, label='Small', marker='o')\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_medium\"], 'g-', linewidth=2, label='Medium', marker='s')\n",
    "    plt.plot(history[\"epoch\"], history[\"ar_large\"], 'r-', linewidth=2, label='Large', marker='^')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('AR@0.5:0.95')\n",
    "    plt.title('AR por Tamanho de Objeto')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_history.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… GrÃ¡ficos salvos em: {os.path.join(save_dir, 'training_history.png')}\")\n",
    "else:\n",
    "    print(\"âš ï¸  HistÃ³rico nÃ£o encontrado. Execute o treinamento primeiro.\")\n",
    "\n",
    "print(\"\\nâœ… CÃ³digo de grÃ¡ficos completo criado!\")\n",
    "print(\"   ğŸ“ Substitua o cÃ³digo de grÃ¡ficos na cÃ©lula 12 por este cÃ³digo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ RESUMO DAS MODIFICAÃ‡Ã•ES\n",
    "\n",
    "### Passo 1: Modificar funÃ§Ã£o `evaluate_coco` (CÃ©lula 14)\n",
    "- Substitua a funÃ§Ã£o `evaluate_coco` pelo cÃ³digo da **cÃ©lula 15** (`evaluate_coco_completo`)\n",
    "- A funÃ§Ã£o agora retorna um dicionÃ¡rio com todas as mÃ©tricas COCO\n",
    "\n",
    "### Passo 2: Modificar histÃ³rico e loop de treinamento (CÃ©lula 9)\n",
    "1. **Substitua o histÃ³rico** pelo cÃ³digo da **cÃ©lula 16**\n",
    "2. **Adicione o cÃ³digo de avaliaÃ§Ã£o** da **cÃ©lula 17** apÃ³s `current_lr = optimizer.param_groups[0]['lr']`\n",
    "3. **Altere** `best_loss = float('inf')` para `best_map = 0.0`\n",
    "4. **Modifique** a condiÃ§Ã£o de salvar melhor modelo para usar `best_map` e `current_map`\n",
    "\n",
    "### Passo 3: Modificar grÃ¡ficos (CÃ©lula 12)\n",
    "- Substitua o cÃ³digo de grÃ¡ficos pelo cÃ³digo da **cÃ©lula 18**\n",
    "- Agora vocÃª terÃ¡ 6 grÃ¡ficos mostrando todas as mÃ©tricas\n",
    "\n",
    "### Resultado Final:\n",
    "- âœ… MÃ©tricas COCO serÃ£o calculadas apÃ³s cada Ã©poca\n",
    "- âœ… Todas as mÃ©tricas serÃ£o salvas no histÃ³rico\n",
    "- âœ… GrÃ¡ficos completos com todas as mÃ©tricas (mAP e AR)\n",
    "- âœ… Melhor modelo salvo baseado em mAP@0.5:0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando melhor modelo: runs/aula9_coco_gun\\best_model.pth\n",
      "âœ… Modelo carregado com sucesso!\n",
      "âœ… Modelo em modo eval() para avaliaÃ§Ã£o\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CARREGAR MELHOR MODELO PARA AVALIAÃ‡ÃƒO\n",
    "# ================================================================\n",
    "\n",
    "# Carregar o melhor modelo salvo\n",
    "model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"ğŸ“‚ Carregando melhor modelo: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))\n",
    "    print(\"âœ… Modelo carregado com sucesso!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Modelo nÃ£o encontrado em {model_path}\")\n",
    "    print(\"   Usando modelo atual (Ãºltima Ã©poca)\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"âœ… Modelo em modo eval() para avaliaÃ§Ã£o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coco(model, dataset, ann_path, device=\"cuda\", score_threshold=0.3, valid_indices=None):\n",
    "    \"\"\"\n",
    "    Avalia modelo usando mÃ©tricas COCO.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de detecÃ§Ã£o\n",
    "        dataset: Dataset de validaÃ§Ã£o\n",
    "        ann_path: Caminho para anotaÃ§Ãµes COCO\n",
    "        device: Device (cuda/cpu)\n",
    "        score_threshold: Score mÃ­nimo para incluir prediÃ§Ã£o\n",
    "        valid_indices: Lista de Ã­ndices vÃ¡lidos do dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” INÃCIO DA AVALIAÃ‡ÃƒO COCO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ğŸ“‚ Arquivo de anotaÃ§Ãµes: {ann_path}\")\n",
    "    print(f\"   ğŸ¯ Score threshold: {score_threshold}\")\n",
    "    print(f\"   ğŸ’» Device: {device}\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\nğŸ“‹ Carregando anotaÃ§Ãµes COCO...\")\n",
    "    coco_gt = COCO(ann_path)\n",
    "    \n",
    "    # Garantir que o dataset tenha a chave 'info' necessÃ¡ria para loadRes\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        print(f\"   âš ï¸  Adicionando chave 'info' ao dataset\")\n",
    "        coco_gt.dataset['info'] = {\n",
    "            \"description\": \"COCO Dataset - Gun Detection\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2024\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   âœ… Chave 'info' jÃ¡ existe no dataset\")\n",
    "    \n",
    "    # Obter category_id correto (deve ser 1 para gun)\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    print(f\"   ğŸ“Š Categorias encontradas: {cat_ids}\")\n",
    "    expected_category_id = cat_ids[0] if len(cat_ids) > 0 else 1\n",
    "    print(f\"   ğŸ“‹ Category ID esperado: {expected_category_id} (gun)\")\n",
    "    \n",
    "    # Coletar image_ids do subset que serÃ¡ avaliado\n",
    "    subset_image_ids = []\n",
    "    results = []\n",
    "    processed_images = 0\n",
    "    \n",
    "    # Usar apenas Ã­ndices vÃ¡lidos se especificado\n",
    "    indices_to_use = valid_indices if valid_indices is not None else list(range(len(dataset)))\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Processando {len(indices_to_use)} imagens...\")\n",
    "    \n",
    "    for idx in indices_to_use:\n",
    "        sample = dataset[idx]\n",
    "        if sample is None:\n",
    "            continue\n",
    "            \n",
    "        img, target = sample\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "        image_id = int(target[\"image_id\"].item())\n",
    "        subset_image_ids.append(image_id)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        boxes = output[\"boxes\"].cpu().numpy()\n",
    "        scores = output[\"scores\"].cpu().numpy()\n",
    "        labels = output[\"labels\"].cpu().numpy()\n",
    "\n",
    "        # Filtrar por score threshold\n",
    "        mask = scores >= score_threshold\n",
    "        boxes = boxes[mask]\n",
    "        scores = scores[mask]\n",
    "        labels = labels[mask]\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            # Converter para formato COCO [x, y, width, height]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            # Garantir valores vÃ¡lidos\n",
    "            if w > 0 and h > 0:\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": expected_category_id,  # Sempre usar category_id do dataset\n",
    "                    \"bbox\": [x1, y1, w, h],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "        \n",
    "        processed_images += 1\n",
    "        if processed_images % 10 == 0:\n",
    "            print(f\"   â³ Processadas {processed_images}/{len(indices_to_use)} imagens...\")\n",
    "\n",
    "    print(f\"\\nâœ… Processamento concluÃ­do!\")\n",
    "    print(f\"   ğŸ“Š Total de prediÃ§Ãµes: {len(results)}\")\n",
    "    print(f\"   ğŸ“¸ Imagens processadas: {processed_images}\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"âš ï¸  Nenhuma prediÃ§Ã£o gerada!\")\n",
    "        return {\n",
    "            'map50_95': 0.0, 'map50': 0.0, 'map75': 0.0,\n",
    "            'map50_95_small': 0.0, 'map50_95_medium': 0.0, 'map50_95_large': 0.0,\n",
    "            'ar_all_1': 0.0, 'ar_all_10': 0.0, 'ar_all_100': 0.0,\n",
    "            'ar_small': 0.0, 'ar_medium': 0.0, 'ar_large': 0.0\n",
    "        }\n",
    "\n",
    "    # Filtrar COCO GT para usar apenas as imagens do subset\n",
    "    print(f\"\\nğŸ“‹ Filtrando anotaÃ§Ãµes COCO para o subset...\")\n",
    "    print(f\"   ğŸ“¸ Image IDs no subset: {len(subset_image_ids)}\")\n",
    "    \n",
    "    # Avaliar com COCOeval\n",
    "    print(f\"\\nğŸ“Š Calculando mÃ©tricas COCO...\")\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    \n",
    "    # Filtrar para usar apenas as imagens do subset\n",
    "    coco_eval.params.imgIds = subset_image_ids\n",
    "    \n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # Extrair todas as mÃ©tricas COCO\n",
    "    # stats[0]: AP @ IoU=0.50:0.95 area=all maxDets=100\n",
    "    # stats[1]: AP @ IoU=0.50 area=all maxDets=100\n",
    "    # stats[2]: AP @ IoU=0.75 area=all maxDets=100\n",
    "    # stats[3]: AP @ IoU=0.50:0.95 area=small maxDets=100\n",
    "    # stats[4]: AP @ IoU=0.50:0.95 area=medium maxDets=100\n",
    "    # stats[5]: AP @ IoU=0.50:0.95 area=large maxDets=100\n",
    "    # stats[6]: AR @ IoU=0.50:0.95 area=all maxDets=1\n",
    "    # stats[7]: AR @ IoU=0.50:0.95 area=all maxDets=10\n",
    "    # stats[8]: AR @ IoU=0.50:0.95 area=all maxDets=100\n",
    "    # stats[9]: AR @ IoU=0.50:0.95 area=small maxDets=100\n",
    "    # stats[10]: AR @ IoU=0.50:0.95 area=medium maxDets=100\n",
    "    # stats[11]: AR @ IoU=0.50:0.95 area=large maxDets=100\n",
    "    \n",
    "    metrics = {\n",
    "        'map50_95': coco_eval.stats[0],  # mAP@0.5:0.95\n",
    "        'map50': coco_eval.stats[1],     # mAP@0.5\n",
    "        'map75': coco_eval.stats[2],     # mAP@0.75\n",
    "        'map50_95_small': coco_eval.stats[3],\n",
    "        'map50_95_medium': coco_eval.stats[4],\n",
    "        'map50_95_large': coco_eval.stats[5],\n",
    "        'ar_all_1': coco_eval.stats[6],\n",
    "        'ar_all_10': coco_eval.stats[7],\n",
    "        'ar_all_100': coco_eval.stats[8],\n",
    "        'ar_small': coco_eval.stats[9],\n",
    "        'ar_medium': coco_eval.stats[10],\n",
    "        'ar_large': coco_eval.stats[11]\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” INÃCIO DA AVALIAÃ‡ÃƒO COCO\n",
      "============================================================\n",
      "   ğŸ“‚ Arquivo de anotaÃ§Ãµes: dataset_final_coco\\coco_annotations_test.json\n",
      "   ğŸ¯ Score threshold: 0.3\n",
      "   ğŸ’» Device: cuda\n",
      "\n",
      "ğŸ“‹ Carregando anotaÃ§Ãµes COCO...\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "   âš ï¸  Adicionando chave 'info' ao dataset\n",
      "   ğŸ“Š Categorias encontradas: [1]\n",
      "   ğŸ“‹ Category ID esperado: 1 (gun)\n",
      "\n",
      "ğŸ”„ Processando 50 imagens...\n",
      "   â³ Processadas 10/50 imagens...\n",
      "   â³ Processadas 20/50 imagens...\n",
      "   â³ Processadas 30/50 imagens...\n",
      "   â³ Processadas 40/50 imagens...\n",
      "   â³ Processadas 50/50 imagens...\n",
      "\n",
      "âœ… Processamento concluÃ­do!\n",
      "   ğŸ“Š Total de prediÃ§Ãµes: 108\n",
      "   ğŸ“¸ Imagens processadas: 50\n",
      "\n",
      "ğŸ“‹ Filtrando anotaÃ§Ãµes COCO para o subset...\n",
      "   ğŸ“¸ Image IDs no subset: 50\n",
      "\n",
      "ğŸ“Š Calculando mÃ©tricas COCO...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.812\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.470\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.404\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.415\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.427\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.640\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š RESULTADOS DA AVALIAÃ‡ÃƒO\n",
      "============================================================\n",
      "âœ… mAP@0.5:      0.8123 (81.23%)\n",
      "âœ… mAP@0.5:0.95: 0.4910 (49.10%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# AVALIAÃ‡ÃƒO FINAL NO DATASET TEST (apenas uma vez no final)\n",
    "# ================================================================\n",
    "\n",
    "# Carregar dataset de TESTE (avaliaÃ§Ã£o final)\n",
    "test_dataset = CocoAlbumentationsDataset(\n",
    "    img_dir=os.path.join(root_dir, \"test\", \"images\"),\n",
    "    ann_file=os.path.join(root_dir, \"coco_annotations_test.json\"),\n",
    "    transforms=val_transforms,\n",
    "    subset_size=SUBSET_SIZE\n",
    ")\n",
    "\n",
    "test_valid_indices = [i for i in range(len(test_dataset)) if test_dataset[i] is not None]\n",
    "\n",
    "# Avaliar modelo no dataset de TESTE\n",
    "ann_path = os.path.join(root_dir, \"coco_annotations_test.json\")\n",
    "\n",
    "metrics = evaluate_coco(\n",
    "    model,\n",
    "    test_dataset,\n",
    "    ann_path,\n",
    "    device=device,\n",
    "    score_threshold=0.3,\n",
    "    valid_indices=test_valid_indices\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š RESULTADOS FINAIS DA AVALIAÃ‡ÃƒO (TEST)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"âœ… mAP@0.5:      {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)\")\n",
    "print(f\"âœ… mAP@0.5:0.95: {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)\")\n",
    "print(f\"âœ… mAP@0.75:     {metrics['map75']:.4f} ({metrics['map75']*100:.2f}%)\")\n",
    "print(f\"âœ… AR@100:       {metrics['ar_all_100']:.4f} ({metrics['ar_all_100']*100:.2f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
