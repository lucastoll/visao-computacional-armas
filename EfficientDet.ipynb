{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qEsQBCPL4j6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1241eaf-7d7b-4454-bc28-fc86f6fe32c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: effdet in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.12/dist-packages (from effdet) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet) (0.23.0+cu126)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from effdet) (1.0.22)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet) (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->effdet) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm>=0.9.2->effdet) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm>=0.9.2->effdet) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->effdet) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->effdet) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.1->effdet) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.1->effdet) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install effdet\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqNnp8_Ov8zA",
        "outputId": "86be6374-3ea3-4f02-ae97-62613cffd3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset extra√≠do em: /content/dataset/dataset_final_coco\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1) MONTAR DRIVE E DESCOMPACTAR O DATASET\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/dataset_final_coco.zip\"\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall('/content/dataset')\n",
        "\n",
        "root_dir = \"/content/dataset/dataset_final_coco\"\n",
        "print(\"Dataset extra√≠do em:\", root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fkLOOU-34F8z"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ------------------------------\n",
        "# TRAIN TRANSFORMS (fortes)\n",
        "# ------------------------------\n",
        "def get_train_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.HueSaturationValue(\n",
        "                hue_shift_limit=0.015,\n",
        "                sat_shift_limit=0.7,\n",
        "                val_shift_limit=0.4,\n",
        "                p=1.0\n",
        "            ),\n",
        "\n",
        "            A.Rotate(limit=30, border_mode=0, p=1.0),\n",
        "\n",
        "            A.Affine(\n",
        "                translate_percent=0.1,\n",
        "                scale=(0.5, 1.5),\n",
        "                shear=(-5, 5),\n",
        "                p=1.0\n",
        "            ),\n",
        "\n",
        "            A.Perspective(scale=(0.0005, 0.0005), p=1.0),\n",
        "\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "\n",
        "            A.RandomCrop(height=512, width=512, p=1.0, pad_if_needed=True),\n",
        "            A.Resize(height=640, width=640, p=1.0),\n",
        "\n",
        "            A.Normalize(\n",
        "                mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225),\n",
        "                max_pixel_value=255.0\n",
        "            ),\n",
        "\n",
        "            ToTensorV2()\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"pascal_voc\",\n",
        "            label_fields=[\"labels\"],\n",
        "            min_visibility=0.2\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# VAL/TEST TRANSFORMS (leves)\n",
        "# ------------------------------\n",
        "def get_eval_transforms():\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(640, 640),\n",
        "            A.Normalize(\n",
        "                mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225),\n",
        "                max_pixel_value=255.0\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format=\"pascal_voc\",\n",
        "            label_fields=[\"labels\"],\n",
        "            min_visibility=0.0\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pyREej1016ZH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "class CocoAlbumentationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, ann_file, transforms=None):\n",
        "        import json\n",
        "        self.img_dir = img_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        with open(ann_file, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
        "\n",
        "        # agrupar anota√ß√µes por imagem\n",
        "        self.annotations = {}\n",
        "        for ann in data[\"annotations\"]:\n",
        "            img_id = ann[\"image_id\"]\n",
        "            if img_id not in self.annotations:\n",
        "                self.annotations[img_id] = []\n",
        "            self.annotations[img_id].append(ann)\n",
        "\n",
        "        self.ids = list(self.images.keys())\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        info = self.images[img_id]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, info[\"file_name\"])\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "        annots = self.annotations.get(img_id, [])\n",
        "\n",
        "        # se n√£o tem boxes ‚Üí pula imagem\n",
        "        if len(annots) == 0:\n",
        "            return None\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in annots:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(\n",
        "                image=image,\n",
        "                bboxes=boxes,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            image = transformed[\"image\"]\n",
        "            boxes = transformed[\"bboxes\"]\n",
        "            labels = transformed[\"labels\"]\n",
        "\n",
        "        # üö® Prote√ß√£o contra augmentations removerem TODAS as caixas\n",
        "        if len(boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZjJplR4N2E8V"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    return tuple(zip(*batch))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IedNChtP4Fvz"
      },
      "outputs": [],
      "source": [
        "train_dataset = CocoAlbumentationsDataset(\n",
        "    f\"{root_dir}/train/images\",\n",
        "    f\"{root_dir}/coco_annotations_train.json\",\n",
        "    transforms=get_train_transforms()\n",
        ")\n",
        "\n",
        "val_dataset = CocoAlbumentationsDataset(\n",
        "    f\"{root_dir}/val/images\",\n",
        "    f\"{root_dir}/coco_annotations_val.json\",\n",
        "    transforms=get_eval_transforms()\n",
        ")\n",
        "\n",
        "test_dataset = CocoAlbumentationsDataset(\n",
        "    f\"{root_dir}/test/images\",\n",
        "    f\"{root_dir}/coco_annotations_test.json\",\n",
        "    transforms=get_eval_transforms()\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_vlvjq1XweXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08149904-80ca-4148-ad50-911df456ab87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        }
      ],
      "source": [
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "\n",
        "num_classes = 1  # apenas GUN (EfficientDet n√£o usa \"background\")\n",
        "\n",
        "# Config do modelo (D0: 512px, D1: 640px, D2: 768px)\n",
        "config = get_efficientdet_config('tf_efficientdet_d1')\n",
        "\n",
        "config.num_classes = num_classes\n",
        "config.image_size = (640, 640)\n",
        "config.norm_kwargs = dict(eps=1e-4)\n",
        "\n",
        "# Carregar modelo pr√©-treinado\n",
        "model = EfficientDet(config, pretrained_backbone=True)\n",
        "\n",
        "# Substituir head\n",
        "model.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
        "\n",
        "# Wrap para treinamento correto\n",
        "model = DetBenchTrain(model, config)\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "# Otimizador\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.937,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=1.0,\n",
        "    end_factor=0.01,\n",
        "    total_iters=100  # 100 √©pocas\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vTcwm5wOw5X2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "save_dir = \"runs/fasterrcnn/train1\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"lr\": [],\n",
        "    \"epoch\": [],\n",
        "    \"map50\": [],\n",
        "    \"map5095\": []\n",
        "}\n",
        "\n",
        "with open(f\"{save_dir}/history.json\", \"w\") as f:\n",
        "    json.dump(history, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "enXUlgmTw8-F"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# M√âTRICAS COCO COMPLETAS\n",
        "# ================================================================\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "\n",
        "def evaluate_coco(model, dataset, ann_path, device=\"cuda\"):\n",
        "    model.eval()\n",
        "\n",
        "    coco_gt = COCO(ann_path)\n",
        "    results = []\n",
        "\n",
        "    for img, target in dataset:\n",
        "        img_tensor = img.to(device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(img_tensor)[0]\n",
        "\n",
        "        boxes = output[\"boxes\"].cpu().numpy()\n",
        "        scores = output[\"scores\"].cpu().numpy()\n",
        "        labels = output[\"labels\"].cpu().numpy()\n",
        "\n",
        "        image_id = int(target[\"image_id\"].item())\n",
        "\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            results.append({\n",
        "                \"image_id\": image_id,\n",
        "                \"category_id\": 1,\n",
        "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                \"score\": float(score)\n",
        "            })\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(results)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    # mAP50 = COCO metric 1\n",
        "    # mAP50-95 = COCO metric 0\n",
        "    map50 = coco_eval.stats[1]\n",
        "    map5095 = coco_eval.stats[0]\n",
        "\n",
        "    return map50, map5095\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FUNCTION ‚Äî CONFUSION MATRIX\n",
        "# ================================================================\n",
        "def plot_confusion_matrix(y_true, y_pred, save_path):\n",
        "    labels = [\"background\", \"Gun\"]\n",
        "    cm = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t][p] += 1\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FUNCTION ‚Äî PRECISION-RECALL CURVE\n",
        "# ================================================================\n",
        "def plot_pr_curve(precisions, recalls, save_path):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(recalls, precisions)\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.grid()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FUNCTION ‚Äî F1 CURVE\n",
        "# ================================================================\n",
        "def plot_f1_curve(precisions, recalls, save_path):\n",
        "    f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(recalls, f1)\n",
        "    plt.title(\"F1 Curve\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.grid()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL4alx3NwhcS",
        "outputId": "6cf54bee-394a-4baa-ff22-552b4adc14a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0] Batch 0: 3 imagens\n",
            "  Loss do batch: 103.2849\n",
            "[Epoch 0] Batch 1: 4 imagens\n",
            "  Loss do batch: 289.4260\n",
            "[Epoch 0] Batch 2: 4 imagens\n",
            "  Loss do batch: 28.8074\n",
            "[Epoch 0] Batch 3: 4 imagens\n",
            "  Loss do batch: 4.4617\n",
            "[Epoch 0] Batch 4: 3 imagens\n",
            "  Loss do batch: 3.6976\n",
            "[Epoch 0] Batch 5: 4 imagens\n",
            "  Loss do batch: 4.0289\n",
            "[Epoch 0] Batch 6: 3 imagens\n",
            "  Loss do batch: 6.0310\n",
            "[Epoch 0] Batch 7: 4 imagens\n",
            "  Loss do batch: 6.2578\n",
            "[Epoch 0] Batch 8: 4 imagens\n",
            "  Loss do batch: 9.1418\n",
            "[Epoch 0] Batch 9: 4 imagens\n",
            "  Loss do batch: 5.6619\n",
            "[Epoch 0] Batch 10: 4 imagens\n",
            "  Loss do batch: 6.6357\n",
            "[Epoch 0] Batch 11: 4 imagens\n",
            "  Loss do batch: 5.2862\n",
            "[Epoch 0] Batch 12: 3 imagens\n",
            "  Loss do batch: 5.7345\n",
            "[Epoch 0] Batch 13: 4 imagens\n",
            "  Loss do batch: 11.7474\n",
            "[Epoch 0] Batch 14: 4 imagens\n",
            "  Loss do batch: 8.2129\n",
            "[Epoch 0] Batch 15: 4 imagens\n",
            "  Loss do batch: 8.8456\n",
            "[Epoch 0] Batch 16: 3 imagens\n",
            "  Loss do batch: 12.1127\n",
            "[Epoch 0] Batch 17: 4 imagens\n",
            "  Loss do batch: 18.6662\n",
            "[Epoch 0] Batch 18: 4 imagens\n",
            "  Loss do batch: 2.5412\n",
            "[Epoch 0] Batch 19: 4 imagens\n",
            "  Loss do batch: 3.8395\n",
            "[Epoch 0] Batch 20: 4 imagens\n",
            "  Loss do batch: 5.1457\n",
            "[Epoch 0] Batch 21: 3 imagens\n",
            "  Loss do batch: 5.2825\n",
            "[Epoch 0] Batch 22: 4 imagens\n",
            "  Loss do batch: 5.7515\n",
            "[Epoch 0] Batch 23: 4 imagens\n",
            "  Loss do batch: 2.6965\n",
            "[Epoch 0] Batch 24: 4 imagens\n",
            "  Loss do batch: 10.6883\n",
            "[Epoch 0] Batch 25: 2 imagens\n",
            "  Loss do batch: 2.3590\n",
            "[Epoch 0] Batch 26: 4 imagens\n",
            "  Loss do batch: 3.9455\n",
            "[Epoch 0] Batch 27: 4 imagens\n",
            "  Loss do batch: 2.4992\n",
            "[Epoch 0] Batch 28: 4 imagens\n",
            "  Loss do batch: 6.8341\n",
            "[Epoch 0] Batch 29: 4 imagens\n",
            "  Loss do batch: 5.5187\n",
            "[Epoch 0] Batch 30: 4 imagens\n",
            "  Loss do batch: 10.2869\n",
            "[Epoch 0] Batch 31: 4 imagens\n",
            "  Loss do batch: 6.3109\n",
            "[Epoch 0] Batch 32: 3 imagens\n",
            "  Loss do batch: 4.2559\n",
            "[Epoch 0] Batch 33: 4 imagens\n",
            "  Loss do batch: 4.4378\n",
            "[Epoch 0] Batch 34: 3 imagens\n",
            "  Loss do batch: 8.1938\n",
            "[Epoch 0] Batch 35: 4 imagens\n",
            "  Loss do batch: 13.4054\n",
            "[Epoch 0] Batch 36: 4 imagens\n",
            "  Loss do batch: 3.7342\n",
            "[Epoch 0] Batch 37: 4 imagens\n",
            "  Loss do batch: 3.6447\n",
            "[Epoch 0] Batch 38: 4 imagens\n",
            "  Loss do batch: 3.3691\n",
            "[Epoch 0] Batch 39: 4 imagens\n",
            "  Loss do batch: 16.1273\n",
            "[Epoch 0] Batch 40: 4 imagens\n",
            "  Loss do batch: 3.8097\n",
            "[Epoch 0] Batch 41: 4 imagens\n",
            "  Loss do batch: 4.4323\n",
            "[Epoch 0] Batch 42: 4 imagens\n",
            "  Loss do batch: 4.5861\n",
            "[Epoch 0] Batch 43: 4 imagens\n",
            "  Loss do batch: 2.8506\n",
            "[Epoch 0] Batch 44: 3 imagens\n",
            "  Loss do batch: 1.8691\n",
            "[Epoch 0] Batch 45: 4 imagens\n",
            "  Loss do batch: 9.6781\n",
            "[Epoch 0] Batch 46: 4 imagens\n",
            "  Loss do batch: 3.8040\n",
            "[Epoch 0] Batch 47: 4 imagens\n",
            "  Loss do batch: 1.6112\n",
            "[Epoch 0] Batch 48: 4 imagens\n",
            "  Loss do batch: 1.4185\n",
            "[Epoch 0] Batch 49: 3 imagens\n",
            "  Loss do batch: 2.5302\n",
            "[Epoch 0] Batch 50: 3 imagens\n",
            "  Loss do batch: 2.7047\n",
            "[Epoch 0] Batch 51: 3 imagens\n",
            "  Loss do batch: 9.0078\n",
            "[Epoch 0] Batch 52: 4 imagens\n",
            "  Loss do batch: 1.2262\n",
            "[Epoch 0] Batch 53: 3 imagens\n",
            "  Loss do batch: 8.7851\n",
            "[Epoch 0] Batch 54: 4 imagens\n",
            "  Loss do batch: 3.7944\n",
            "[Epoch 0] Batch 55: 3 imagens\n",
            "  Loss do batch: 3.6242\n",
            "[Epoch 0] Batch 56: 4 imagens\n",
            "  Loss do batch: 3.6713\n",
            "[Epoch 0] Batch 57: 4 imagens\n",
            "  Loss do batch: 8.5774\n",
            "[Epoch 0] Batch 58: 4 imagens\n",
            "  Loss do batch: 1.7338\n",
            "[Epoch 0] Batch 59: 3 imagens\n",
            "  Loss do batch: 1.6176\n",
            "[Epoch 0] Batch 60: 4 imagens\n",
            "  Loss do batch: 3.5679\n",
            "[Epoch 0] Batch 61: 4 imagens\n",
            "  Loss do batch: 4.7678\n",
            "[Epoch 0] Batch 62: 4 imagens\n",
            "  Loss do batch: 4.0707\n",
            "[Epoch 0] Batch 63: 3 imagens\n",
            "  Loss do batch: 1.3237\n",
            "[Epoch 0] Batch 64: 4 imagens\n",
            "  Loss do batch: 1.8874\n",
            "[Epoch 0] Batch 65: 4 imagens\n",
            "  Loss do batch: 2.4579\n",
            "[Epoch 0] Batch 66: 4 imagens\n",
            "  Loss do batch: 1.2340\n",
            "[Epoch 0] Batch 67: 4 imagens\n",
            "  Loss do batch: 6.9262\n",
            "[Epoch 0] Batch 68: 4 imagens\n",
            "  Loss do batch: 1.5402\n",
            "[Epoch 0] Batch 69: 4 imagens\n",
            "  Loss do batch: 1.2973\n",
            "[Epoch 0] Batch 70: 4 imagens\n",
            "  Loss do batch: 2.8467\n",
            "[Epoch 0] Batch 71: 4 imagens\n",
            "  Loss do batch: 2.5343\n",
            "[Epoch 0] Batch 72: 3 imagens\n",
            "  Loss do batch: 34.5415\n",
            "[Epoch 0] Batch 73: 4 imagens\n",
            "  Loss do batch: 6.3920\n",
            "[Epoch 0] Batch 74: 4 imagens\n",
            "  Loss do batch: 2.8837\n",
            "[Epoch 0] Batch 75: 4 imagens\n",
            "  Loss do batch: 5.2261\n",
            "[Epoch 0] Batch 76: 3 imagens\n",
            "  Loss do batch: 2.5492\n",
            "[Epoch 0] Batch 77: 3 imagens\n",
            "  Loss do batch: 1.3709\n",
            "[Epoch 0] Batch 78: 4 imagens\n",
            "  Loss do batch: 2.2451\n",
            "[Epoch 0] Batch 79: 3 imagens\n",
            "  Loss do batch: 2.1669\n",
            "[Epoch 0] Batch 80: 3 imagens\n",
            "  Loss do batch: 2.5310\n",
            "[Epoch 0] Batch 81: 4 imagens\n",
            "  Loss do batch: 2.1362\n",
            "[Epoch 0] Batch 82: 4 imagens\n",
            "  Loss do batch: 2.1368\n",
            "[Epoch 0] Batch 83: 4 imagens\n",
            "  Loss do batch: 2.3115\n",
            "[Epoch 0] Batch 84: 4 imagens\n",
            "  Loss do batch: 2.7609\n",
            "[Epoch 0] Batch 85: 4 imagens\n",
            "  Loss do batch: 1.7687\n",
            "[Epoch 0] Batch 86: 4 imagens\n",
            "  Loss do batch: 2.3810\n",
            "[Epoch 0] Batch 87: 4 imagens\n",
            "  Loss do batch: 2.9121\n",
            "[Epoch 0] Batch 88: 4 imagens\n",
            "  Loss do batch: 1.5292\n",
            "[Epoch 0] Batch 89: 4 imagens\n",
            "  Loss do batch: 2.3022\n",
            "[Epoch 0] Batch 90: 4 imagens\n",
            "  Loss do batch: 2.0377\n",
            "[Epoch 0] Batch 91: 4 imagens\n",
            "  Loss do batch: 1.8171\n",
            "[Epoch 0] Batch 92: 4 imagens\n",
            "  Loss do batch: 2.1170\n",
            "[Epoch 0] Batch 93: 3 imagens\n",
            "  Loss do batch: 3.9338\n",
            "[Epoch 0] Batch 94: 3 imagens\n",
            "  Loss do batch: 1.6729\n",
            "[Epoch 0] Batch 95: 3 imagens\n",
            "  Loss do batch: 1.5435\n",
            "[Epoch 0] Batch 96: 4 imagens\n",
            "  Loss do batch: 4.4403\n",
            "[Epoch 0] Batch 97: 4 imagens\n",
            "  Loss do batch: 1.6251\n",
            "[Epoch 0] Batch 98: 4 imagens\n",
            "  Loss do batch: 5.4774\n",
            "[Epoch 0] Batch 99: 4 imagens\n",
            "  Loss do batch: 3.2097\n",
            "[Epoch 0] Batch 100: 4 imagens\n",
            "  Loss do batch: 1.2551\n",
            "[Epoch 0] Batch 101: 3 imagens\n",
            "  Loss do batch: 1.1580\n",
            "[Epoch 0] Batch 102: 4 imagens\n",
            "  Loss do batch: 2.1889\n",
            "[Epoch 0] Batch 103: 4 imagens\n",
            "  Loss do batch: 3.1861\n",
            "[Epoch 0] Batch 104: 3 imagens\n",
            "  Loss do batch: 2.3605\n",
            "[Epoch 0] Batch 105: 3 imagens\n",
            "  Loss do batch: 1.3492\n",
            "[Epoch 0] Batch 106: 4 imagens\n",
            "  Loss do batch: 3.9313\n",
            "[Epoch 0] Batch 107: 2 imagens\n",
            "  Loss do batch: 2.6018\n",
            "[Epoch 0] Batch 108: 4 imagens\n",
            "  Loss do batch: 1.2913\n",
            "[Epoch 0] Batch 109: 4 imagens\n",
            "  Loss do batch: 2.1377\n",
            "[Epoch 0] Batch 110: 4 imagens\n",
            "  Loss do batch: 1.1990\n",
            "[Epoch 0] Batch 111: 3 imagens\n",
            "  Loss do batch: 1.2999\n",
            "[Epoch 0] Batch 112: 3 imagens\n",
            "  Loss do batch: 1.3840\n",
            "[Epoch 0] Batch 113: 4 imagens\n",
            "  Loss do batch: 2.0296\n",
            "[Epoch 0] Batch 114: 3 imagens\n",
            "  Loss do batch: 2.1401\n",
            "[Epoch 0] Batch 115: 4 imagens\n",
            "  Loss do batch: 1.2241\n",
            "[Epoch 0] Batch 116: 4 imagens\n",
            "  Loss do batch: 4.2479\n",
            "[Epoch 0] Batch 117: 4 imagens\n",
            "  Loss do batch: 1.2069\n",
            "[Epoch 0] Batch 118: 4 imagens\n",
            "  Loss do batch: 1.3106\n",
            "[Epoch 0] Batch 119: 4 imagens\n",
            "  Loss do batch: 2.0904\n",
            "[Epoch 0] Batch 120: 3 imagens\n",
            "  Loss do batch: 1.1054\n",
            "[Epoch 0] Batch 121: 3 imagens\n",
            "  Loss do batch: 11.0586\n",
            "[Epoch 0] Batch 122: 4 imagens\n",
            "  Loss do batch: 1.4849\n",
            "[Epoch 0] Batch 123: 3 imagens\n",
            "  Loss do batch: 1.3936\n",
            "[Epoch 0] Batch 124: 4 imagens\n",
            "  Loss do batch: 1.5910\n",
            "[Epoch 0] Batch 125: 4 imagens\n",
            "  Loss do batch: 1.5907\n",
            "[Epoch 0] Batch 126: 3 imagens\n",
            "  Loss do batch: 1.4329\n",
            "[Epoch 0] Batch 127: 4 imagens\n",
            "  Loss do batch: 1.2161\n",
            "[Epoch 0] Batch 128: 3 imagens\n",
            "  Loss do batch: 1.3311\n",
            "[Epoch 0] Batch 129: 4 imagens\n",
            "  Loss do batch: 1.4703\n",
            "[Epoch 0] Batch 130: 3 imagens\n",
            "  Loss do batch: 1.2960\n",
            "[Epoch 0] Batch 131: 4 imagens\n",
            "  Loss do batch: 2.1439\n",
            "[Epoch 0] Batch 132: 2 imagens\n",
            "  Loss do batch: 1.4266\n",
            "[Epoch 0] Batch 133: 4 imagens\n",
            "  Loss do batch: 1.9816\n",
            "[Epoch 0] Batch 134: 3 imagens\n",
            "  Loss do batch: 1.2569\n",
            "[Epoch 0] Batch 135: 4 imagens\n",
            "  Loss do batch: 3.2325\n",
            "[Epoch 0] Batch 136: 4 imagens\n",
            "  Loss do batch: 1.9400\n",
            "[Epoch 0] Batch 137: 3 imagens\n",
            "  Loss do batch: 1.8520\n",
            "[Epoch 0] Batch 138: 4 imagens\n",
            "  Loss do batch: 1.3229\n",
            "[Epoch 0] Batch 139: 4 imagens\n",
            "  Loss do batch: 1.3546\n",
            "[Epoch 0] Batch 140: 4 imagens\n",
            "  Loss do batch: 2.0287\n",
            "[Epoch 0] Batch 141: 4 imagens\n",
            "  Loss do batch: 1.2925\n",
            "[Epoch 0] Batch 142: 4 imagens\n",
            "  Loss do batch: 2.0776\n",
            "[Epoch 0] Batch 143: 4 imagens\n",
            "  Loss do batch: 2.4974\n",
            "[Epoch 0] Batch 144: 4 imagens\n",
            "  Loss do batch: 1.1453\n",
            "[Epoch 0] Batch 145: 4 imagens\n",
            "  Loss do batch: 1.1612\n",
            "[Epoch 0] Batch 146: 2 imagens\n",
            "  Loss do batch: 2.0247\n",
            "[Epoch 0] Batch 147: 4 imagens\n",
            "  Loss do batch: 2.1315\n",
            "[Epoch 0] Batch 148: 3 imagens\n",
            "  Loss do batch: 1.6621\n",
            "[Epoch 0] Batch 149: 3 imagens\n",
            "  Loss do batch: 2.2125\n",
            "[Epoch 0] Batch 150: 4 imagens\n",
            "  Loss do batch: 9.7172\n",
            "[Epoch 0] Batch 151: 4 imagens\n",
            "  Loss do batch: 1.8153\n",
            "[Epoch 0] Batch 152: 4 imagens\n",
            "  Loss do batch: 1.7338\n",
            "[Epoch 0] Batch 153: 4 imagens\n",
            "  Loss do batch: 1.2210\n",
            "[Epoch 0] Batch 154: 3 imagens\n",
            "  Loss do batch: 1.2682\n",
            "[Epoch 0] Batch 155: 3 imagens\n",
            "  Loss do batch: 1.1258\n",
            "[Epoch 0] Batch 156: 3 imagens\n",
            "  Loss do batch: 2.0578\n",
            "[Epoch 0] Batch 157: 4 imagens\n",
            "  Loss do batch: 1.5803\n",
            "[Epoch 0] Batch 158: 4 imagens\n",
            "  Loss do batch: 1.7311\n",
            "[Epoch 0] Batch 159: 3 imagens\n",
            "  Loss do batch: 1.9125\n",
            "[Epoch 0] Batch 160: 4 imagens\n",
            "  Loss do batch: 2.4937\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "patience = 20\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
        "\n",
        "        # üîç LOG DO BATCH\n",
        "        print(f\"[Epoch {epoch}] Batch {batch_idx}: {len(imgs)} imagens\")\n",
        "\n",
        "        # detectar erros comuns\n",
        "        for i, t in enumerate(targets):\n",
        "            if t[\"boxes\"].shape[0] == 0:\n",
        "                print(f\"  ‚ö†Ô∏è  Aten√ß√£o: imagem {i} no batch {batch_idx} est√° com boxes vazios!\")\n",
        "\n",
        "        # GPU -> imgs\n",
        "        imgs = torch.stack([img.cuda() for img in imgs], dim=0)\n",
        "\n",
        "        # GPU -> targets individuais\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # =======================================================\n",
        "        #   CONVERTER TARGETS PARA O FORMATO DO EFFICIENTDET\n",
        "        # =======================================================\n",
        "\n",
        "        batch_bboxes = [t[\"boxes\"] for t in targets]\n",
        "        batch_labels = [t[\"labels\"] for t in targets]\n",
        "\n",
        "        # n√∫mero m√°ximo de boxes no batch\n",
        "        max_boxes = max(b.shape[0] for b in batch_bboxes)\n",
        "\n",
        "        padded_boxes = []\n",
        "        padded_labels = []\n",
        "\n",
        "        for b, l in zip(batch_bboxes, batch_labels):\n",
        "            num = b.shape[0]\n",
        "            if num < max_boxes:\n",
        "                pad_b = torch.zeros((max_boxes - num, 4), device=b.device)\n",
        "                pad_l = torch.zeros((max_boxes - num,), device=l.device, dtype=l.dtype)\n",
        "                b = torch.cat([b, pad_b], dim=0)\n",
        "                l = torch.cat([l, pad_l], dim=0)\n",
        "            padded_boxes.append(b)\n",
        "            padded_labels.append(l)\n",
        "\n",
        "        # agora EfficientDet aceita\n",
        "        eff_target = {\n",
        "            \"bbox\": torch.stack(padded_boxes, dim=0),   # [B, max_boxes, 4]\n",
        "            \"cls\": torch.stack(padded_labels, dim=0),   # [B, max_boxes]\n",
        "        }\n",
        "\n",
        "        # =======================================================\n",
        "        #   FORWARD\n",
        "        # =======================================================\n",
        "        # forward\n",
        "        losses = model(imgs, eff_target)\n",
        "\n",
        "        # EfficientDet retorna dict\n",
        "        if isinstance(losses, dict):\n",
        "            loss_value = losses[\"loss\"]  # geralmente \"loss\", pode ser \"loss_total\" dependendo da vers√£o\n",
        "        else:\n",
        "            loss_value = losses          # (fallback)\n",
        "\n",
        "        print(f\"  Loss do batch: {loss_value.item():.4f}\")\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss_value.item()\n",
        "\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # =======================================================\n",
        "    # AVALIA√á√ÉO COCO EM VAL\n",
        "    # =======================================================\n",
        "    map50, map5095 = evaluate_coco(\n",
        "        model,\n",
        "        val_dataset,\n",
        "        f\"{root_dir}/annotations/instances_val.json\"\n",
        "    )\n",
        "\n",
        "    print(f\"   mAP50={map50:.4f} | mAP50-95={map5095:.4f}\")\n",
        "\n",
        "    # =======================================================\n",
        "    # Salvar hist√≥rico\n",
        "    # =======================================================\n",
        "    history[\"train_loss\"].append(total_loss)\n",
        "    history[\"map50\"].append(map50)\n",
        "    history[\"map5095\"].append(map5095)\n",
        "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "    history[\"epoch\"].append(epoch)\n",
        "\n",
        "    with open(f\"{save_dir}/history.json\", \"w\") as f:\n",
        "        json.dump(history, f, indent=4)\n",
        "\n",
        "    # =======================================================\n",
        "    # EARLY STOPPING\n",
        "    # =======================================================\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), f\"{save_dir}/best_model.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"EARLY STOPPING\")\n",
        "            break\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# GERAR GR√ÅFICOS AP√ìS O TREINAMENTO\n",
        "# =======================================================\n",
        "\n",
        "# Loss plot\n",
        "plt.figure()\n",
        "plt.plot(history[\"epoch\"], history[\"train_loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.savefig(f\"{save_dir}/losses.png\")\n",
        "plt.close()\n",
        "\n",
        "# mAP plot\n",
        "plt.figure()\n",
        "plt.plot(history[\"epoch\"], history[\"map50\"], label=\"mAP50\")\n",
        "plt.plot(history[\"epoch\"], history[\"map5095\"], label=\"mAP50-95\")\n",
        "plt.legend()\n",
        "plt.title(\"mAP Curve\")\n",
        "plt.savefig(f\"{save_dir}/map_curve.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TboIwUKrxDna"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# FINAL EVALUATION FOR PR + F1 + CONFUSION MATRIX\n",
        "# ================================================================\n",
        "\n",
        "all_scores = []\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "for img, target in val_dataset:\n",
        "    img_tensor = img.cuda().unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(img_tensor)[0]\n",
        "\n",
        "    scores = out[\"scores\"].cpu().numpy()\n",
        "    labels_pred = out[\"labels\"].cpu().numpy()\n",
        "    labels_true = target[\"labels\"].numpy()\n",
        "\n",
        "    # armazenar\n",
        "    all_scores.extend(scores)\n",
        "    all_preds.extend(labels_pred)\n",
        "    all_labels.extend(labels_true)\n",
        "\n",
        "\n",
        "# ORDENAR POR SCORE PARA PR CURVE\n",
        "order = np.argsort(-np.array(all_scores))\n",
        "preds = np.array(all_preds)[order]\n",
        "labels = np.array(all_labels)[order]\n",
        "\n",
        "tp = (preds == labels)\n",
        "fp = (preds != labels)\n",
        "fn = (labels != preds)\n",
        "\n",
        "precision = np.cumsum(tp) / (np.cumsum(tp + fp) + 1e-6)\n",
        "recall = np.cumsum(tp) / (len(labels) + 1e-6)\n",
        "\n",
        "plot_pr_curve(precision, recall, f\"{save_dir}/pr_curve.png\")\n",
        "plot_f1_curve(precision, recall, f\"{save_dir}/f1_curve.png\")\n",
        "plot_confusion_matrix(all_labels, all_preds, f\"{save_dir}/confusion_matrix.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xweWGwbbxG8N"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img1 = Image.open(f\"{save_dir}/losses.png\")\n",
        "img2 = Image.open(f\"{save_dir}/map_curve.png\")\n",
        "img3 = Image.open(f\"{save_dir}/pr_curve.png\")\n",
        "img4 = Image.open(f\"{save_dir}/f1_curve.png\")\n",
        "\n",
        "width = max(img1.width, img2.width)\n",
        "height = img1.height + img2.height + img3.height + img4.height\n",
        "\n",
        "results = Image.new(\"RGB\", (width, height), \"white\")\n",
        "\n",
        "y = 0\n",
        "for img in [img1, img2, img3, img4]:\n",
        "    results.paste(img, (0, y))\n",
        "    y += img.height\n",
        "\n",
        "results.save(f\"{save_dir}/results.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPlBbJh3wkSr"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"best_fasterrcnn.pth\"))\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "for img, target in test_dataset:\n",
        "    img_tensor = img.cuda().unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)[0]\n",
        "\n",
        "    results.append({\n",
        "        \"image_id\": target[\"image_id\"].item(),\n",
        "        \"boxes\": output[\"boxes\"].cpu().tolist(),\n",
        "        \"scores\": output[\"scores\"].cpu().tolist(),\n",
        "        \"labels\": output[\"labels\"].cpu().tolist()\n",
        "    })\n",
        "\n",
        "print(\"Infer√™ncia conclu√≠da!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}